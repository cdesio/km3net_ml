{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "import root_numpy as rnp\n",
    "from plotly.offline import init_notebook_mode, iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of useful constants and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NFLOORS = 18\n",
    "NSTRING = 115\n",
    "PMTSPERDOM = 31\n",
    "pmtstot = NFLOORS * NSTRING * PMTSPERDOM\n",
    "ndoms = NFLOORS * NSTRING\n",
    "\n",
    "coord_origin = np.asarray((13.887,6.713,405.932))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute this cell to load files on the Vagrant machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detfile = \"../trigger_optimisation_files/km3net_jul13_90m.detx\"\n",
    "nuefile = \"../../ROOT_files/km3_v4_nuecc_1.evt.JTE.aa.root\"\n",
    "numufile = \"../../ROOT_files/km3_v4_numucc_1_B.evt.aa.root\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute this cell to load files on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detfile = \"utilities/km3net_jul13_90m.detx\"\n",
    "nuefile = \"utilities/km3_v4_nuecc_1.evt.JTE.aa.root\"\n",
    "numufile = \"utilities/km3_v4_numucc_1_B.evt.aa.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this cell for multiple input files\n",
    "nuefile1 = \"utilities/km3_v4_nuecc_1.evt.JTE.aa.root\"\n",
    "numufile1 = \"utilities/km3_v4_numucc_1_B.evt.aa.root\"\n",
    "nuefile2 = \"utilities/km3_v4_nuecc_2.evt.JTE.aa.root\"\n",
    "numufile2 = \"utilities/km3_v4_numucc_2.evt.JTE.aa.root\"\n",
    "nuefile3 = \"utilities/km3_v4_nuecc_3.evt.JTE.aa.root\"\n",
    "numufile3 = \"utilities/km3_v4_numucc_3.evt.JTE.aa.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eef5a5133a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetector_positions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured_positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructured_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'detfile' is not defined"
     ]
    }
   ],
   "source": [
    "from detector_positions import structured_positions\n",
    "\n",
    "doms, pmts = structured_positions(detfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ch_id` (Channel ID)\n",
    "\n",
    "For each _hit_, `ch_id` contains the IDs of each hit PMT (i.e. PMT that receives signal).\n",
    "\n",
    "\n",
    "The total number of PMTs is `NFLOORS * NSTRING * PMTSPERDOM` (see `pmtstot`) \n",
    "\n",
    "#### Description:\n",
    "\n",
    "* Size: \n",
    "* Ndims: \n",
    "* Type: `numpy.array`\n",
    "* Dtype: `np.bool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dom_id` (Dom ID)\n",
    "\n",
    "For each _hit_, `dom_id` contains the IDs of each hit DOM (i.e. DOM that receives signal)\n",
    "\n",
    "The total number of DOMs is `NFLOORS * NSTRING` (see `ndoms`) \n",
    "\n",
    "#### Description:\n",
    "\n",
    "* Size: \n",
    "* Ndims: \n",
    "* Type: `numpy.array`\n",
    "* Dtype: `np.bool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `trig` (Triggered)\n",
    "\n",
    "For each _hit_, `trig` is a mask (i.e. Boolean array) indicating whether the corresponding hit has been triggered or not.\n",
    "\n",
    "#### Description:\n",
    "\n",
    "* Size: \n",
    "* Ndims: \n",
    "* Type: `numpy.array`\n",
    "* Dtype: `np.bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_trees(filename):\n",
    "    \"\"\"\n",
    "    Function to import the trees from the aanet input file\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename : str\n",
    "        aanet input file\n",
    "    Returns:\n",
    "    --------\n",
    "    ch_id : np.ndarray\n",
    "        array containing the id of the pmts hit for all of the events (triggered and not triggered) \n",
    "    dom_id : np.ndarray\n",
    "        array containing the id of the doms hit for all of the events (triggered and not triggered) \n",
    "    trig : np.ndarray\n",
    "        array containing the flag `0` or `1` indicating whether the event has been triggered. \n",
    "        The information is stored for each hit\n",
    "    t : np.ndarray\n",
    "        array containing the times of the hits for all of the triggered events \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ch_id = rnp.root2array(filename, treename=\"E\", branches=\"Evt.hits.channel_id\")\n",
    "    dom_id = rnp.root2array(filename, treename=\"E\", branches=\"Evt.hits.dom_id\")\n",
    "    trig = rnp.root2array(filename, treename= \"E\", branches=\"Evt.hits.trig\")\n",
    "    t = rnp.root2array(filename, treename=\"E\", branches=\"Evt.hits.t\")\n",
    "    times = np.asarray([t[evt][trig[evt]==True] for evt in range(t.size)])\n",
    "    \n",
    "    return ch_id, dom_id, trig, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doms and pmts hit for numu and nue files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 file per type\n",
    "ch_id_numu, dom_id_numu, trig_numu, times_numu = import_trees(numufile)\n",
    "ch_id_nue, dom_id_nue, trig_nue, times_nue = import_trees(nuefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  multiple input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_id_numu1, dom_id_numu1, trig_numu1, times_numu1 = import_trees(numufile1)\n",
    "ch_id_nue1, dom_id_nue1, trig_nue1, times_nue1 = import_trees(nuefile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "unable to access tree 'E' in utilities/km3_v4_numucc_2.evt.JTE.aa.root",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-53f0eee0f859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mch_id_numu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdom_id_numu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrig_numu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes_numu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumufile2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mch_id_nue2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdom_id_nue2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrig_nue2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes_nue2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuefile2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-07277c6545b6>\u001b[0m in \u001b[0;36mimport_trees\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot2array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"E\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evt.hits.channel_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdom_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot2array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"E\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evt.hits.dom_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot2array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"E\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evt.hits.trig\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/root_numpy/_tree.pyc\u001b[0m in \u001b[0;36mroot2array\u001b[0;34m(filenames, treename, branches, selection, object_selection, start, stop, step, include_weight, weight_name, cache_size, warn_missing_tree)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         warn_missing_tree)\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mroot_numpy/src/tree.pyx\u001b[0m in \u001b[0;36m_librootnumpy.root2array_fromfile (root_numpy/src/_librootnumpy.cpp:600)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: unable to access tree 'E' in utilities/km3_v4_numucc_2.evt.JTE.aa.root"
     ]
    }
   ],
   "source": [
    "ch_id_numu2, dom_id_numu2, trig_numu2, times_numu2 = import_trees(numufile2)\n",
    "ch_id_nue2, dom_id_nue2, trig_nue2, times_nue2 = import_trees(nuefile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_id_numu3, dom_id_numu3, trig_numu3, times_numu3 = import_trees(numufile3)\n",
    "ch_id_nue3, dom_id_nue3, trig_nue3, times_nue3 = import_trees(nuefile3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hits_positions(evt, dom_id, trig, ch_id):\n",
    "    \"\"\"\n",
    "    Function to calculate the 3D positions of the triggered events\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    evt : np.int\n",
    "        the event id\n",
    "    Returns:\n",
    "    --------\n",
    "    ppmts_hit : np.ndarray\n",
    "        array containing the positions of the pmts hit for the selected event \n",
    "    pdoms_hit : np.ndarray\n",
    "        array containing the positions of the doms hit for the selected event  \n",
    "    \"\"\"\n",
    "    dom_filter = dom_id[evt][trig[evt]==True]-1\n",
    "    pmt_filter = (dom_filter * PMTSPERDOM) + ch_id[evt][trig[evt]==True]\n",
    "    ppmts_hit = pmts[pmt_filter]\n",
    "    pdoms_hit = doms[dom_filter]\n",
    "    return ppmts_hit, pdoms_hit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the hits_positions_numu function to get the positions for all of the events\n",
    "\n",
    "pmts_hit_numu = []\n",
    "doms_hit_numu = []\n",
    "for evt in range(0, dom_id_numu.size):\n",
    "    pm,dm = hits_positions(evt, dom_id_numu, trig_numu, ch_id_numu)\n",
    "    pmts_hit_numu.append(pm)\n",
    "    doms_hit_numu.append(dm)\n",
    "\n",
    "pmts_hit_numu = np.asarray(pmts_hit_numu)\n",
    "doms_hit_numu = np.asarray(doms_hit_numu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmts_hit_nue = []\n",
    "doms_hit_nue = []\n",
    "for evt in range(0, dom_id_nue.size):\n",
    "    pm,dm = hits_positions(evt, dom_id_nue, trig_nue, ch_id_nue)\n",
    "    pmts_hit_nue.append(pm)\n",
    "    doms_hit_nue.append(dm)\n",
    "\n",
    "pmts_hit_nue = np.asarray(pmts_hit_nue)\n",
    "doms_hit_nue = np.asarray(doms_hit_nue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Event selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  select events with at least 8 doms hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doms_hit_sel = []\n",
    "for evt in range(doms_hit_numu.size):\n",
    "    if len(np.unique(doms_hit_numu[evt]))>=8:\n",
    "        doms_hit_sel.append(np.unique(doms_hit_numu[evt]))\n",
    "doms_hit_sel = np.asarray(doms_hit_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### create Data structure with the id of the hit DOMs and the discretized times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tslices(times_numu, times_nue):\n",
    "    min_t_nue = np.min(np.hstack(times_nue))\n",
    "    max_t_nue = np.max(np.hstack(times_nue))\n",
    "    min_t_numu = np.min(np.hstack(times_numu))\n",
    "    max_t_numu= np.max(np.hstack(times_numu))\n",
    "    t_range_min = np.min((min_t_nue, min_t_numu))\n",
    "    t_range_max = np.max((max_t_nue, max_t_numu))\n",
    "    timeslices = np.arange(t_range_min, t_range_max+200, 150)\n",
    "    print(min_t_nue, min_t_numu, max_t_nue, max_t_numu, t_range_min, t_range_max)\n",
    "    return timeslices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tslices_multi_files(times_numu1, times_nue1, times_numu2, times_nue2, times_numu3, times_nue3):\n",
    "    min_t_nue1 = np.min(np.hstack(times_nue1))\n",
    "    max_t_nue1 = np.max(np.hstack(times_nue1))\n",
    "    min_t_numu1 = np.min(np.hstack(times_numu1))\n",
    "    max_t_numu1 = np.max(np.hstack(times_numu1))\n",
    "                        \n",
    "    min_t_nue2 = np.min(np.hstack(times_nue2))\n",
    "    max_t_nue2 = np.max(np.hstack(times_nue2))\n",
    "    min_t_numu2 = np.min(np.hstack(times_numu2))\n",
    "    max_t_numu2 = np.max(np.hstack(times_numu2))\n",
    "                        \n",
    "    min_t_nue3 = np.min(np.hstack(times_nue3))\n",
    "    max_t_nue3 = np.max(np.hstack(times_nue3))\n",
    "    min_t_numu3 = np.min(np.hstack(times_numu3))\n",
    "    max_t_numu3 = np.max(np.hstack(times_numu3))\n",
    "    \n",
    "    t_range_min = np.min((min_t_nue1, min_t_numu1, min_t_nue2, min_t_numu2, min_t_nue3, min_t_numu3 ))\n",
    "    t_range_max = np.max((max_t_nue1, max_t_numu1, max_t_nue2, max_t_numu2, max_t_nue3, max_t_numu3))\n",
    "    timeslices = np.arange(t_range_min, t_range_max+200, 150)\n",
    "    print(t_range_min, t_range_max)\n",
    "    return timeslices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49997887.0, 49993098.0, 50000747.0, 50004225.0, 49993098.0, 50004225.0)\n"
     ]
    }
   ],
   "source": [
    "timeslice = tslices(times_numu, times_nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49993098.0, 50004225.0)\n"
     ]
    }
   ],
   "source": [
    "timeslice = tslices_multi_files(times_numu1, times_nue1, times_numu2, times_nue2, times_numu3, times_nue3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49993098.,  49993248.,  49993398.,  49993548.,  49993698.,\n",
       "        49993848.,  49993998.,  49994148.,  49994298.,  49994448.,\n",
       "        49994598.,  49994748.,  49994898.,  49995048.,  49995198.,\n",
       "        49995348.,  49995498.,  49995648.,  49995798.,  49995948.,\n",
       "        49996098.,  49996248.,  49996398.,  49996548.,  49996698.,\n",
       "        49996848.,  49996998.,  49997148.,  49997298.,  49997448.,\n",
       "        49997598.,  49997748.,  49997898.,  49998048.,  49998198.,\n",
       "        49998348.,  49998498.,  49998648.,  49998798.,  49998948.,\n",
       "        49999098.,  49999248.,  49999398.,  49999548.,  49999698.,\n",
       "        49999848.,  49999998.,  50000148.,  50000298.,  50000448.,\n",
       "        50000598.,  50000748.,  50000898.,  50001048.,  50001198.,\n",
       "        50001348.,  50001498.,  50001648.,  50001798.,  50001948.,\n",
       "        50002098.,  50002248.,  50002398.,  50002548.,  50002698.,\n",
       "        50002848.,  50002998.,  50003148.,  50003298.,  50003448.,\n",
       "        50003598.,  50003748.,  50003898.,  50004048.,  50004198.,\n",
       "        50004348.])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeslice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeslice.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in times_numu:\n",
    "    for j in i:\n",
    "        if j > np.max(timeslice):\n",
    "            print j, np.max(timeslice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50004348.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(timeslice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function creates X with dimension (nevents, timeslice.shape, 2070)\n",
    "def X_creation(timeslices, dom_id, trig, times):\n",
    "    numu_events = dom_id.shape[0]\n",
    "    n_timeslices = timeslices.shape[0] - 1\n",
    "\n",
    "    X_nu = np.zeros((numu_events, n_timeslices, ndoms))\n",
    "\n",
    "    # timeslices = np.arange.....\n",
    "\n",
    "    # Iterate on events\n",
    "    # Get Hit count for each timeslice\n",
    "    for evt in range(numu_events):\n",
    "        # Get all DOM ids for all triggered hits in current event\n",
    "        triggered_dom_ids = (dom_id[evt][trig[evt] == True]) - 1\n",
    "        #print(\"triggered_dom_ids\", triggered_dom_ids)\n",
    "        times_event_hits = times[evt] # select only hits for current event \n",
    "        #print(\"times_event_hits\", times_event_hits, times_event_hits.shape[0])\n",
    "        \n",
    "        for ts, tslice in enumerate(zip(timeslices[:-1], timeslices[1:])):\n",
    "            low, high = tslice\n",
    "            #print(ts, \"low\", low, \"high\", high)\n",
    "            # hits will hold indices of hits matching the condition of being in the selected timeslice\n",
    "            hits = np.where((times_event_hits >= low) & (times_event_hits < high))[0] \n",
    "            #print('hits: ', hits)\n",
    "            #continue\n",
    "            if not len(hits):\n",
    "                continue\n",
    "\n",
    "            # Get all DOM ids associated to all hits in current time slice.\n",
    "            dom_hit_in_slice = triggered_dom_ids[hits]\n",
    "            #print(\"doms_hit_in_slice\", dom_hit_in_slice)\n",
    "            #print(\"hits\", hits)\n",
    "            #print(hits.shape)\n",
    "            # Activate all DOMs for current event, timeslice.\n",
    "            X_nu[evt, ts, dom_hit_in_slice] = 1\n",
    "    return X_nu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_numu = X_creation(timeslice, dom_id_numu, trig_numu, times_numu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_nue = X_creation(timeslice, dom_id_nue, trig_nue, times_nue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_numu = np.ones(dom_id_numu.shape[0])\n",
    "Y_nue = np.zeros(dom_id_nue.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alternative definition of data structure\n",
    "#### DOMs are now organized in strings and floors\n",
    "### let\"s try this with a convolutional neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def X_creation_fl_str(timeslices, dom_id, trig, times):\n",
    "    numu_events = dom_id.shape[0]\n",
    "    n_timeslices = timeslices.shape[0] - 1\n",
    "    X_nu = np.zeros((numu_events, n_timeslices, NSTRING, NFLOORS))\n",
    "    # Iterate on events\n",
    "    # Get Hit count for each timeslice\n",
    "    for evt in range(numu_events):\n",
    "        # Get all DOM ids for all triggered hits in current event\n",
    "        triggered_dom_ids = (dom_id[evt][trig[evt] == True]) - 1\n",
    "        triggered_fl_ids = triggered_dom_ids%18\n",
    "        triggered_str_ids = triggered_dom_ids/18\n",
    "        times_event_hits = times[evt] # select only hits for current event \n",
    "        for ts, tslice in enumerate(zip(timeslices[:-1], timeslices[1:])):\n",
    "            low, high = tslice\n",
    "            # hits will hold indices of hits matching the condition of being in the selected timeslice\n",
    "            hits = np.where((times_event_hits >= low) & (times_event_hits < high))[0] \n",
    "            if not len(hits):\n",
    "                continue\n",
    "            # Get all DOM ids associated to all hits in current time slice.\n",
    "            dom_hit_in_slice = triggered_dom_ids[hits]\n",
    "            fl_hit_in_slice = triggered_fl_ids[hits]\n",
    "            str_hit_in_slice = triggered_str_ids[hits]\n",
    "    \n",
    "            # Activate all DOMs for current event, timeslice.\n",
    "            X_nu[evt, ts, str_hit_in_slice, fl_hit_in_slice] += 1\n",
    "    return X_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_numu = X_creation_fl_str(timeslice, dom_id_numu, trig_numu, times_numu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_nue = X_creation_fl_str(timeslice, dom_id_nue, trig_nue, times_nue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mutiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_numu1 = X_creation_fl_str(timeslice, dom_id_numu1, trig_numu1, times_numu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_nue1 = X_creation_fl_str(timeslice, dom_id_nue1, trig_nue1, times_nue1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_numu2 = X_creation_fl_str(timeslice, dom_id_numu2, trig_numu2, times_numu2)\n",
    "X_nue2 = X_creation_fl_str(timeslice, dom_id_nue2, trig_nue2, times_nue2)\n",
    "X_numu3 = X_creation_fl_str(timeslice, dom_id_numu3, trig_numu3, times_numu3)\n",
    "X_nue3 = X_creation_fl_str(timeslice, dom_id_nue3, trig_nue3, times_nue3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1541, 75, 115, 18)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_numu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183, 75, 115, 18)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nue.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create Y:\n",
    "- Y=1 for numu\n",
    "- Y=0 for nue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_numu = np.ones(dom_id_numu.shape[0])\n",
    "Y_nue = np.zeros(dom_id_nue.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_numu1 = np.ones(dom_id_numu1.shape[0])\n",
    "Y_nue1 = np.zeros(dom_id_nue1.shape[0])\n",
    "\n",
    "Y_numu2 = np.ones(dom_id_numu2.shape[0])\n",
    "Y_nue2 = np.zeros(dom_id_nue2.shape[0])\n",
    "\n",
    "Y_numu3 = np.ones(dom_id_numu3.shape[0])\n",
    "Y_nue3 = np.zeros(dom_id_nue3.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the shape of the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_numu: ', (1541, 75, 115, 18), 'X_nue: ', (1183, 75, 115, 18), 'Y_numu: ', (1541,), 'Y_nue: ', (1183,))\n"
     ]
    }
   ],
   "source": [
    "#single file\n",
    "print('X_numu: ', X_numu.shape, 'X_nue: ', X_nue.shape, 'Y_numu: ', Y_numu.shape, 'Y_nue: ', Y_nue.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_numu: ', (1541, 75, 115, 18), 'X_nue: ', (1183, 75, 115, 18), 'Y_numu: ', (1541,), 'Y_nue: ', (1183,))\n"
     ]
    }
   ],
   "source": [
    "print('X_numu: ', X_numu1.shape, 'X_nue: ', X_nue1.shape, 'Y_numu: ', Y_numu1.shape, 'Y_nue: ', Y_nue1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_numu: ', (1609, 75, 115, 18), 'X_nue: ', (1220, 75, 115, 18), 'Y_numu: ', (1609,), 'Y_nue: ', (1220,))\n"
     ]
    }
   ],
   "source": [
    "print('X_numu: ', X_numu2.shape, 'X_nue: ', X_nue2.shape, 'Y_numu: ', Y_numu2.shape, 'Y_nue: ', Y_nue2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_numu: ', (1613, 75, 115, 18), 'X_nue: ', (1234, 75, 115, 18), 'Y_numu: ', (1613,), 'Y_nue: ', (1234,))\n"
     ]
    }
   ],
   "source": [
    "print('X_numu: ', X_numu3.shape, 'X_nue: ', X_nue3.shape, 'Y_numu: ', Y_numu3.shape, 'Y_nue: ', Y_nue3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the X matrix to output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('numu_nue_data.npz', X_numu=X_numu, X_nue=X_nue, y_numu=Y_numu, y_nue=Y_nue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memo to load back data from `.npz` compressed file.\n",
    "\n",
    "```python\n",
    "#load\n",
    "data = np.load('X_data.npz')\n",
    "X_numu = data.files['X_numu']\n",
    "X_nue = data.files['X_nue']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append nue to numu data, without mixing : from now on, data will be X an Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.vstack((X_numu, X_nue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.concatenate((Y_numu, Y_nue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_numu = np.vstack((X_numu1, X_numu2, X_numu3))\n",
    "X_nue = np.vstack((X_nue1, X_nue2, X_nue3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.vstack((X_numu, X_nue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_numu = np.concatenate((Y_numu1, Y_numu2, Y_numu3))\n",
    "Y_nue = np.concatenate((Y_nue1, Y_nue2, Y_nue3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.concatenate((Y_numu, Y_nue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save to output file X and Y\n",
    "#np.save('X_matrix.npy', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used later in cross validation\n",
    "#y_numu_categ = np_utils.to_categorical(Y_numu, 2)\n",
    "#y_nue_categ = np_utils.to_categorical(Y_nue, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y = np.concatenate((y_numu_categ, y_nue_categ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `sklearn` to split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 780 Ti (CNMeM is disabled, cuDNN not available)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"]= \"mode=FAST_RUN, device=gpu,floatX=float32\"\n",
    "import theano\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start here if using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (2043, 75, 115, 18), 'X_test: ', (681, 75, 115, 18), 'Y_train: ', (2043,), 'Y_test: ', (681,))\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ', X_train.shape, 'X_test: ', X_test.shape, 'Y_train: ', Y_train.shape, 'Y_test: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape to fit Sklearn requirements\n",
    "shape_train = X_train.shape\n",
    "shape_test = X_test.shape\n",
    "X_train = X_train.reshape((shape_train[0], shape_train[1]*shape_train[2]))\n",
    "X_test = X_test.reshape((shape_test[0], shape_test[1]*shape_test[2]))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reset Shapes to original ones\n",
    "X_train = X_train.reshape(shape_train)\n",
    "X_test = X_test.reshape(shape_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_validation_acc = 0.0\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fully connected model. To be used with different dataset (from CNN), because data must be reshaped to 2D\n",
    "def dense_model():\n",
    "    ## TRY different activations, e.g. tanh, sigmoid.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2070, input_shape=((Xtrain.shape[1], Xtrain.shape[2])), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2070, activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  data format for keras (differen in `th` and `tf`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensorflow\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theano\n",
    "img_channel = X_train.shape[1]\n",
    "img_rows = X_train.shape[2]\n",
    "img_cols = X_train.shape[3]\n",
    "input_shape = (img_channel, img_rows, img_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2043, 75, 115, 18)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 115, 18)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(128, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_deep():\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(128, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_deep2():\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(128, kernel_size=(4, 4),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "    model.add(Convolution2D(64, (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution2D(32, (1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_deep3():\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(128, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "   \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dense(2070, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2070, activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Activation, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "def cnn_1d():\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(128, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution1D(64, kernel_size=3, activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Convolution1D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADADELTA: An Adaptive Learning Rate Method\n",
    "\n",
    "Matthew D. Zeiler\n",
    "(Submitted on 22 Dec 2012)\n",
    "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.\n",
    "\n",
    "`keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_nn_model = dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_nn_model = cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_nn_model = cnn_model_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_nn_model = cnn_model_deep2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_nn_model = cnn_model_deep3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_nn_model = cnn_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2043, 75, 115, 18)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape data for FC network\n",
    "X_FC_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2]*X_train.shape[3])\n",
    "X_FC_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2]*X_test.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[1] == 2070, but the output's size on that axis is 75.\nApply node that caused the error: GpuElemwise{Add}[(0, 0)](GpuReshape{3}.0, GpuDimShuffle{x,0,x}.0)\nToposort index: 72\nInputs types: [CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, (True, False, True))]\nInputs shapes: [(115, 75, 2070), (1, 2070, 1)]\nInputs strides: [(155250, 2070, 1), (0, 1, 0)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{(i0 + Abs(i0))},no_inplace}(GpuElemwise{Add}[(0, 0)].0), GpuElemwise{Composite{((i0 * Composite{((i0 * i1 * i2) + i3)}(i1, i2, i3, i4)) + (i0 * Composite{((i0 * i1 * i2) + i3)}(i1, i2, i3, i4) * sgn(i5)))}}[(0, 2)](CudaNdarrayConstant{[[[ 0.5]]]}, CudaNdarrayConstant{[[[ 1.25]]]}, if{inplace,gpu}.0, GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0, if{inplace,gpu}.0, GpuElemwise{Add}[(0, 0)].0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-6445f2d4b6d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     history = model.fit(Xtrain, Ytrain, batch_size=115,\n\u001b[1;32m     13\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         validation_data = (Xvalidation, Yvalidation))\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvalidation_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_validation_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[1] == 2070, but the output's size on that axis is 75.\nApply node that caused the error: GpuElemwise{Add}[(0, 0)](GpuReshape{3}.0, GpuDimShuffle{x,0,x}.0)\nToposort index: 72\nInputs types: [CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, (True, False, True))]\nInputs shapes: [(115, 75, 2070), (1, 2070, 1)]\nInputs strides: [(155250, 2070, 1), (0, 1, 0)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{(i0 + Abs(i0))},no_inplace}(GpuElemwise{Add}[(0, 0)].0), GpuElemwise{Composite{((i0 * Composite{((i0 * i1 * i2) + i3)}(i1, i2, i3, i4)) + (i0 * Composite{((i0 * i1 * i2) + i3)}(i1, i2, i3, i4) * sgn(i5)))}}[(0, 2)](CudaNdarrayConstant{[[[ 0.5]]]}, CudaNdarrayConstant{[[[ 1.25]]]}, if{inplace,gpu}.0, GpuElemwise{Composite{Cast{float32}(LT(i0, i1))}}[(0, 0)].0, if{inplace,gpu}.0, GpuElemwise{Add}[(0, 0)].0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "#Fully connecter netword( w old dataset - ndoms not splitted in strings and floors)\n",
    "\n",
    "for i, (train_index, validation_index) in enumerate(skf.split(X_FC_train, Y_train)):\n",
    "    print(i+1)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_FC_train[train_index], X_FC_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=115,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_FC_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.6697 - acc: 0.5624 - val_loss: 0.6137 - val_acc: 0.8093\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.5701 - acc: 0.7485 - val_loss: 0.4309 - val_acc: 0.8582\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3880 - acc: 0.8415 - val_loss: 0.3722 - val_acc: 0.8729\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3311 - acc: 0.8770 - val_loss: 0.3951 - val_acc: 0.8631\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2998 - acc: 0.8788 - val_loss: 0.3402 - val_acc: 0.8924\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2635 - acc: 0.8966 - val_loss: 0.3686 - val_acc: 0.8973\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2614 - acc: 0.8966 - val_loss: 0.3166 - val_acc: 0.9022\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2466 - acc: 0.9045 - val_loss: 0.3229 - val_acc: 0.8924\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2348 - acc: 0.9113 - val_loss: 0.3152 - val_acc: 0.8753\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2155 - acc: 0.9168 - val_loss: 0.3309 - val_acc: 0.8900\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2527 - acc: 0.8984 - val_loss: 0.2895 - val_acc: 0.8924\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1971 - acc: 0.9198 - val_loss: 0.2869 - val_acc: 0.8949\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1953 - acc: 0.9192 - val_loss: 0.3103 - val_acc: 0.8924\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1963 - acc: 0.9204 - val_loss: 0.2982 - val_acc: 0.8973\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2119 - acc: 0.9223 - val_loss: 0.2779 - val_acc: 0.8973\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.1835 - acc: 0.9302 - val_loss: 0.2740 - val_acc: 0.9022\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1829 - acc: 0.9321 - val_loss: 0.2911 - val_acc: 0.8900\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2817 - acc: 0.8984 - val_loss: 0.2768 - val_acc: 0.8875\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1721 - acc: 0.9321 - val_loss: 0.2793 - val_acc: 0.8998\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1653 - acc: 0.9315 - val_loss: 0.2846 - val_acc: 0.8949\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.6583 - acc: 0.6144 - val_loss: 0.5737 - val_acc: 0.8191\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.5041 - acc: 0.7729 - val_loss: 0.4166 - val_acc: 0.8313\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3695 - acc: 0.8550 - val_loss: 0.3806 - val_acc: 0.8264\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3279 - acc: 0.8672 - val_loss: 0.3320 - val_acc: 0.8680\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2876 - acc: 0.8886 - val_loss: 0.3107 - val_acc: 0.8631\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2520 - acc: 0.9039 - val_loss: 0.3039 - val_acc: 0.8704\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2431 - acc: 0.9100 - val_loss: 0.3034 - val_acc: 0.8753\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2347 - acc: 0.9180 - val_loss: 0.3015 - val_acc: 0.8606\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2105 - acc: 0.9247 - val_loss: 0.4003 - val_acc: 0.8704\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2115 - acc: 0.9247 - val_loss: 0.2947 - val_acc: 0.8753\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1968 - acc: 0.9278 - val_loss: 0.3546 - val_acc: 0.8778\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1988 - acc: 0.9272 - val_loss: 0.3012 - val_acc: 0.8680\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2016 - acc: 0.9235 - val_loss: 0.3145 - val_acc: 0.8826\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1898 - acc: 0.9284 - val_loss: 0.3341 - val_acc: 0.8851\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1805 - acc: 0.9357 - val_loss: 0.2923 - val_acc: 0.8924\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1872 - acc: 0.9315 - val_loss: 0.3019 - val_acc: 0.8826\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1776 - acc: 0.9351 - val_loss: 0.2904 - val_acc: 0.8729\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1727 - acc: 0.9400 - val_loss: 0.2798 - val_acc: 0.8875\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1718 - acc: 0.9406 - val_loss: 0.2902 - val_acc: 0.8949\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1683 - acc: 0.9412 - val_loss: 0.3022 - val_acc: 0.8851\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([ 813,  815,  819,  820,  821,  822,  824,  825,  826,  827,  828,\n",
      "        829,  830,  831,  832,  833,  834,  835,  836,  837,  838,  839,\n",
      "        840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081,\n",
      "       1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092,\n",
      "       1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103,\n",
      "       1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125,\n",
      "       1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136,\n",
      "       1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "       1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n",
      "       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169,\n",
      "       1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,\n",
      "       1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "       1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202,\n",
      "       1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1213, 1214,\n",
      "       1215, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1229, 1230, 1233,\n",
      "       1236, 1237]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.6596 - acc: 0.5894 - val_loss: 0.5972 - val_acc: 0.5795\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.5214 - acc: 0.7528 - val_loss: 0.3636 - val_acc: 0.8924\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.4766 - acc: 0.8299 - val_loss: 0.3922 - val_acc: 0.8215\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3340 - acc: 0.8641 - val_loss: 0.3410 - val_acc: 0.8557\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.3060 - acc: 0.8703 - val_loss: 0.3297 - val_acc: 0.8533\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2792 - acc: 0.8862 - val_loss: 0.3067 - val_acc: 0.8826\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.2786 - acc: 0.9002 - val_loss: 0.3080 - val_acc: 0.8826\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2690 - acc: 0.8911 - val_loss: 0.2975 - val_acc: 0.8826\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2713 - acc: 0.8831 - val_loss: 0.2882 - val_acc: 0.8826\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2239 - acc: 0.9045 - val_loss: 0.2671 - val_acc: 0.8998\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2208 - acc: 0.9119 - val_loss: 0.2982 - val_acc: 0.8949\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2160 - acc: 0.9125 - val_loss: 0.2884 - val_acc: 0.8949\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2100 - acc: 0.9106 - val_loss: 0.2792 - val_acc: 0.9022\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2071 - acc: 0.9211 - val_loss: 0.2485 - val_acc: 0.9071\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2083 - acc: 0.9143 - val_loss: 0.2689 - val_acc: 0.8998\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2038 - acc: 0.9186 - val_loss: 0.2611 - val_acc: 0.8998\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1827 - acc: 0.9223 - val_loss: 0.3031 - val_acc: 0.8778\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1863 - acc: 0.9211 - val_loss: 0.2631 - val_acc: 0.9095\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1812 - acc: 0.9247 - val_loss: 0.3179 - val_acc: 0.8753\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.1772 - acc: 0.9296 - val_loss: 0.2529 - val_acc: 0.9144\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([1209, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1234, 1235,\n",
      "       1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248,\n",
      "       1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,\n",
      "       1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270,\n",
      "       1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281,\n",
      "       1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292,\n",
      "       1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "       1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,\n",
      "       1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325,\n",
      "       1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,\n",
      "       1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "       1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358,\n",
      "       1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369,\n",
      "       1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380,\n",
      "       1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391,\n",
      "       1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402,\n",
      "       1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413,\n",
      "       1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424,\n",
      "       1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "       1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446,\n",
      "       1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457,\n",
      "       1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468,\n",
      "       1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "       1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490,\n",
      "       1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501,\n",
      "       1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512,\n",
      "       1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523,\n",
      "       1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534,\n",
      "       1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
      "       1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556,\n",
      "       1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567,\n",
      "       1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578,\n",
      "       1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589,\n",
      "       1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600,\n",
      "       1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611,\n",
      "       1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622,\n",
      "       1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1633, 1635,\n",
      "       1637, 1638]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.6635 - acc: 0.5789 - val_loss: 0.6245 - val_acc: 0.7506\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.4927 - acc: 0.8042 - val_loss: 0.4366 - val_acc: 0.8582\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3588 - acc: 0.8666 - val_loss: 0.3081 - val_acc: 0.8753\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3433 - acc: 0.8611 - val_loss: 0.2965 - val_acc: 0.9022\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.2965 - acc: 0.8856 - val_loss: 0.2730 - val_acc: 0.9022\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.2766 - acc: 0.8929 - val_loss: 0.2515 - val_acc: 0.9169\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2560 - acc: 0.9027 - val_loss: 0.3784 - val_acc: 0.8557\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.3680 - acc: 0.8690 - val_loss: 0.2566 - val_acc: 0.9071\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2351 - acc: 0.9045 - val_loss: 0.2412 - val_acc: 0.9095\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2283 - acc: 0.9106 - val_loss: 0.2410 - val_acc: 0.9120\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2152 - acc: 0.9143 - val_loss: 0.2427 - val_acc: 0.9120\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2411 - acc: 0.9033 - val_loss: 0.2336 - val_acc: 0.9046\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2152 - acc: 0.9168 - val_loss: 0.2294 - val_acc: 0.9095\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.2019 - acc: 0.9204 - val_loss: 0.2322 - val_acc: 0.9046\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1922 - acc: 0.9229 - val_loss: 0.2702 - val_acc: 0.8851\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1954 - acc: 0.9174 - val_loss: 0.2293 - val_acc: 0.9071\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1804 - acc: 0.9284 - val_loss: 0.2250 - val_acc: 0.9169\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 75s - loss: 0.1784 - acc: 0.9284 - val_loss: 0.2229 - val_acc: 0.9046\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 76s - loss: 0.1733 - acc: 0.9327 - val_loss: 0.2295 - val_acc: 0.9046\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 77s - loss: 0.1726 - acc: 0.9241 - val_loss: 0.2310 - val_acc: 0.9120\n",
      "('TRAIN:', array([   0,    1,    2, ..., 1635, 1637, 1638]), 'TEST:', array([1632, 1634, 1636, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
      "       1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
      "       1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668,\n",
      "       1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
      "       1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
      "       1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
      "       1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
      "       1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "       1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
      "       1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745,\n",
      "       1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756,\n",
      "       1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767,\n",
      "       1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778,\n",
      "       1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789,\n",
      "       1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
      "       1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811,\n",
      "       1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822,\n",
      "       1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833,\n",
      "       1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844,\n",
      "       1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,\n",
      "       1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866,\n",
      "       1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877,\n",
      "       1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888,\n",
      "       1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899,\n",
      "       1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910,\n",
      "       1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921,\n",
      "       1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932,\n",
      "       1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943,\n",
      "       1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954,\n",
      "       1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965,\n",
      "       1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976,\n",
      "       1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
      "       1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
      "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
      "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n",
      "       2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n",
      "       2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 76s - loss: 0.6600 - acc: 0.6137 - val_loss: 0.5982 - val_acc: 0.7862\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.4732 - acc: 0.8026 - val_loss: 0.3476 - val_acc: 0.8624\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 76s - loss: 0.3560 - acc: 0.8576 - val_loss: 0.3375 - val_acc: 0.8477\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 76s - loss: 0.3094 - acc: 0.8845 - val_loss: 0.3030 - val_acc: 0.8722\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 76s - loss: 0.2659 - acc: 0.8930 - val_loss: 0.2942 - val_acc: 0.8919\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.2656 - acc: 0.8943 - val_loss: 0.2799 - val_acc: 0.8919\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.2411 - acc: 0.9077 - val_loss: 0.2938 - val_acc: 0.8796\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.2322 - acc: 0.9040 - val_loss: 0.2758 - val_acc: 0.8968\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.2143 - acc: 0.9144 - val_loss: 0.2749 - val_acc: 0.8968\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 76s - loss: 0.2146 - acc: 0.9120 - val_loss: 0.2680 - val_acc: 0.9042\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.2190 - acc: 0.9095 - val_loss: 0.2708 - val_acc: 0.8919\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.2061 - acc: 0.9101 - val_loss: 0.2603 - val_acc: 0.9066\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.1921 - acc: 0.9175 - val_loss: 0.2677 - val_acc: 0.9140\n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.1946 - acc: 0.9291 - val_loss: 0.2833 - val_acc: 0.9042\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.1840 - acc: 0.9260 - val_loss: 0.2749 - val_acc: 0.9042\n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.1777 - acc: 0.9273 - val_loss: 0.2708 - val_acc: 0.9140\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 76s - loss: 0.1744 - acc: 0.9279 - val_loss: 0.2692 - val_acc: 0.9115\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.1723 - acc: 0.9334 - val_loss: 0.2715 - val_acc: 0.9066\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.1811 - acc: 0.9260 - val_loss: 0.2871 - val_acc: 0.8993\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636/1636 [==============================] - 75s - loss: 0.1699 - acc: 0.9340 - val_loss: 0.3184 - val_acc: 0.8968\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1974 - acc: 0.9211    \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1954 - acc: 0.9254    \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1918 - acc: 0.9285    \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1842 - acc: 0.9236    \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1771 - acc: 0.9340    \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1753 - acc: 0.9267    \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1762 - acc: 0.9315    \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1682 - acc: 0.9407    \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1637 - acc: 0.9407    \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1635 - acc: 0.9401    \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1631 - acc: 0.9352    \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1534 - acc: 0.9425    \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1500 - acc: 0.9401    \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1525 - acc: 0.9432    \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1516 - acc: 0.9450    \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1508 - acc: 0.9419    \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1414 - acc: 0.9505    \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1411 - acc: 0.9505    \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1482 - acc: 0.9456    \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 69s - loss: 0.1347 - acc: 0.9542    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92364170337738616"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn first attempt\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n",
    "num_classes = 2\n",
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `accuracy_score(Y_test, cls_predictions)`:0.92364170337738616 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.6627 - acc: 0.5961 - val_loss: 0.6838 - val_acc: 0.5770\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.5294 - acc: 0.7528 - val_loss: 0.3998 - val_acc: 0.8484\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.4011 - acc: 0.8366 - val_loss: 0.3636 - val_acc: 0.8729\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3345 - acc: 0.8703 - val_loss: 0.3575 - val_acc: 0.8753\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2906 - acc: 0.8911 - val_loss: 0.3255 - val_acc: 0.8973\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2430 - acc: 0.9064 - val_loss: 0.3109 - val_acc: 0.8973\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 73s - loss: 0.2289 - acc: 0.9125 - val_loss: 0.3123 - val_acc: 0.8973\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2196 - acc: 0.9149 - val_loss: 0.3225 - val_acc: 0.8900\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2097 - acc: 0.9223 - val_loss: 0.3061 - val_acc: 0.9022\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1949 - acc: 0.9272 - val_loss: 0.3050 - val_acc: 0.8998\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1954 - acc: 0.9278 - val_loss: 0.2839 - val_acc: 0.8998\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1795 - acc: 0.9339 - val_loss: 0.2818 - val_acc: 0.8998\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1763 - acc: 0.9308 - val_loss: 0.2841 - val_acc: 0.9095\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1753 - acc: 0.9296 - val_loss: 0.2906 - val_acc: 0.8998\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1664 - acc: 0.9388 - val_loss: 0.2847 - val_acc: 0.9046\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1697 - acc: 0.9370 - val_loss: 0.2794 - val_acc: 0.9046\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1551 - acc: 0.9388 - val_loss: 0.2903 - val_acc: 0.9022\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1545 - acc: 0.9394 - val_loss: 0.2968 - val_acc: 0.9022\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1602 - acc: 0.9364 - val_loss: 0.3029 - val_acc: 0.9046\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1438 - acc: 0.9486 - val_loss: 0.3072 - val_acc: 0.8998\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.6560 - acc: 0.6126 - val_loss: 0.5922 - val_acc: 0.7824\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 73s - loss: 0.4995 - acc: 0.7785 - val_loss: 0.3992 - val_acc: 0.8411\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.4095 - acc: 0.8476 - val_loss: 0.3856 - val_acc: 0.8362\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3507 - acc: 0.8684 - val_loss: 0.3988 - val_acc: 0.8362\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3089 - acc: 0.8794 - val_loss: 0.3965 - val_acc: 0.8631\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2723 - acc: 0.9064 - val_loss: 0.3170 - val_acc: 0.8582\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2453 - acc: 0.9100 - val_loss: 0.2948 - val_acc: 0.8753\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2319 - acc: 0.9143 - val_loss: 0.2714 - val_acc: 0.8802\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2160 - acc: 0.9223 - val_loss: 0.2705 - val_acc: 0.8729\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2004 - acc: 0.9290 - val_loss: 0.2670 - val_acc: 0.8753\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1957 - acc: 0.9259 - val_loss: 0.2668 - val_acc: 0.8851\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3089 - acc: 0.8966 - val_loss: 0.2990 - val_acc: 0.8802\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1909 - acc: 0.9278 - val_loss: 0.2688 - val_acc: 0.8753\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1860 - acc: 0.9211 - val_loss: 0.2736 - val_acc: 0.8704\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1718 - acc: 0.9382 - val_loss: 0.3817 - val_acc: 0.8851\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1700 - acc: 0.9321 - val_loss: 0.2806 - val_acc: 0.8826\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1591 - acc: 0.9370 - val_loss: 0.2737 - val_acc: 0.8802\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1640 - acc: 0.9431 - val_loss: 0.2887 - val_acc: 0.8875\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1528 - acc: 0.9412 - val_loss: 0.2738 - val_acc: 0.8802\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1519 - acc: 0.9437 - val_loss: 0.2781 - val_acc: 0.8802\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([ 813,  815,  819,  820,  821,  822,  824,  825,  826,  827,  828,\n",
      "        829,  830,  831,  832,  833,  834,  835,  836,  837,  838,  839,\n",
      "        840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081,\n",
      "       1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092,\n",
      "       1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103,\n",
      "       1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125,\n",
      "       1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136,\n",
      "       1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "       1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n",
      "       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169,\n",
      "       1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,\n",
      "       1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "       1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202,\n",
      "       1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1213, 1214,\n",
      "       1215, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1229, 1230, 1233,\n",
      "       1236, 1237]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 73s - loss: 0.6541 - acc: 0.6206 - val_loss: 0.5993 - val_acc: 0.7188\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.4717 - acc: 0.7913 - val_loss: 0.3881 - val_acc: 0.8264\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3699 - acc: 0.8464 - val_loss: 0.3855 - val_acc: 0.8533\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3585 - acc: 0.8641 - val_loss: 0.3291 - val_acc: 0.8875\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3003 - acc: 0.8819 - val_loss: 0.2853 - val_acc: 0.8802\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2622 - acc: 0.9009 - val_loss: 0.2897 - val_acc: 0.8924\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2433 - acc: 0.9064 - val_loss: 0.2698 - val_acc: 0.8900\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2269 - acc: 0.9051 - val_loss: 0.3128 - val_acc: 0.8851\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2207 - acc: 0.9143 - val_loss: 0.3077 - val_acc: 0.8851\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2027 - acc: 0.9204 - val_loss: 0.2478 - val_acc: 0.9022\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1830 - acc: 0.9308 - val_loss: 0.2477 - val_acc: 0.9169\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2824 - acc: 0.8935 - val_loss: 0.2644 - val_acc: 0.8949\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1838 - acc: 0.9272 - val_loss: 0.2387 - val_acc: 0.9144\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1775 - acc: 0.9308 - val_loss: 0.2454 - val_acc: 0.9169\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1701 - acc: 0.9364 - val_loss: 0.2547 - val_acc: 0.9071\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1653 - acc: 0.9364 - val_loss: 0.2376 - val_acc: 0.9046\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1688 - acc: 0.9339 - val_loss: 0.2404 - val_acc: 0.9120\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1601 - acc: 0.9382 - val_loss: 0.2397 - val_acc: 0.9095\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1571 - acc: 0.9382 - val_loss: 0.2395 - val_acc: 0.9144\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1616 - acc: 0.9345 - val_loss: 0.2990 - val_acc: 0.9071\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([1209, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1234, 1235,\n",
      "       1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248,\n",
      "       1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,\n",
      "       1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270,\n",
      "       1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281,\n",
      "       1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292,\n",
      "       1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "       1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,\n",
      "       1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325,\n",
      "       1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,\n",
      "       1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "       1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358,\n",
      "       1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369,\n",
      "       1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380,\n",
      "       1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391,\n",
      "       1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402,\n",
      "       1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413,\n",
      "       1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424,\n",
      "       1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "       1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446,\n",
      "       1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457,\n",
      "       1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468,\n",
      "       1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "       1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490,\n",
      "       1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501,\n",
      "       1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512,\n",
      "       1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523,\n",
      "       1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534,\n",
      "       1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
      "       1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556,\n",
      "       1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567,\n",
      "       1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578,\n",
      "       1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589,\n",
      "       1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600,\n",
      "       1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611,\n",
      "       1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622,\n",
      "       1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1633, 1635,\n",
      "       1637, 1638]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.6489 - acc: 0.6297 - val_loss: 0.5592 - val_acc: 0.6528\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.4754 - acc: 0.7901 - val_loss: 0.4722 - val_acc: 0.8289\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3818 - acc: 0.8476 - val_loss: 0.3460 - val_acc: 0.8851\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3756 - acc: 0.8525 - val_loss: 0.3608 - val_acc: 0.8875\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.5591 - acc: 0.8464 - val_loss: 0.3578 - val_acc: 0.8875\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.3013 - acc: 0.8917 - val_loss: 0.3060 - val_acc: 0.9022\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2717 - acc: 0.8972 - val_loss: 0.2925 - val_acc: 0.9120\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2548 - acc: 0.8929 - val_loss: 0.2862 - val_acc: 0.8704\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2579 - acc: 0.8990 - val_loss: 0.2634 - val_acc: 0.9071\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2242 - acc: 0.9113 - val_loss: 0.2383 - val_acc: 0.9267\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2243 - acc: 0.9119 - val_loss: 0.2470 - val_acc: 0.9144\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2110 - acc: 0.9131 - val_loss: 0.2700 - val_acc: 0.8998\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2051 - acc: 0.9198 - val_loss: 0.2368 - val_acc: 0.9193\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.2007 - acc: 0.9198 - val_loss: 0.2495 - val_acc: 0.9095\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1886 - acc: 0.9266 - val_loss: 0.2290 - val_acc: 0.9169\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1778 - acc: 0.9253 - val_loss: 0.2011 - val_acc: 0.9315\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1821 - acc: 0.9266 - val_loss: 0.2069 - val_acc: 0.9242\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1646 - acc: 0.9357 - val_loss: 0.2321 - val_acc: 0.9291\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1715 - acc: 0.9296 - val_loss: 0.2041 - val_acc: 0.9267\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 74s - loss: 0.1679 - acc: 0.9327 - val_loss: 0.1922 - val_acc: 0.9389\n",
      "('TRAIN:', array([   0,    1,    2, ..., 1635, 1637, 1638]), 'TEST:', array([1632, 1634, 1636, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
      "       1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
      "       1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668,\n",
      "       1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
      "       1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
      "       1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
      "       1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
      "       1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "       1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
      "       1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745,\n",
      "       1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756,\n",
      "       1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767,\n",
      "       1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778,\n",
      "       1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789,\n",
      "       1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
      "       1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811,\n",
      "       1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822,\n",
      "       1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833,\n",
      "       1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844,\n",
      "       1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,\n",
      "       1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866,\n",
      "       1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877,\n",
      "       1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888,\n",
      "       1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899,\n",
      "       1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910,\n",
      "       1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921,\n",
      "       1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932,\n",
      "       1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943,\n",
      "       1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954,\n",
      "       1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965,\n",
      "       1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976,\n",
      "       1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
      "       1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
      "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
      "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n",
      "       2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n",
      "       2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 33, 53, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 31, 51, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 50592)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 101186    \n",
      "=================================================================\n",
      "Total params: 223,554\n",
      "Trainable params: 223,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.6612 - acc: 0.5978 - val_loss: 0.5932 - val_acc: 0.7518\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 75s - loss: 0.5662 - acc: 0.7586 - val_loss: 0.4350 - val_acc: 0.8428\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.4215 - acc: 0.8130 - val_loss: 0.4417 - val_acc: 0.8305\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.3547 - acc: 0.8588 - val_loss: 0.3680 - val_acc: 0.8624\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.3162 - acc: 0.8704 - val_loss: 0.3301 - val_acc: 0.8747\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.2809 - acc: 0.8998 - val_loss: 0.3123 - val_acc: 0.8698\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.2500 - acc: 0.9077 - val_loss: 0.3040 - val_acc: 0.8943\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.2493 - acc: 0.8924 - val_loss: 0.3040 - val_acc: 0.8943\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.2274 - acc: 0.9150 - val_loss: 0.3185 - val_acc: 0.8821\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.2220 - acc: 0.9120 - val_loss: 0.2733 - val_acc: 0.8943\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.2151 - acc: 0.9126 - val_loss: 0.2760 - val_acc: 0.9042\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1947 - acc: 0.9260 - val_loss: 0.2837 - val_acc: 0.8943\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1866 - acc: 0.9297 - val_loss: 0.2855 - val_acc: 0.9066\n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1871 - acc: 0.9273 - val_loss: 0.2794 - val_acc: 0.9238\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1912 - acc: 0.9267 - val_loss: 0.2835 - val_acc: 0.9115\n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1801 - acc: 0.9328 - val_loss: 0.2793 - val_acc: 0.9115\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1742 - acc: 0.9328 - val_loss: 0.2950 - val_acc: 0.9091\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1703 - acc: 0.9328 - val_loss: 0.2992 - val_acc: 0.9017\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 74s - loss: 0.1703 - acc: 0.9346 - val_loss: 0.2941 - val_acc: 0.9066\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636/1636 [==============================] - 74s - loss: 0.1644 - acc: 0.9383 - val_loss: 0.2857 - val_acc: 0.9115\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1837 - acc: 0.9260    \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1813 - acc: 0.9273    \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1717 - acc: 0.9309    \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1691 - acc: 0.9334    \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1609 - acc: 0.9328    \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1592 - acc: 0.9328    \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1511 - acc: 0.9383    \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1465 - acc: 0.9419    \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1481 - acc: 0.9438    \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1443 - acc: 0.9425    \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1399 - acc: 0.9456    \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1392 - acc: 0.9456    \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1435 - acc: 0.9480    \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1323 - acc: 0.9450    \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1285 - acc: 0.9554    \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1276 - acc: 0.9535    \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1244 - acc: 0.9535    \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1240 - acc: 0.9523    \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.1211 - acc: 0.9511    \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1112 - acc: 0.9554    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90748898678414092"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn first attempt - new dataset\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n",
    "num_classes = 2\n",
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.90748898678414092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.6894 - acc: 0.5465 - val_loss: 0.6848 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.6760 - acc: 0.5673 - val_loss: 0.6700 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.6422 - acc: 0.5673 - val_loss: 0.6040 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.5574 - acc: 0.5979 - val_loss: 0.5389 - val_acc: 0.7457\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.4411 - acc: 0.8531 - val_loss: 0.3901 - val_acc: 0.8533\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.3767 - acc: 0.8421 - val_loss: 0.3469 - val_acc: 0.8704\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.3053 - acc: 0.8874 - val_loss: 0.3301 - val_acc: 0.8826\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2940 - acc: 0.8849 - val_loss: 0.5391 - val_acc: 0.8337\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3004 - acc: 0.8843 - val_loss: 0.3478 - val_acc: 0.8949\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2436 - acc: 0.9088 - val_loss: 0.3223 - val_acc: 0.9071\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2299 - acc: 0.9162 - val_loss: 0.3049 - val_acc: 0.9046\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2323 - acc: 0.9106 - val_loss: 0.3010 - val_acc: 0.9095\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2149 - acc: 0.9186 - val_loss: 0.2930 - val_acc: 0.9095\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2033 - acc: 0.9229 - val_loss: 0.3032 - val_acc: 0.8924\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2019 - acc: 0.9211 - val_loss: 0.2928 - val_acc: 0.9144\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1883 - acc: 0.9308 - val_loss: 0.2771 - val_acc: 0.9169\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1767 - acc: 0.9296 - val_loss: 0.2900 - val_acc: 0.9169\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1734 - acc: 0.9315 - val_loss: 0.2790 - val_acc: 0.9242\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1558 - acc: 0.9449 - val_loss: 0.3047 - val_acc: 0.9120\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1713 - acc: 0.9370 - val_loss: 0.2649 - val_acc: 0.9218\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6864 - acc: 0.5569 - val_loss: 0.6770 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6772 - acc: 0.5673 - val_loss: 0.6647 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6547 - acc: 0.5673 - val_loss: 0.6169 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5937 - acc: 0.5704 - val_loss: 0.5274 - val_acc: 0.5892\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5501 - acc: 0.7001 - val_loss: 0.5202 - val_acc: 0.8655\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.4638 - acc: 0.8348 - val_loss: 0.3968 - val_acc: 0.8557\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3985 - acc: 0.8372 - val_loss: 0.3580 - val_acc: 0.8509\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3437 - acc: 0.8647 - val_loss: 0.3078 - val_acc: 0.8753\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5926 - acc: 0.8299 - val_loss: 0.3244 - val_acc: 0.8655\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3073 - acc: 0.8782 - val_loss: 0.3072 - val_acc: 0.8729\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2851 - acc: 0.8886 - val_loss: 0.2843 - val_acc: 0.8900\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2772 - acc: 0.8935 - val_loss: 0.2769 - val_acc: 0.8900\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2755 - acc: 0.8917 - val_loss: 0.2751 - val_acc: 0.8875\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2569 - acc: 0.9002 - val_loss: 0.2712 - val_acc: 0.8875\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2507 - acc: 0.9039 - val_loss: 0.2644 - val_acc: 0.8924\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2390 - acc: 0.9076 - val_loss: 0.2599 - val_acc: 0.8973\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2306 - acc: 0.9119 - val_loss: 0.2699 - val_acc: 0.8900\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2234 - acc: 0.9119 - val_loss: 0.2624 - val_acc: 0.8924\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2165 - acc: 0.9137 - val_loss: 0.2533 - val_acc: 0.8973\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2037 - acc: 0.9192 - val_loss: 0.2546 - val_acc: 0.8973\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([ 813,  815,  819,  820,  821,  822,  824,  825,  826,  827,  828,\n",
      "        829,  830,  831,  832,  833,  834,  835,  836,  837,  838,  839,\n",
      "        840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081,\n",
      "       1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092,\n",
      "       1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103,\n",
      "       1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125,\n",
      "       1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136,\n",
      "       1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "       1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n",
      "       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169,\n",
      "       1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,\n",
      "       1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "       1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202,\n",
      "       1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1213, 1214,\n",
      "       1215, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1229, 1230, 1233,\n",
      "       1236, 1237]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6889 - acc: 0.5386 - val_loss: 0.6793 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6834 - acc: 0.5673 - val_loss: 0.6794 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6697 - acc: 0.5673 - val_loss: 0.6573 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6350 - acc: 0.5741 - val_loss: 0.5954 - val_acc: 0.5697\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5828 - acc: 0.6971 - val_loss: 0.5034 - val_acc: 0.8215\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5623 - acc: 0.8078 - val_loss: 0.4309 - val_acc: 0.8289\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3775 - acc: 0.8556 - val_loss: 0.4140 - val_acc: 0.8068\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3802 - acc: 0.8354 - val_loss: 0.3823 - val_acc: 0.8484\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3053 - acc: 0.8837 - val_loss: 0.3669 - val_acc: 0.8582\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2934 - acc: 0.8862 - val_loss: 0.3593 - val_acc: 0.8851\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2645 - acc: 0.8996 - val_loss: 0.3575 - val_acc: 0.8655\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2570 - acc: 0.9009 - val_loss: 0.3557 - val_acc: 0.8753\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2444 - acc: 0.9009 - val_loss: 0.3391 - val_acc: 0.8851\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2367 - acc: 0.9106 - val_loss: 0.3352 - val_acc: 0.8900\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2349 - acc: 0.9070 - val_loss: 0.3448 - val_acc: 0.8802\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2174 - acc: 0.9119 - val_loss: 0.3269 - val_acc: 0.8924\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2118 - acc: 0.9162 - val_loss: 0.3233 - val_acc: 0.8973\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2090 - acc: 0.9131 - val_loss: 0.3143 - val_acc: 0.9022\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2030 - acc: 0.9192 - val_loss: 0.3107 - val_acc: 0.9095\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1988 - acc: 0.9217 - val_loss: 0.3181 - val_acc: 0.8949\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([1209, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1234, 1235,\n",
      "       1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248,\n",
      "       1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,\n",
      "       1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270,\n",
      "       1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281,\n",
      "       1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292,\n",
      "       1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "       1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,\n",
      "       1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325,\n",
      "       1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,\n",
      "       1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "       1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358,\n",
      "       1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369,\n",
      "       1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380,\n",
      "       1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391,\n",
      "       1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402,\n",
      "       1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413,\n",
      "       1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424,\n",
      "       1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "       1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446,\n",
      "       1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457,\n",
      "       1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468,\n",
      "       1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "       1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490,\n",
      "       1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501,\n",
      "       1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512,\n",
      "       1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523,\n",
      "       1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534,\n",
      "       1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
      "       1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556,\n",
      "       1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567,\n",
      "       1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578,\n",
      "       1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589,\n",
      "       1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600,\n",
      "       1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611,\n",
      "       1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622,\n",
      "       1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1633, 1635,\n",
      "       1637, 1638]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6883 - acc: 0.5447 - val_loss: 0.6800 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6708 - acc: 0.5673 - val_loss: 0.6621 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6479 - acc: 0.5698 - val_loss: 0.6098 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5833 - acc: 0.6157 - val_loss: 0.5438 - val_acc: 0.8704\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.4689 - acc: 0.8244 - val_loss: 0.5852 - val_acc: 0.6015\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.4139 - acc: 0.8140 - val_loss: 0.3275 - val_acc: 0.8802\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3336 - acc: 0.8586 - val_loss: 0.3201 - val_acc: 0.8875\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3290 - acc: 0.8494 - val_loss: 0.2806 - val_acc: 0.8753\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2949 - acc: 0.8856 - val_loss: 0.2729 - val_acc: 0.8924\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2694 - acc: 0.8947 - val_loss: 0.2585 - val_acc: 0.9095\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2546 - acc: 0.8996 - val_loss: 0.2691 - val_acc: 0.9242\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2516 - acc: 0.8990 - val_loss: 0.2256 - val_acc: 0.9144\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2275 - acc: 0.9094 - val_loss: 0.2159 - val_acc: 0.9071\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2180 - acc: 0.9131 - val_loss: 0.2083 - val_acc: 0.9218\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2197 - acc: 0.9106 - val_loss: 0.1984 - val_acc: 0.9242\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2015 - acc: 0.9217 - val_loss: 0.1917 - val_acc: 0.9267\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1985 - acc: 0.9211 - val_loss: 0.1968 - val_acc: 0.9169\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1896 - acc: 0.9192 - val_loss: 0.1914 - val_acc: 0.9340\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1833 - acc: 0.9272 - val_loss: 0.1845 - val_acc: 0.9340\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1799 - acc: 0.9284 - val_loss: 0.1753 - val_acc: 0.9364\n",
      "('TRAIN:', array([   0,    1,    2, ..., 1635, 1637, 1638]), 'TEST:', array([1632, 1634, 1636, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
      "       1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
      "       1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668,\n",
      "       1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
      "       1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
      "       1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
      "       1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
      "       1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "       1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
      "       1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745,\n",
      "       1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756,\n",
      "       1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767,\n",
      "       1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778,\n",
      "       1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789,\n",
      "       1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
      "       1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811,\n",
      "       1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822,\n",
      "       1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833,\n",
      "       1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844,\n",
      "       1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,\n",
      "       1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866,\n",
      "       1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877,\n",
      "       1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888,\n",
      "       1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899,\n",
      "       1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910,\n",
      "       1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921,\n",
      "       1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932,\n",
      "       1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943,\n",
      "       1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954,\n",
      "       1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965,\n",
      "       1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976,\n",
      "       1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
      "       1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
      "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
      "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n",
      "       2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n",
      "       2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.6847 - acc: 0.5654 - val_loss: 0.6790 - val_acc: 0.5676\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.6678 - acc: 0.5672 - val_loss: 0.6548 - val_acc: 0.5676\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.6211 - acc: 0.5685 - val_loss: 0.5738 - val_acc: 0.6241\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.5382 - acc: 0.7145 - val_loss: 0.4683 - val_acc: 0.8378\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.3971 - acc: 0.8460 - val_loss: 0.3346 - val_acc: 0.8698\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.4225 - acc: 0.8185 - val_loss: 0.3367 - val_acc: 0.8821\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.3077 - acc: 0.8851 - val_loss: 0.3199 - val_acc: 0.8943\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2819 - acc: 0.8881 - val_loss: 0.3296 - val_acc: 0.8771\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2700 - acc: 0.8894 - val_loss: 0.3228 - val_acc: 0.9091\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2582 - acc: 0.9028 - val_loss: 0.3317 - val_acc: 0.8943\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2386 - acc: 0.9059 - val_loss: 0.3195 - val_acc: 0.9091\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2264 - acc: 0.9132 - val_loss: 0.3205 - val_acc: 0.9140\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2250 - acc: 0.9108 - val_loss: 0.3255 - val_acc: 0.9042\n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2024 - acc: 0.9218 - val_loss: 0.3230 - val_acc: 0.9091\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1929 - acc: 0.9242 - val_loss: 0.3419 - val_acc: 0.9140\n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1899 - acc: 0.9254 - val_loss: 0.3261 - val_acc: 0.9115\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1874 - acc: 0.9303 - val_loss: 0.3256 - val_acc: 0.9165\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1751 - acc: 0.9358 - val_loss: 0.3334 - val_acc: 0.9214\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1695 - acc: 0.9364 - val_loss: 0.3351 - val_acc: 0.9189\n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1618 - acc: 0.9395 - val_loss: 0.3374 - val_acc: 0.9165\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636/1636 [==============================] - 4s - loss: 0.1597 - acc: 0.9438     \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1580 - acc: 0.9395     \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1518 - acc: 0.9401     \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1443 - acc: 0.9474     \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1349 - acc: 0.9517     \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1537 - acc: 0.9450     \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1380 - acc: 0.9456     \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1308 - acc: 0.9542     \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1274 - acc: 0.9480     \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1316 - acc: 0.9505     \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1252 - acc: 0.9572     \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1213 - acc: 0.9554     \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1171 - acc: 0.9590     \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1151 - acc: 0.9584     \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1114 - acc: 0.9603     \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1106 - acc: 0.9584     \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1049 - acc: 0.9639     \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1068 - acc: 0.9621     \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1033 - acc: 0.9621     \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1053 - acc: 0.9597     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92804698972099853"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn first attempt - new dataset + theano (to use \" channel_first \")\n",
    "num_classes = 2\n",
    "for i, (train_index, validation_index) in enumerate(skf.split(X_train, Y_train)):\n",
    "    print(i+1)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.92804698972099853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_82 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/cdesio/.theano/compiledir_Linux-4.2--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.6867 - acc: 0.5563 - val_loss: 0.6803 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.6768 - acc: 0.5673 - val_loss: 0.6645 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.6461 - acc: 0.5728 - val_loss: 0.6151 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.5642 - acc: 0.6683 - val_loss: 0.5145 - val_acc: 0.8289\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 4s - loss: 0.4518 - acc: 0.8152 - val_loss: 0.4690 - val_acc: 0.8484\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3765 - acc: 0.8507 - val_loss: 0.3746 - val_acc: 0.8582\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3114 - acc: 0.8764 - val_loss: 0.3254 - val_acc: 0.8851\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2802 - acc: 0.8966 - val_loss: 0.3600 - val_acc: 0.8875\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2735 - acc: 0.8953 - val_loss: 0.3096 - val_acc: 0.8875\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2446 - acc: 0.9051 - val_loss: 0.2994 - val_acc: 0.8949\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2248 - acc: 0.9198 - val_loss: 0.3668 - val_acc: 0.8851\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2298 - acc: 0.9149 - val_loss: 0.2940 - val_acc: 0.9046\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2038 - acc: 0.9217 - val_loss: 0.2862 - val_acc: 0.8998\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1857 - acc: 0.9308 - val_loss: 0.2869 - val_acc: 0.9120\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1832 - acc: 0.9308 - val_loss: 0.2741 - val_acc: 0.9169\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1890 - acc: 0.9333 - val_loss: 0.2628 - val_acc: 0.9193\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1654 - acc: 0.9351 - val_loss: 0.2590 - val_acc: 0.9242\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1654 - acc: 0.9437 - val_loss: 0.2672 - val_acc: 0.8998\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1638 - acc: 0.9443 - val_loss: 0.2790 - val_acc: 0.9193\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1519 - acc: 0.9437 - val_loss: 0.2555 - val_acc: 0.9218\n",
      "2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_86 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6854 - acc: 0.5655 - val_loss: 0.6744 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6703 - acc: 0.5673 - val_loss: 0.6516 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6346 - acc: 0.5673 - val_loss: 0.6364 - val_acc: 0.7946\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5563 - acc: 0.7032 - val_loss: 0.4729 - val_acc: 0.8435\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.4220 - acc: 0.8458 - val_loss: 0.3708 - val_acc: 0.8509\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3586 - acc: 0.8525 - val_loss: 0.3330 - val_acc: 0.8582\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3206 - acc: 0.8807 - val_loss: 0.3265 - val_acc: 0.8655\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2959 - acc: 0.8856 - val_loss: 0.2990 - val_acc: 0.8655\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2680 - acc: 0.8978 - val_loss: 0.2923 - val_acc: 0.8851\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2482 - acc: 0.9076 - val_loss: 0.2913 - val_acc: 0.8900\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2366 - acc: 0.9113 - val_loss: 0.2946 - val_acc: 0.8900\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2245 - acc: 0.9168 - val_loss: 0.2549 - val_acc: 0.8949\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2155 - acc: 0.9174 - val_loss: 0.2490 - val_acc: 0.8973\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1937 - acc: 0.9278 - val_loss: 0.2687 - val_acc: 0.8900\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1912 - acc: 0.9272 - val_loss: 0.2679 - val_acc: 0.8949\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1802 - acc: 0.9290 - val_loss: 0.2803 - val_acc: 0.8924\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1752 - acc: 0.9345 - val_loss: 0.2525 - val_acc: 0.8949\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1583 - acc: 0.9321 - val_loss: 0.2744 - val_acc: 0.8998\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1594 - acc: 0.9388 - val_loss: 0.2690 - val_acc: 0.8998\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1375 - acc: 0.9474 - val_loss: 0.2810 - val_acc: 0.9022\n",
      "3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_90 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6867 - acc: 0.5661 - val_loss: 0.6782 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6701 - acc: 0.5673 - val_loss: 0.6573 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6338 - acc: 0.5698 - val_loss: 0.5799 - val_acc: 0.5819\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5614 - acc: 0.7368 - val_loss: 0.4841 - val_acc: 0.7897\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.4285 - acc: 0.8299 - val_loss: 0.3890 - val_acc: 0.8582\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3475 - acc: 0.8531 - val_loss: 0.3799 - val_acc: 0.8435\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3107 - acc: 0.8819 - val_loss: 0.3611 - val_acc: 0.8826\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2865 - acc: 0.8978 - val_loss: 0.3505 - val_acc: 0.8802\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2594 - acc: 0.9015 - val_loss: 0.3397 - val_acc: 0.8802\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2340 - acc: 0.9088 - val_loss: 0.3365 - val_acc: 0.8802\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2865 - acc: 0.8813 - val_loss: 0.3433 - val_acc: 0.8729\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2142 - acc: 0.9119 - val_loss: 0.3280 - val_acc: 0.8851\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2059 - acc: 0.9174 - val_loss: 0.3134 - val_acc: 0.9046\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1953 - acc: 0.9198 - val_loss: 0.3138 - val_acc: 0.9095\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2006 - acc: 0.9211 - val_loss: 0.3050 - val_acc: 0.9169\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1850 - acc: 0.9266 - val_loss: 0.2966 - val_acc: 0.9169\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1900 - acc: 0.9241 - val_loss: 0.3115 - val_acc: 0.8924\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1769 - acc: 0.9308 - val_loss: 0.2904 - val_acc: 0.9071\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1676 - acc: 0.9315 - val_loss: 0.2857 - val_acc: 0.9095\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1648 - acc: 0.9382 - val_loss: 0.2938 - val_acc: 0.9046\n",
      "4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_94 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6869 - acc: 0.5618 - val_loss: 0.6795 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6747 - acc: 0.5673 - val_loss: 0.7057 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.6527 - acc: 0.5673 - val_loss: 0.6367 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.5927 - acc: 0.6206 - val_loss: 0.5235 - val_acc: 0.8386\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.4695 - acc: 0.8237 - val_loss: 0.4679 - val_acc: 0.7359\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.4063 - acc: 0.8305 - val_loss: 0.3395 - val_acc: 0.8313\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.3350 - acc: 0.8721 - val_loss: 0.2790 - val_acc: 0.8851\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2947 - acc: 0.8849 - val_loss: 0.2556 - val_acc: 0.8900\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2923 - acc: 0.8892 - val_loss: 0.2675 - val_acc: 0.8826\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2600 - acc: 0.9015 - val_loss: 0.2327 - val_acc: 0.8924\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2399 - acc: 0.9094 - val_loss: 0.2209 - val_acc: 0.9022\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2398 - acc: 0.9070 - val_loss: 0.2186 - val_acc: 0.9071\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2155 - acc: 0.9155 - val_loss: 0.2049 - val_acc: 0.9144\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.2209 - acc: 0.9113 - val_loss: 0.2018 - val_acc: 0.9267\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1961 - acc: 0.9235 - val_loss: 0.1940 - val_acc: 0.9267\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1880 - acc: 0.9259 - val_loss: 0.1914 - val_acc: 0.9267\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1772 - acc: 0.9284 - val_loss: 0.2061 - val_acc: 0.9364\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1743 - acc: 0.9327 - val_loss: 0.1784 - val_acc: 0.9291\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1622 - acc: 0.9308 - val_loss: 0.1724 - val_acc: 0.9413\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 5s - loss: 0.1595 - acc: 0.9321 - val_loss: 0.1692 - val_acc: 0.9438\n",
      "5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_98 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636/1636 [==============================] - 5s - loss: 0.6866 - acc: 0.5544 - val_loss: 0.6793 - val_acc: 0.5676\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.6737 - acc: 0.5672 - val_loss: 0.6599 - val_acc: 0.5676\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.6353 - acc: 0.5672 - val_loss: 0.5984 - val_acc: 0.5676\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.5583 - acc: 0.6546 - val_loss: 0.4905 - val_acc: 0.8231\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.4478 - acc: 0.8123 - val_loss: 0.4053 - val_acc: 0.8452\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.3415 - acc: 0.8594 - val_loss: 0.3812 - val_acc: 0.8747\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.3157 - acc: 0.8747 - val_loss: 0.3219 - val_acc: 0.8821\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2952 - acc: 0.8845 - val_loss: 0.3154 - val_acc: 0.9017\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2595 - acc: 0.9022 - val_loss: 0.3488 - val_acc: 0.8894\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2423 - acc: 0.9028 - val_loss: 0.3005 - val_acc: 0.9066\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2247 - acc: 0.9132 - val_loss: 0.3213 - val_acc: 0.9115\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2329 - acc: 0.9053 - val_loss: 0.3046 - val_acc: 0.9115\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.2131 - acc: 0.9163 - val_loss: 0.3088 - val_acc: 0.9115\n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1916 - acc: 0.9285 - val_loss: 0.3034 - val_acc: 0.9066\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1793 - acc: 0.9322 - val_loss: 0.3574 - val_acc: 0.9140\n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1763 - acc: 0.9291 - val_loss: 0.2883 - val_acc: 0.9165\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1815 - acc: 0.9358 - val_loss: 0.3159 - val_acc: 0.9238\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1613 - acc: 0.9370 - val_loss: 0.3278 - val_acc: 0.9165\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1553 - acc: 0.9383 - val_loss: 0.2963 - val_acc: 0.9263\n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 5s - loss: 0.1616 - acc: 0.9395 - val_loss: 0.2901 - val_acc: 0.9214\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1489 - acc: 0.9456     \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1414 - acc: 0.9517     \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1348 - acc: 0.9517     \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1322 - acc: 0.9529     \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1287 - acc: 0.9584     \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1355 - acc: 0.9493     \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1240 - acc: 0.9560     \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1215 - acc: 0.9584     \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1244 - acc: 0.9511     \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1118 - acc: 0.9578     \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1239 - acc: 0.9603     \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1077 - acc: 0.9584     \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1047 - acc: 0.9639     \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1034 - acc: 0.9578     \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1052 - acc: 0.9627     \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0957 - acc: 0.9652     \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.1024 - acc: 0.9597     \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0887 - acc: 0.9682     \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0844 - acc: 0.9713     \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0927 - acc: 0.9645     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91336270190895741"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn first attempt - new dataset + theano (to use \" channel_first \")\n",
    "#dropouts changed to 0.1 and 0.25 instead of 0.25 and 0.50\n",
    "num_classes = 2\n",
    "for i, (train_index, validation_index) in enumerate(skf.split(X_train, Y_train)):\n",
    "    print(i+1)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.91336270190895741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe8d00edc10>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBl/j/NR28n6qiDbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8d3447cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDHGRMUChjHGmKj8f0c6KymEVB6fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8d0a421d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_first'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300, 75, 115, 18)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 11s - loss: 0.6855 - acc: 0.5643 - val_loss: 0.6784 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 11s - loss: 0.6653 - acc: 0.5673 - val_loss: 0.6420 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 11s - loss: 0.6259 - acc: 0.5673 - val_loss: 0.5809 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1536/1634 [===========================>..] - ETA: 0s - loss: 0.5324 - acc: 0.7135"
     ]
    }
   ],
   "source": [
    "#cnn first attempt - new dataset + theano (to use \" channel_first \")\n",
    "#theano on gpux01\n",
    "num_classes = 2\n",
    "for i, (train_index, validation_index) in enumerate(skf.split(X_train, Y_train)):\n",
    "    print(i+1)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_144 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_145 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_146 (Conv2D)          (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_147 (Conv2D)          (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_43 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5039 samples, validate on 1261 samples\n",
      "Epoch 1/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.6855 - acc: 0.5626 - val_loss: 0.6818 - val_acc: 0.5654\n",
      "Epoch 2/20\n",
      "5039/5039 [==============================] - 15s - loss: 0.6163 - acc: 0.6348 - val_loss: 0.4642 - val_acc: 0.8239\n",
      "Epoch 3/20\n",
      "5039/5039 [==============================] - 15s - loss: 0.4265 - acc: 0.8252 - val_loss: 0.3634 - val_acc: 0.8652\n",
      "Epoch 4/20\n",
      "5039/5039 [==============================] - 15s - loss: 0.3577 - acc: 0.8581 - val_loss: 0.3368 - val_acc: 0.8731\n",
      "Epoch 5/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.3294 - acc: 0.8758 - val_loss: 0.3278 - val_acc: 0.8771\n",
      "Epoch 6/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.3070 - acc: 0.8831 - val_loss: 0.3188 - val_acc: 0.8771\n",
      "Epoch 7/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2937 - acc: 0.8910 - val_loss: 0.3093 - val_acc: 0.8842\n",
      "Epoch 8/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2845 - acc: 0.8912 - val_loss: 0.2999 - val_acc: 0.8842\n",
      "Epoch 9/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2780 - acc: 0.8954 - val_loss: 0.3317 - val_acc: 0.8723\n",
      "Epoch 10/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2759 - acc: 0.8920 - val_loss: 0.2987 - val_acc: 0.8858\n",
      "Epoch 11/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2696 - acc: 0.8964 - val_loss: 0.3041 - val_acc: 0.8810\n",
      "Epoch 12/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2671 - acc: 0.8982 - val_loss: 0.2735 - val_acc: 0.8834\n",
      "Epoch 13/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2595 - acc: 0.9006 - val_loss: 0.2753 - val_acc: 0.8898\n",
      "Epoch 14/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2597 - acc: 0.9047 - val_loss: 0.2809 - val_acc: 0.8842\n",
      "Epoch 15/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2510 - acc: 0.9049 - val_loss: 0.2693 - val_acc: 0.8914\n",
      "Epoch 16/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2495 - acc: 0.9049 - val_loss: 0.2723 - val_acc: 0.8882\n",
      "Epoch 17/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2437 - acc: 0.9047 - val_loss: 0.2812 - val_acc: 0.8898\n",
      "Epoch 18/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2396 - acc: 0.9109 - val_loss: 0.2625 - val_acc: 0.8898\n",
      "Epoch 19/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2364 - acc: 0.9121 - val_loss: 0.2704 - val_acc: 0.8898\n",
      "Epoch 20/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2365 - acc: 0.9109 - val_loss: 0.2885 - val_acc: 0.8850\n",
      "2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_148 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_149 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_151 (Conv2D)          (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5039 samples, validate on 1261 samples\n",
      "Epoch 1/20\n",
      "5039/5039 [==============================] - 15s - loss: 0.6816 - acc: 0.5618 - val_loss: 0.6657 - val_acc: 0.5654\n",
      "Epoch 2/20\n",
      "5039/5039 [==============================] - 15s - loss: 0.5899 - acc: 0.6599 - val_loss: 0.4765 - val_acc: 0.8041\n",
      "Epoch 3/20\n",
      "5039/5039 [==============================] - 15s - loss: 0.4039 - acc: 0.8357 - val_loss: 0.4209 - val_acc: 0.8208\n",
      "Epoch 4/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.3530 - acc: 0.8553 - val_loss: 0.3391 - val_acc: 0.8739\n",
      "Epoch 5/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.3244 - acc: 0.8746 - val_loss: 0.3313 - val_acc: 0.8668\n",
      "Epoch 6/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.3036 - acc: 0.8831 - val_loss: 0.3224 - val_acc: 0.8739\n",
      "Epoch 7/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2984 - acc: 0.8831 - val_loss: 0.3091 - val_acc: 0.8803\n",
      "Epoch 8/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2844 - acc: 0.8922 - val_loss: 0.3068 - val_acc: 0.8810\n",
      "Epoch 9/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2736 - acc: 0.8958 - val_loss: 0.3108 - val_acc: 0.8842\n",
      "Epoch 10/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2706 - acc: 0.8968 - val_loss: 0.3178 - val_acc: 0.8747\n",
      "Epoch 11/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2632 - acc: 0.8964 - val_loss: 0.3162 - val_acc: 0.8810\n",
      "Epoch 12/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2591 - acc: 0.8986 - val_loss: 0.3034 - val_acc: 0.8858\n",
      "Epoch 13/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2564 - acc: 0.9034 - val_loss: 0.3135 - val_acc: 0.8787\n",
      "Epoch 14/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2479 - acc: 0.9065 - val_loss: 0.3006 - val_acc: 0.8850\n",
      "Epoch 15/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2447 - acc: 0.9059 - val_loss: 0.3541 - val_acc: 0.8636\n",
      "Epoch 16/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2420 - acc: 0.9053 - val_loss: 0.3048 - val_acc: 0.8834\n",
      "Epoch 17/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2374 - acc: 0.9107 - val_loss: 0.3092 - val_acc: 0.8810\n",
      "Epoch 18/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2356 - acc: 0.9125 - val_loss: 0.3067 - val_acc: 0.8818\n",
      "Epoch 19/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2319 - acc: 0.9097 - val_loss: 0.3118 - val_acc: 0.8818\n",
      "Epoch 20/20\n",
      "5039/5039 [==============================] - 16s - loss: 0.2270 - acc: 0.9121 - val_loss: 0.3103 - val_acc: 0.8818\n",
      "3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_152 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_153 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_154 (Conv2D)          (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_155 (Conv2D)          (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5040 samples, validate on 1260 samples\n",
      "Epoch 1/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.6820 - acc: 0.5599 - val_loss: 0.6637 - val_acc: 0.5659\n",
      "Epoch 2/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.5692 - acc: 0.6881 - val_loss: 0.4606 - val_acc: 0.8127\n",
      "Epoch 3/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.4087 - acc: 0.8331 - val_loss: 0.3692 - val_acc: 0.8548\n",
      "Epoch 4/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.3603 - acc: 0.8583 - val_loss: 0.3703 - val_acc: 0.8405\n",
      "Epoch 5/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.3298 - acc: 0.8591 - val_loss: 0.3369 - val_acc: 0.8762\n",
      "Epoch 6/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.3045 - acc: 0.8768 - val_loss: 0.3194 - val_acc: 0.8770\n",
      "Epoch 7/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2931 - acc: 0.8829 - val_loss: 0.3187 - val_acc: 0.8786\n",
      "Epoch 8/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2830 - acc: 0.8849 - val_loss: 0.2950 - val_acc: 0.8897\n",
      "Epoch 9/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2752 - acc: 0.8885 - val_loss: 0.3085 - val_acc: 0.8802\n",
      "Epoch 10/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2751 - acc: 0.8931 - val_loss: 0.2965 - val_acc: 0.8810\n",
      "Epoch 11/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2662 - acc: 0.8938 - val_loss: 0.2887 - val_acc: 0.8865\n",
      "Epoch 12/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2599 - acc: 0.8992 - val_loss: 0.2825 - val_acc: 0.8897\n",
      "Epoch 13/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2552 - acc: 0.8990 - val_loss: 0.2814 - val_acc: 0.8976\n",
      "Epoch 14/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2505 - acc: 0.9008 - val_loss: 0.2965 - val_acc: 0.8825\n",
      "Epoch 15/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2479 - acc: 0.9036 - val_loss: 0.2862 - val_acc: 0.8889\n",
      "Epoch 16/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2440 - acc: 0.9040 - val_loss: 0.3274 - val_acc: 0.8778\n",
      "Epoch 17/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2397 - acc: 0.9075 - val_loss: 0.2844 - val_acc: 0.8913\n",
      "Epoch 18/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2348 - acc: 0.9109 - val_loss: 0.2885 - val_acc: 0.8905\n",
      "Epoch 19/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2330 - acc: 0.9089 - val_loss: 0.2936 - val_acc: 0.8881\n",
      "Epoch 20/20\n",
      "5040/5040 [==============================] - 16s - loss: 0.2274 - acc: 0.9147 - val_loss: 0.2888 - val_acc: 0.8889\n",
      "4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_156 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_157 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_158 (Conv2D)          (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_159 (Conv2D)          (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5041 samples, validate on 1259 samples\n",
      "Epoch 1/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.6785 - acc: 0.5652 - val_loss: 0.6560 - val_acc: 0.5655\n",
      "Epoch 2/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.5145 - acc: 0.7449 - val_loss: 0.4311 - val_acc: 0.8078\n",
      "Epoch 3/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.3782 - acc: 0.8480 - val_loss: 0.3529 - val_acc: 0.8578\n",
      "Epoch 4/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.3363 - acc: 0.8671 - val_loss: 0.3371 - val_acc: 0.8689\n",
      "Epoch 5/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.3147 - acc: 0.8752 - val_loss: 0.3328 - val_acc: 0.8864\n",
      "Epoch 6/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2966 - acc: 0.8877 - val_loss: 0.3049 - val_acc: 0.8872\n",
      "Epoch 7/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2865 - acc: 0.8863 - val_loss: 0.3064 - val_acc: 0.8864\n",
      "Epoch 8/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2792 - acc: 0.8867 - val_loss: 0.2955 - val_acc: 0.8912\n",
      "Epoch 9/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2706 - acc: 0.8929 - val_loss: 0.2938 - val_acc: 0.8896\n",
      "Epoch 10/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2649 - acc: 0.8931 - val_loss: 0.2949 - val_acc: 0.8904\n",
      "Epoch 11/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2627 - acc: 0.8959 - val_loss: 0.3066 - val_acc: 0.8896\n",
      "Epoch 12/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2555 - acc: 0.9024 - val_loss: 0.3028 - val_acc: 0.8848\n",
      "Epoch 13/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2514 - acc: 0.8986 - val_loss: 0.2896 - val_acc: 0.8928\n",
      "Epoch 14/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2475 - acc: 0.9032 - val_loss: 0.2954 - val_acc: 0.8936\n",
      "Epoch 15/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2409 - acc: 0.9046 - val_loss: 0.2918 - val_acc: 0.8983\n",
      "Epoch 16/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2429 - acc: 0.9036 - val_loss: 0.2855 - val_acc: 0.8952\n",
      "Epoch 17/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2345 - acc: 0.9072 - val_loss: 0.3025 - val_acc: 0.8824\n",
      "Epoch 18/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2331 - acc: 0.9068 - val_loss: 0.2959 - val_acc: 0.8944\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5041/5041 [==============================] - 16s - loss: 0.2323 - acc: 0.9084 - val_loss: 0.3041 - val_acc: 0.8952\n",
      "Epoch 20/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2260 - acc: 0.9115 - val_loss: 0.2938 - val_acc: 0.9015\n",
      "5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_160 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_161 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_162 (Conv2D)          (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_163 (Conv2D)          (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5041 samples, validate on 1259 samples\n",
      "Epoch 1/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.6826 - acc: 0.5634 - val_loss: 0.6694 - val_acc: 0.5655\n",
      "Epoch 2/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.5813 - acc: 0.6993 - val_loss: 0.4223 - val_acc: 0.8435\n",
      "Epoch 3/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.3965 - acc: 0.8371 - val_loss: 0.3581 - val_acc: 0.8586\n",
      "Epoch 4/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.3408 - acc: 0.8641 - val_loss: 0.3419 - val_acc: 0.8658\n",
      "Epoch 5/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.3108 - acc: 0.8762 - val_loss: 0.3257 - val_acc: 0.8753\n",
      "Epoch 6/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2958 - acc: 0.8857 - val_loss: 0.3344 - val_acc: 0.8681\n",
      "Epoch 7/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2804 - acc: 0.8887 - val_loss: 0.3242 - val_acc: 0.8832\n",
      "Epoch 8/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2744 - acc: 0.8893 - val_loss: 0.3157 - val_acc: 0.8856\n",
      "Epoch 9/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2660 - acc: 0.8915 - val_loss: 0.3210 - val_acc: 0.8777\n",
      "Epoch 10/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2593 - acc: 0.8941 - val_loss: 0.3149 - val_acc: 0.8809\n",
      "Epoch 11/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2542 - acc: 0.8947 - val_loss: 0.3167 - val_acc: 0.8896\n",
      "Epoch 12/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2478 - acc: 0.9016 - val_loss: 0.3108 - val_acc: 0.8848\n",
      "Epoch 13/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2418 - acc: 0.9010 - val_loss: 0.3118 - val_acc: 0.8888\n",
      "Epoch 14/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2433 - acc: 0.9036 - val_loss: 0.3073 - val_acc: 0.8872\n",
      "Epoch 15/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2390 - acc: 0.9032 - val_loss: 0.3093 - val_acc: 0.8936\n",
      "Epoch 16/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2357 - acc: 0.9076 - val_loss: 0.3041 - val_acc: 0.8944\n",
      "Epoch 17/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2304 - acc: 0.9078 - val_loss: 0.3292 - val_acc: 0.8801\n",
      "Epoch 18/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2257 - acc: 0.9105 - val_loss: 0.3317 - val_acc: 0.8777\n",
      "Epoch 19/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2232 - acc: 0.9093 - val_loss: 0.3149 - val_acc: 0.8872\n",
      "Epoch 20/20\n",
      "5041/5041 [==============================] - 16s - loss: 0.2200 - acc: 0.9111 - val_loss: 0.3216 - val_acc: 0.8872\n",
      "Epoch 1/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2378 - acc: 0.9064    \n",
      "Epoch 2/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2269 - acc: 0.9084    \n",
      "Epoch 3/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2216 - acc: 0.9089    \n",
      "Epoch 4/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2180 - acc: 0.9153    \n",
      "Epoch 5/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2114 - acc: 0.9147    \n",
      "Epoch 6/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2097 - acc: 0.9153    \n",
      "Epoch 7/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2031 - acc: 0.9197    \n",
      "Epoch 8/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.2033 - acc: 0.9191    \n",
      "Epoch 9/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1997 - acc: 0.9207    \n",
      "Epoch 10/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1895 - acc: 0.9226    \n",
      "Epoch 11/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1900 - acc: 0.9220    \n",
      "Epoch 12/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1835 - acc: 0.9288    \n",
      "Epoch 13/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1802 - acc: 0.9296    \n",
      "Epoch 14/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1769 - acc: 0.9316    \n",
      "Epoch 15/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1727 - acc: 0.9310    \n",
      "Epoch 16/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1737 - acc: 0.9312    \n",
      "Epoch 17/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1692 - acc: 0.9318    \n",
      "Epoch 18/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1647 - acc: 0.9347    \n",
      "Epoch 19/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1598 - acc: 0.9395    \n",
      "Epoch 20/20\n",
      "5041/5041 [==============================] - 15s - loss: 0.1540 - acc: 0.9403    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88476190476190475"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"cnn first attempt\" - new dataset * 3 files per type\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n",
    "num_classes = 2\n",
    "for i, (train_index, validation_index) in enumerate(skf.split(X_train, Y_train)):\n",
    "    print(i+1)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_170 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_171 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "conv2d_172 (Conv2D)          (None, 64, 109, 12)       36928     \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 64, 109, 12)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_173 (Conv2D)          (None, 32, 107, 10)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_174 (Conv2D)          (None, 32, 105, 8)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 32, 105, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_175 (Conv2D)          (None, 32, 103, 6)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 39554     \n",
      "=================================================================\n",
      "Total params: 273,762\n",
      "Trainable params: 273,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5039 samples, validate on 1261 samples\n",
      "Epoch 1/20\n",
      "5039/5039 [==============================] - 23s - loss: 0.6849 - acc: 0.5646 - val_loss: 0.6701 - val_acc: 0.5654\n",
      "Epoch 2/20\n",
      "5039/5039 [==============================] - 21s - loss: 0.6022 - acc: 0.6398 - val_loss: 0.4600 - val_acc: 0.8097\n",
      "Epoch 3/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.4266 - acc: 0.8131 - val_loss: 0.3638 - val_acc: 0.8541\n",
      "Epoch 4/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.3686 - acc: 0.8498 - val_loss: 0.3374 - val_acc: 0.8699\n",
      "Epoch 5/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.3325 - acc: 0.8724 - val_loss: 0.3317 - val_acc: 0.8707\n",
      "Epoch 6/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.3208 - acc: 0.8782 - val_loss: 0.3212 - val_acc: 0.8771\n",
      "Epoch 7/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.3008 - acc: 0.8865 - val_loss: 0.3152 - val_acc: 0.8826\n",
      "Epoch 8/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2968 - acc: 0.8907 - val_loss: 0.3434 - val_acc: 0.8755\n",
      "Epoch 9/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2825 - acc: 0.8932 - val_loss: 0.3006 - val_acc: 0.8890\n",
      "Epoch 10/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2807 - acc: 0.8942 - val_loss: 0.3025 - val_acc: 0.8929\n",
      "Epoch 11/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2689 - acc: 0.9006 - val_loss: 0.3140 - val_acc: 0.8953\n",
      "Epoch 12/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2667 - acc: 0.9014 - val_loss: 0.2939 - val_acc: 0.8953\n",
      "Epoch 13/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2610 - acc: 0.9012 - val_loss: 0.3044 - val_acc: 0.8993\n",
      "Epoch 14/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2567 - acc: 0.9041 - val_loss: 0.2869 - val_acc: 0.8993\n",
      "Epoch 15/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2516 - acc: 0.9081 - val_loss: 0.3037 - val_acc: 0.8961\n",
      "Epoch 16/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2492 - acc: 0.9045 - val_loss: 0.2900 - val_acc: 0.8866\n",
      "Epoch 17/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2429 - acc: 0.9105 - val_loss: 0.2911 - val_acc: 0.8945\n",
      "Epoch 18/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2458 - acc: 0.9091 - val_loss: 0.2932 - val_acc: 0.8969\n",
      "Epoch 19/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2324 - acc: 0.9137 - val_loss: 0.2949 - val_acc: 0.8977\n",
      "Epoch 20/20\n",
      "5039/5039 [==============================] - 22s - loss: 0.2314 - acc: 0.9125 - val_loss: 0.2944 - val_acc: 0.8993\n",
      "2\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-e4797425c0b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_nn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Try cnn_deep with theano - MaxPooling removed\n",
    "#cnn deep - new dataset + theano (to use \" channel_first \")\n",
    "num_classes = 2\n",
    "for i, (train_index, validation_index) in enumerate(skf.split(X_train, Y_train)):\n",
    "    print(i+1)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('The specified size contains a dimension with value <= 0', (-2016, 2))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-64ab8e16c8fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_nn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mYtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mYvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-263-8213061430c2>\u001b[0m in \u001b[0;36mcnn_model_deep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     model.compile(loss=categorical_crossentropy,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    464\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m    557\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    825\u001b[0m                                       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             self.bias = self.add_weight(shape=(self.units,),\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/initializers.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             return K.random_uniform(shape, -limit, limit,\n\u001b[0;32m--> 208\u001b[0;31m                                     dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2052\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/sandbox/rng_mrg.pyc\u001b[0m in \u001b[0;36muniform\u001b[0;34m(self, size, low, high, ndim, dtype, nstreams)\u001b[0m\n\u001b[1;32m   1342\u001b[0m                 raise ValueError(\n\u001b[1;32m   1343\u001b[0m                     \u001b[0;34m\"The specified size contains a dimension with value <= 0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                     size)\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('The specified size contains a dimension with value <= 0', (-2016, 2))"
     ]
    }
   ],
   "source": [
    "#Try cnn_deep with theano - MaxPooling\n",
    "#cnn deep - new dataset + theano (to use \" channel_first \")\n",
    "num_classes = 2\n",
    "for i, (train_index, validation_index) in enumerate(skf.split(X_train, Y_train)):\n",
    "    print(i+1)\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/cdesio/.theano/compiledir_Linux-4.2--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_114 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_115 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 64, 109, 12)       36928     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 64, 109, 12)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 32, 107, 10)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 32, 105, 8)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 32, 105, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 32, 103, 6)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 2)                 39554     \n",
      "=================================================================\n",
      "Total params: 273,762\n",
      "Trainable params: 273,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.6880 - acc: 0.5588 - val_loss: 0.6830 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.6758 - acc: 0.5673 - val_loss: 0.6639 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.6300 - acc: 0.5704 - val_loss: 0.5776 - val_acc: 0.6650\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.5852 - acc: 0.7460 - val_loss: 0.5525 - val_acc: 0.7531\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.4577 - acc: 0.8072 - val_loss: 0.4289 - val_acc: 0.8142\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.4835 - acc: 0.7656 - val_loss: 0.4267 - val_acc: 0.8215\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.3694 - acc: 0.8470 - val_loss: 0.4079 - val_acc: 0.8533\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.3323 - acc: 0.8623 - val_loss: 0.3905 - val_acc: 0.8606\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.3179 - acc: 0.8690 - val_loss: 0.3827 - val_acc: 0.8680\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.2964 - acc: 0.8770 - val_loss: 0.3650 - val_acc: 0.8778\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.2706 - acc: 0.8880 - val_loss: 0.3431 - val_acc: 0.8875\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2615 - acc: 0.8978 - val_loss: 0.3410 - val_acc: 0.8924\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2390 - acc: 0.9076 - val_loss: 0.3320 - val_acc: 0.8729\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2241 - acc: 0.9149 - val_loss: 0.3400 - val_acc: 0.8875\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2174 - acc: 0.9204 - val_loss: 0.3293 - val_acc: 0.9022\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1977 - acc: 0.9229 - val_loss: 0.3138 - val_acc: 0.9071\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1888 - acc: 0.9290 - val_loss: 0.3163 - val_acc: 0.9071\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1855 - acc: 0.9315 - val_loss: 0.3155 - val_acc: 0.8998\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1740 - acc: 0.9370 - val_loss: 0.3085 - val_acc: 0.9071\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1700 - acc: 0.9339 - val_loss: 0.3005 - val_acc: 0.9120\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_120 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 64, 109, 12)       36928     \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 64, 109, 12)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 32, 107, 10)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 32, 105, 8)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 32, 105, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 32, 103, 6)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 2)                 39554     \n",
      "=================================================================\n",
      "Total params: 273,762\n",
      "Trainable params: 273,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 6s - loss: 0.6897 - acc: 0.5557 - val_loss: 0.6831 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6814 - acc: 0.5673 - val_loss: 0.6740 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6725 - acc: 0.5673 - val_loss: 0.6419 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6340 - acc: 0.5679 - val_loss: 0.5674 - val_acc: 0.5697\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.5365 - acc: 0.6665 - val_loss: 0.4784 - val_acc: 0.7384\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4799 - acc: 0.8054 - val_loss: 0.4535 - val_acc: 0.8557\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4445 - acc: 0.8654 - val_loss: 0.4253 - val_acc: 0.8509\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4280 - acc: 0.8647 - val_loss: 0.4173 - val_acc: 0.8802\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4133 - acc: 0.8947 - val_loss: 0.4097 - val_acc: 0.8973\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3989 - acc: 0.9015 - val_loss: 0.4003 - val_acc: 0.8998\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3881 - acc: 0.9137 - val_loss: 0.3916 - val_acc: 0.8998\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3734 - acc: 0.9149 - val_loss: 0.3874 - val_acc: 0.9046\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3666 - acc: 0.9229 - val_loss: 0.3867 - val_acc: 0.9022\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3631 - acc: 0.9192 - val_loss: 0.3878 - val_acc: 0.9046\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3521 - acc: 0.9217 - val_loss: 0.3790 - val_acc: 0.9120\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3417 - acc: 0.9272 - val_loss: 0.3857 - val_acc: 0.8998\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3368 - acc: 0.9235 - val_loss: 0.3788 - val_acc: 0.9046\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3271 - acc: 0.9351 - val_loss: 0.3765 - val_acc: 0.9071\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3243 - acc: 0.9339 - val_loss: 0.3796 - val_acc: 0.9095\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3164 - acc: 0.9394 - val_loss: 0.3769 - val_acc: 0.9095\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([ 813,  815,  819,  820,  821,  822,  824,  825,  826,  827,  828,\n",
      "        829,  830,  831,  832,  833,  834,  835,  836,  837,  838,  839,\n",
      "        840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081,\n",
      "       1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092,\n",
      "       1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103,\n",
      "       1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125,\n",
      "       1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136,\n",
      "       1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "       1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n",
      "       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169,\n",
      "       1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,\n",
      "       1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "       1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202,\n",
      "       1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1213, 1214,\n",
      "       1215, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1229, 1230, 1233,\n",
      "       1236, 1237]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_126 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 64, 109, 12)       36928     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 64, 109, 12)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 32, 107, 10)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_130 (Conv2D)          (None, 32, 105, 8)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 32, 105, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_131 (Conv2D)          (None, 32, 103, 6)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 2)                 39554     \n",
      "=================================================================\n",
      "Total params: 273,762\n",
      "Trainable params: 273,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6879 - acc: 0.5643 - val_loss: 0.6820 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6811 - acc: 0.5673 - val_loss: 0.6722 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6580 - acc: 0.5673 - val_loss: 0.8460 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6578 - acc: 0.5973 - val_loss: 0.6201 - val_acc: 0.7090\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.5669 - acc: 0.6995 - val_loss: 0.5006 - val_acc: 0.8044\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4426 - acc: 0.8072 - val_loss: 0.4298 - val_acc: 0.8509\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4203 - acc: 0.8323 - val_loss: 0.4218 - val_acc: 0.8386\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3287 - acc: 0.8672 - val_loss: 0.4225 - val_acc: 0.8264\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3006 - acc: 0.8843 - val_loss: 0.3850 - val_acc: 0.8557\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2971 - acc: 0.8794 - val_loss: 0.3925 - val_acc: 0.8582\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2744 - acc: 0.8966 - val_loss: 0.3835 - val_acc: 0.8753\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2465 - acc: 0.9094 - val_loss: 0.3768 - val_acc: 0.8826\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2806 - acc: 0.8953 - val_loss: 0.3686 - val_acc: 0.8557\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2265 - acc: 0.9137 - val_loss: 0.3560 - val_acc: 0.8900\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2106 - acc: 0.9162 - val_loss: 0.3484 - val_acc: 0.9046\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1984 - acc: 0.9180 - val_loss: 0.3424 - val_acc: 0.9071\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1971 - acc: 0.9223 - val_loss: 0.3359 - val_acc: 0.9095\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.2100 - acc: 0.9211 - val_loss: 0.3540 - val_acc: 0.9022\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1841 - acc: 0.9321 - val_loss: 0.3302 - val_acc: 0.9095\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.1811 - acc: 0.9290 - val_loss: 0.3285 - val_acc: 0.9095\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([1209, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1234, 1235,\n",
      "       1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248,\n",
      "       1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,\n",
      "       1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270,\n",
      "       1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281,\n",
      "       1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292,\n",
      "       1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "       1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,\n",
      "       1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325,\n",
      "       1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,\n",
      "       1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "       1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358,\n",
      "       1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369,\n",
      "       1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380,\n",
      "       1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391,\n",
      "       1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402,\n",
      "       1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413,\n",
      "       1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424,\n",
      "       1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "       1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446,\n",
      "       1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457,\n",
      "       1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468,\n",
      "       1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "       1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490,\n",
      "       1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501,\n",
      "       1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512,\n",
      "       1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523,\n",
      "       1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534,\n",
      "       1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
      "       1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556,\n",
      "       1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567,\n",
      "       1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578,\n",
      "       1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589,\n",
      "       1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600,\n",
      "       1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611,\n",
      "       1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622,\n",
      "       1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1633, 1635,\n",
      "       1637, 1638]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_132 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_133 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 64, 109, 12)       36928     \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 64, 109, 12)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 32, 107, 10)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 32, 105, 8)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 32, 105, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_137 (Conv2D)          (None, 32, 103, 6)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 2)                 39554     \n",
      "=================================================================\n",
      "Total params: 273,762\n",
      "Trainable params: 273,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6882 - acc: 0.5532 - val_loss: 0.6861 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6799 - acc: 0.5673 - val_loss: 0.6765 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6668 - acc: 0.5673 - val_loss: 0.6369 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.6072 - acc: 0.5747 - val_loss: 0.5209 - val_acc: 0.6088\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4996 - acc: 0.7142 - val_loss: 0.5057 - val_acc: 0.8924\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4634 - acc: 0.8366 - val_loss: 0.4379 - val_acc: 0.9169\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4352 - acc: 0.8733 - val_loss: 0.4115 - val_acc: 0.9144\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.4146 - acc: 0.8935 - val_loss: 0.3926 - val_acc: 0.9193\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3992 - acc: 0.9002 - val_loss: 0.3852 - val_acc: 0.9095\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3904 - acc: 0.9106 - val_loss: 0.3777 - val_acc: 0.9095\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3857 - acc: 0.9070 - val_loss: 0.3623 - val_acc: 0.9169\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3690 - acc: 0.9186 - val_loss: 0.3671 - val_acc: 0.9095\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3662 - acc: 0.9198 - val_loss: 0.3510 - val_acc: 0.9218\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3467 - acc: 0.9247 - val_loss: 0.3479 - val_acc: 0.9169\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3413 - acc: 0.9259 - val_loss: 0.3422 - val_acc: 0.9169\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3323 - acc: 0.9272 - val_loss: 0.3396 - val_acc: 0.9218\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3246 - acc: 0.9315 - val_loss: 0.3319 - val_acc: 0.9193\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3167 - acc: 0.9382 - val_loss: 0.3241 - val_acc: 0.9291\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634/1634 [==============================] - 7s - loss: 0.3124 - acc: 0.9339 - val_loss: 0.3214 - val_acc: 0.9267\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 7s - loss: 0.3042 - acc: 0.9370 - val_loss: 0.3252 - val_acc: 0.9242\n",
      "('TRAIN:', array([   0,    1,    2, ..., 1635, 1637, 1638]), 'TEST:', array([1632, 1634, 1636, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
      "       1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
      "       1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668,\n",
      "       1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
      "       1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
      "       1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
      "       1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
      "       1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "       1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
      "       1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745,\n",
      "       1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756,\n",
      "       1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767,\n",
      "       1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778,\n",
      "       1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789,\n",
      "       1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
      "       1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811,\n",
      "       1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822,\n",
      "       1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833,\n",
      "       1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844,\n",
      "       1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,\n",
      "       1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866,\n",
      "       1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877,\n",
      "       1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888,\n",
      "       1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899,\n",
      "       1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910,\n",
      "       1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921,\n",
      "       1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932,\n",
      "       1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943,\n",
      "       1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954,\n",
      "       1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965,\n",
      "       1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976,\n",
      "       1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
      "       1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
      "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
      "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n",
      "       2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n",
      "       2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_138 (Conv2D)          (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_139 (Conv2D)          (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "conv2d_140 (Conv2D)          (None, 64, 109, 12)       36928     \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 64, 109, 12)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_141 (Conv2D)          (None, 32, 107, 10)       18464     \n",
      "_________________________________________________________________\n",
      "conv2d_142 (Conv2D)          (None, 32, 105, 8)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 32, 105, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_143 (Conv2D)          (None, 32, 103, 6)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 19776)             0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 2)                 39554     \n",
      "=================================================================\n",
      "Total params: 273,762\n",
      "Trainable params: 273,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.6862 - acc: 0.5630 - val_loss: 0.6830 - val_acc: 0.5676\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.6841 - acc: 0.5672 - val_loss: 0.6763 - val_acc: 0.5676\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.6718 - acc: 0.5678 - val_loss: 0.6420 - val_acc: 0.5676\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.6402 - acc: 0.5660 - val_loss: 0.5928 - val_acc: 0.5676\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.5656 - acc: 0.6186 - val_loss: 0.5327 - val_acc: 0.6781\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.4831 - acc: 0.7628 - val_loss: 0.5123 - val_acc: 0.8698\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.4464 - acc: 0.8674 - val_loss: 0.4933 - val_acc: 0.8919\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.4200 - acc: 0.8900 - val_loss: 0.4822 - val_acc: 0.9017\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.4668 - acc: 0.8625 - val_loss: 0.4782 - val_acc: 0.9017\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3947 - acc: 0.9028 - val_loss: 0.4793 - val_acc: 0.9066\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3819 - acc: 0.9083 - val_loss: 0.4787 - val_acc: 0.9042\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3697 - acc: 0.9083 - val_loss: 0.4830 - val_acc: 0.9066\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3615 - acc: 0.9089 - val_loss: 0.4842 - val_acc: 0.9042\n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3542 - acc: 0.9101 - val_loss: 0.4945 - val_acc: 0.9091\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3431 - acc: 0.9187 - val_loss: 0.5136 - val_acc: 0.9115\n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3443 - acc: 0.9175 - val_loss: 0.4926 - val_acc: 0.9017\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3321 - acc: 0.9205 - val_loss: 0.5187 - val_acc: 0.9066\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3248 - acc: 0.9230 - val_loss: 0.5147 - val_acc: 0.9115\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3185 - acc: 0.9267 - val_loss: 0.5148 - val_acc: 0.9165\n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 7s - loss: 0.3115 - acc: 0.9297 - val_loss: 0.5220 - val_acc: 0.9115\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0880 - acc: 0.9713     \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0924 - acc: 0.9664     \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0857 - acc: 0.9719     \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0780 - acc: 0.9756     \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0744 - acc: 0.9756     \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0697 - acc: 0.9786     \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0753 - acc: 0.9737     \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0705 - acc: 0.9780     \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0751 - acc: 0.9725     \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0686 - acc: 0.9768     \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0725 - acc: 0.9737     \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0618 - acc: 0.9811     \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0700 - acc: 0.9749     \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0577 - acc: 0.9829     \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0576 - acc: 0.9829     \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0562 - acc: 0.9847     \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0559 - acc: 0.9817     \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0556 - acc: 0.9817     \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0556 - acc: 0.9817     \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 4s - loss: 0.0543 - acc: 0.9798     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91776798825256978"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try cnn_deep with theano - MaxPooling removed\n",
    "#cnn first attempt - new dataset + theano (to use \" channel_first \")\n",
    "num_classes = 2\n",
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_45 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 31, 51, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 15, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 13, 23, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 13, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 11, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 7392)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 14786     \n",
      "=================================================================\n",
      "Total params: 183,330\n",
      "Trainable params: 183,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.6853 - acc: 0.5606 - val_loss: 0.6844 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.6640 - acc: 0.5673 - val_loss: 0.6186 - val_acc: 0.5917\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.5708 - acc: 0.6952 - val_loss: 0.4496 - val_acc: 0.7995\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.5239 - acc: 0.7729 - val_loss: 0.4175 - val_acc: 0.8191\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3808 - acc: 0.8207 - val_loss: 0.3948 - val_acc: 0.8655\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.3228 - acc: 0.8605 - val_loss: 0.4076 - val_acc: 0.8362\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2983 - acc: 0.8752 - val_loss: 0.3825 - val_acc: 0.8680\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2801 - acc: 0.8874 - val_loss: 0.3426 - val_acc: 0.8900\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2806 - acc: 0.8831 - val_loss: 0.3351 - val_acc: 0.9120\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2501 - acc: 0.8911 - val_loss: 0.3494 - val_acc: 0.8851\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2406 - acc: 0.9027 - val_loss: 0.3132 - val_acc: 0.9046\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2413 - acc: 0.9015 - val_loss: 0.3144 - val_acc: 0.8998\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2233 - acc: 0.9058 - val_loss: 0.2909 - val_acc: 0.9144\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2190 - acc: 0.9113 - val_loss: 0.2959 - val_acc: 0.8998\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2155 - acc: 0.9168 - val_loss: 0.2881 - val_acc: 0.9022\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2122 - acc: 0.9180 - val_loss: 0.2767 - val_acc: 0.9218\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2052 - acc: 0.9180 - val_loss: 0.2796 - val_acc: 0.9193\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2005 - acc: 0.9247 - val_loss: 0.3124 - val_acc: 0.8973\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2281 - acc: 0.9149 - val_loss: 0.2805 - val_acc: 0.9071\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.1900 - acc: 0.9253 - val_loss: 0.2806 - val_acc: 0.9071\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_51 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 31, 51, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 15, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 13, 23, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 13, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 11, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 7392)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 14786     \n",
      "=================================================================\n",
      "Total params: 183,330\n",
      "Trainable params: 183,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.6853 - acc: 0.5698 - val_loss: 0.6906 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.6759 - acc: 0.5673 - val_loss: 0.6563 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.6107 - acc: 0.6640 - val_loss: 0.4657 - val_acc: 0.8044\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.5095 - acc: 0.7613 - val_loss: 0.4879 - val_acc: 0.7555\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.4270 - acc: 0.8164 - val_loss: 0.3906 - val_acc: 0.8020\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.3449 - acc: 0.8525 - val_loss: 0.3340 - val_acc: 0.8484\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3126 - acc: 0.8764 - val_loss: 0.5199 - val_acc: 0.8020\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2914 - acc: 0.8898 - val_loss: 0.3528 - val_acc: 0.8435\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2759 - acc: 0.8990 - val_loss: 0.3696 - val_acc: 0.8484\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2598 - acc: 0.8941 - val_loss: 0.5087 - val_acc: 0.8484\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2659 - acc: 0.8966 - val_loss: 0.3149 - val_acc: 0.8680\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2284 - acc: 0.9094 - val_loss: 0.2999 - val_acc: 0.8753\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2265 - acc: 0.9113 - val_loss: 0.3975 - val_acc: 0.8582\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.3397 - acc: 0.8807 - val_loss: 0.3274 - val_acc: 0.8631\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2076 - acc: 0.9162 - val_loss: 0.2851 - val_acc: 0.8778\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.1951 - acc: 0.9241 - val_loss: 0.3052 - val_acc: 0.8753\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2036 - acc: 0.9204 - val_loss: 0.3000 - val_acc: 0.8729\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.1936 - acc: 0.9290 - val_loss: 0.3831 - val_acc: 0.8680\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.1844 - acc: 0.9296 - val_loss: 0.2828 - val_acc: 0.8851\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.1810 - acc: 0.9308 - val_loss: 0.3212 - val_acc: 0.8778\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([ 813,  815,  819,  820,  821,  822,  824,  825,  826,  827,  828,\n",
      "        829,  830,  831,  832,  833,  834,  835,  836,  837,  838,  839,\n",
      "        840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081,\n",
      "       1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092,\n",
      "       1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103,\n",
      "       1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125,\n",
      "       1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136,\n",
      "       1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "       1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n",
      "       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169,\n",
      "       1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,\n",
      "       1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "       1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202,\n",
      "       1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1213, 1214,\n",
      "       1215, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1229, 1230, 1233,\n",
      "       1236, 1237]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_57 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 31, 51, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 15, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 13, 23, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 13, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 11, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 7392)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 14786     \n",
      "=================================================================\n",
      "Total params: 183,330\n",
      "Trainable params: 183,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.6859 - acc: 0.5618 - val_loss: 0.6798 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.6805 - acc: 0.5673 - val_loss: 0.6673 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.6710 - acc: 0.6010 - val_loss: 0.6057 - val_acc: 0.7604\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.5084 - acc: 0.7613 - val_loss: 0.3998 - val_acc: 0.8484\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.4745 - acc: 0.7913 - val_loss: 0.4009 - val_acc: 0.8435\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3764 - acc: 0.8409 - val_loss: 0.3839 - val_acc: 0.8337\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3450 - acc: 0.8501 - val_loss: 0.3219 - val_acc: 0.8557\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3160 - acc: 0.8672 - val_loss: 0.3275 - val_acc: 0.8557\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2991 - acc: 0.8819 - val_loss: 0.3238 - val_acc: 0.8533\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2987 - acc: 0.8776 - val_loss: 0.2686 - val_acc: 0.8949\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2730 - acc: 0.8856 - val_loss: 0.2676 - val_acc: 0.8875\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2695 - acc: 0.8923 - val_loss: 0.3362 - val_acc: 0.8484\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2442 - acc: 0.9021 - val_loss: 0.2587 - val_acc: 0.8851\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2358 - acc: 0.9039 - val_loss: 0.2921 - val_acc: 0.8729\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2467 - acc: 0.8953 - val_loss: 0.2583 - val_acc: 0.8851\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2220 - acc: 0.9064 - val_loss: 0.2504 - val_acc: 0.8949\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2192 - acc: 0.9125 - val_loss: 0.2998 - val_acc: 0.8729\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2304 - acc: 0.9051 - val_loss: 0.2754 - val_acc: 0.8802\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2131 - acc: 0.9143 - val_loss: 0.2324 - val_acc: 0.9022\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2149 - acc: 0.9162 - val_loss: 0.2494 - val_acc: 0.8851\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([1209, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1234, 1235,\n",
      "       1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248,\n",
      "       1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,\n",
      "       1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270,\n",
      "       1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281,\n",
      "       1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292,\n",
      "       1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "       1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,\n",
      "       1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325,\n",
      "       1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,\n",
      "       1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "       1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358,\n",
      "       1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369,\n",
      "       1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380,\n",
      "       1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391,\n",
      "       1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402,\n",
      "       1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413,\n",
      "       1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424,\n",
      "       1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "       1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446,\n",
      "       1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457,\n",
      "       1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468,\n",
      "       1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "       1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490,\n",
      "       1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501,\n",
      "       1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512,\n",
      "       1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523,\n",
      "       1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534,\n",
      "       1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
      "       1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556,\n",
      "       1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567,\n",
      "       1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578,\n",
      "       1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589,\n",
      "       1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600,\n",
      "       1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611,\n",
      "       1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622,\n",
      "       1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1633, 1635,\n",
      "       1637, 1638]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_63 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 31, 51, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 15, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 13, 23, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 13, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 11, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 7392)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 14786     \n",
      "=================================================================\n",
      "Total params: 183,330\n",
      "Trainable params: 183,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.6827 - acc: 0.5655 - val_loss: 0.6743 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.6662 - acc: 0.5900 - val_loss: 0.6250 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.8797 - acc: 0.6701 - val_loss: 0.5842 - val_acc: 0.7042\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.4691 - acc: 0.7968 - val_loss: 0.4028 - val_acc: 0.8313\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3874 - acc: 0.8403 - val_loss: 0.3544 - val_acc: 0.8460\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3710 - acc: 0.8403 - val_loss: 0.3854 - val_acc: 0.8215\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3826 - acc: 0.8470 - val_loss: 0.3703 - val_acc: 0.8362\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3208 - acc: 0.8672 - val_loss: 0.3645 - val_acc: 0.8386\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3248 - acc: 0.8690 - val_loss: 0.3179 - val_acc: 0.8704\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.3026 - acc: 0.8739 - val_loss: 0.2904 - val_acc: 0.8680\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2827 - acc: 0.8892 - val_loss: 0.3008 - val_acc: 0.8778\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2821 - acc: 0.8862 - val_loss: 0.2780 - val_acc: 0.8778\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2752 - acc: 0.8898 - val_loss: 0.2461 - val_acc: 0.9046\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 81s - loss: 0.2514 - acc: 0.8990 - val_loss: 0.2598 - val_acc: 0.8973\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2611 - acc: 0.9039 - val_loss: 0.2411 - val_acc: 0.8998\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2537 - acc: 0.8990 - val_loss: 0.2713 - val_acc: 0.8949\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2385 - acc: 0.9027 - val_loss: 0.2462 - val_acc: 0.8998\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2461 - acc: 0.8990 - val_loss: 0.2193 - val_acc: 0.9120\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2287 - acc: 0.9131 - val_loss: 0.2478 - val_acc: 0.8998\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 82s - loss: 0.2238 - acc: 0.9113 - val_loss: 0.2329 - val_acc: 0.9144\n",
      "('TRAIN:', array([   0,    1,    2, ..., 1635, 1637, 1638]), 'TEST:', array([1632, 1634, 1636, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
      "       1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
      "       1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668,\n",
      "       1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
      "       1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
      "       1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
      "       1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
      "       1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "       1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
      "       1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745,\n",
      "       1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756,\n",
      "       1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767,\n",
      "       1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778,\n",
      "       1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789,\n",
      "       1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
      "       1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811,\n",
      "       1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822,\n",
      "       1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833,\n",
      "       1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844,\n",
      "       1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,\n",
      "       1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866,\n",
      "       1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877,\n",
      "       1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888,\n",
      "       1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899,\n",
      "       1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910,\n",
      "       1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921,\n",
      "       1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932,\n",
      "       1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943,\n",
      "       1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954,\n",
      "       1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965,\n",
      "       1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976,\n",
      "       1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
      "       1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
      "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
      "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n",
      "       2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n",
      "       2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_69 (Conv2D)           (None, 72, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 70, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 31, 51, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 15, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 13, 23, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 13, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 11, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 7392)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 14786     \n",
      "=================================================================\n",
      "Total params: 183,330\n",
      "Trainable params: 183,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.6832 - acc: 0.5623 - val_loss: 0.6750 - val_acc: 0.5676\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.6517 - acc: 0.5972 - val_loss: 0.5669 - val_acc: 0.7445\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.5877 - acc: 0.7433 - val_loss: 0.5551 - val_acc: 0.7297\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.4379 - acc: 0.8136 - val_loss: 0.3412 - val_acc: 0.8428\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.3620 - acc: 0.8484 - val_loss: 0.3278 - val_acc: 0.8575\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.3542 - acc: 0.8411 - val_loss: 0.3442 - val_acc: 0.8452\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.4103 - acc: 0.8441 - val_loss: 0.3120 - val_acc: 0.8698\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.3021 - acc: 0.8851 - val_loss: 0.3017 - val_acc: 0.8673\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2746 - acc: 0.8924 - val_loss: 0.2981 - val_acc: 0.8673\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2778 - acc: 0.8863 - val_loss: 0.3053 - val_acc: 0.8673\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2589 - acc: 0.8979 - val_loss: 0.2804 - val_acc: 0.8821\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2457 - acc: 0.8949 - val_loss: 0.2701 - val_acc: 0.8943\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2468 - acc: 0.9010 - val_loss: 0.2867 - val_acc: 0.8821\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636/1636 [==============================] - 82s - loss: 0.2342 - acc: 0.9022 - val_loss: 0.2501 - val_acc: 0.9066\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2382 - acc: 0.9034 - val_loss: 0.2557 - val_acc: 0.8968\n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2206 - acc: 0.9138 - val_loss: 0.2879 - val_acc: 0.8894\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2169 - acc: 0.9083 - val_loss: 0.2541 - val_acc: 0.9017\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2108 - acc: 0.9101 - val_loss: 0.3841 - val_acc: 0.8698\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2117 - acc: 0.9120 - val_loss: 0.2571 - val_acc: 0.9042\n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 82s - loss: 0.2010 - acc: 0.9181 - val_loss: 0.2627 - val_acc: 0.9042\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 71s - loss: 0.1335 - acc: 0.9548    \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1319 - acc: 0.9548    \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1344 - acc: 0.9566    \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1322 - acc: 0.9505    \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1299 - acc: 0.9529    \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1299 - acc: 0.9584    \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 71s - loss: 0.1264 - acc: 0.9597    \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1225 - acc: 0.9535    \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1198 - acc: 0.9615    \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1169 - acc: 0.9578    \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1237 - acc: 0.9529    \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 71s - loss: 0.1137 - acc: 0.9603    \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 71s - loss: 0.1147 - acc: 0.9590    \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1123 - acc: 0.9621    \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1092 - acc: 0.9639    \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1081 - acc: 0.9633    \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1100 - acc: 0.9603    \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1058 - acc: 0.9609    \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 71s - loss: 0.1080 - acc: 0.9621    \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 70s - loss: 0.1049 - acc: 0.9609    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9192364170337739"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn deeper (try to beat 92%)\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n",
    "num_classes = 2\n",
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `accuracy_score(Y_test, cls_predictions)`:0.9192364170337739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe0c733fe90>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4lFXa+PHvSQ+ppEMoKdQktBARQQQsiKiogBR1XSu7\nvJZ1dXdl393Xdd3yc5t1XevadlXEjkqxoVKUKh1CQpMECCGBQEL6nN8fZxKHkDJJ5pmZJPfnuuaa\nmWeeeebOMMw9zyn3UVprhBBCCAAfTwcghBDCe0hSEEIIUU+SghBCiHqSFIQQQtSTpCCEEKKeJAUh\nhBD1JCkIIYSoJ0lBCCFEPUkKQggh6vlZeXCl1GTgccAXeEFr/XCDxx8FJtrvdgPitNaRzR0zJiZG\nJyUlWRCtEEJ0Xhs2bDimtY5taT/LkoJSyhd4CrgEyAPWKaUWaa131O2jtf65w/53ASNaOm5SUhLr\n16+3IGIhhOi8lFIHnNnPyuajUUCu1nqv1roKWABc1cz+c4A3LIxHCCFEC6xMConAQYf7efZtZ1FK\n9QWSgS+aeHyuUmq9Ump9YWGhywMVQghheEtH82zgba11bWMPaq2f01pnaa2zYmNbbBITQgjRRlZ2\nNOcDvR3u97Jva8xs4A4LYxFCeKHq6mry8vKoqKjwdCidRlBQEL169cLf379Nz7cyKawD+iulkjHJ\nYDZwXcOdlFKDgO7ANxbGIoTwQnl5eYSFhZGUlIRSytPhdHhaa4qKisjLyyM5OblNx7Cs+UhrXQPc\nCSwDdgILtdbblVIPKaWmOuw6G1igZbUfIbqciooKoqOjJSG4iFKK6Ojodp15WTpPQWu9GFjcYNsD\nDe4/aGUMQgjvJgnBtdr7fnpLR7Pl9h0r469Ld1FTa/N0KEII4bW6TFL4ZPsR/vXlHn780lqOl1V5\nOhwhhIcVFRUxfPhwhg8fTkJCAomJifX3q6qc+464+eabyc7Obnafp556itdee80VIbuF6mhN+VlZ\nWbqtM5oXrj/Ib9/bRnxEIM/9KIvBPcJdHJ0QojV27tzJ4MGDPR0GDz74IKGhofziF784Y7vWGq01\nPj4d6/dzY++rUmqD1jqrped2rL+0PWy1zEw8zps/GU1VjY1p/1rNx1sOezoqIYSXyc3NJS0tjeuv\nv5709HQOHz7M3LlzycrKIj09nYceeqh+3/PPP59NmzZRU1NDZGQk8+fPZ9iwYZx33nkcPXoUgN/+\n9rc89thj9fvPnz+fUaNGMXDgQFavXg1AWVkZ06dPJy0tjRkzZpCVlcWmTZvc/8djcUezV/nyYVj5\nKCN+sZsP7zyfn/53A3e8vpEdh1O595KB+PpIZ5cQnvT7D7ez49BJlx4zrWc4v7syvdXP27VrF6++\n+ipZWeaH9cMPP0xUVBQ1NTVMnDiRGTNmkJaWdsZzSkpKGD9+PA8//DD33nsvL774IvPnzz/r2Fpr\n1q5dy6JFi3jooYdYunQpTz75JAkJCbzzzjts3ryZzMzMtv3BLtB1zhTSpoKtGra/S1x4EG/MHc3s\nc3rz1PI93P7qek5WVHs6QiGEl0hNTa1PCABvvPEGmZmZZGZmsnPnTnbs2HHWc4KDg7nssssAGDly\nJPv372/02NOmTTtrn5UrVzJ79mwAhg0bRnp66xOZq3SdM4X4DIhLg81vwjm3Eejny/+bNoT0nuH8\n/sMdXP3PVTx3Yxb94kI9HakQXVJbftFbJSQkpP52Tk4Ojz/+OGvXriUyMpIbbrih0XkAAQEB9bd9\nfX2pqalp9NiBgYEt7uNJXedMQSkYOgvy1kLxXvsmxY/OS+K1286lpLyaa55axec7CzwcqBDCm5w8\neZKwsDDCw8M5fPgwy5Ytc/lrjB07loULFwKwdevWRs9E3KXrJAWAIdcCCrYsPGPzuSnRfHjX+fSN\n6cZtr67nyc9zsNk61qgsIYQ1MjMzSUtLY9CgQdx4442MHTvW5a9x1113kZ+fT1paGr///e9JS0sj\nIiLC5a/jjC41JBWAV66Ekjy4a6M5e3BQUV3L/He28P6mQ0xOT+AfM4cREth1WtiEcDdvGZLqaTU1\nNdTU1BAUFEROTg6TJk0iJycHP7+2ff/IkNTWGDrLNB/lnZ1Ygvx9eXTWcH4zZTCf7DjCtH+t5kBR\nmQeCFEJ0JaWlpYwdO5Zhw4Yxffp0nn322TYnhPbqeklh8FTwC4Itbzb6sFKK2y9I4ZVbRnHkZAVT\n/7mKFTmysI8QwjqRkZFs2LCBzZs3s2XLFiZNmuSxWLpeUggKh4FTYNs7UNP0VPZx/WNZdOdYEsKD\n+PGLa3n+6710tKY2IYRora6XFACGzYbyYtjzebO79Y0O4d3/GcOktAT+tHgn9y3cLB3QQohOrWsm\nhdQLoVsMbF7Q4q4hgX786/pM7rqwH+9+l8/C9QdbfI4QQnRUXTMp+PpDxnTIXgIVJS3u7uOjuPeS\nAYxKiuLhpbsoliqrQohOqmsmBTCjkGorYccHTu2ulOIPV2dQWlHDw0t2WhycEMIdJk6ceNZktMce\ne4x58+Y1+ZzQUFP14NChQ8yYMaPRfSZMmEBLQ+cfe+wxTp8+XX9/ypQpnDhxwtnQLdN1k0JiJkT3\nM2UvnDQwIYxbz09m4fo81u8vtjA4IYQ7zJkzhwULzmxGXrBgAXPmzGnxuT179uTtt99u82s3TAqL\nFy8mMjKyzcdzla6bFOrKXhxYCSec7ye4+6L+9IwI4jfvbaNaVnETokObMWMGH3/8cf2iOvv37+fQ\noUOMGDGCiy66iMzMTIYMGcIHH5zdorB//34yMjIAKC8vZ/bs2QwePJhrrrmG8vLy+v3mzZtXX3b7\nd7/7HQBPPPEEhw4dYuLEiUycOBGApKQkjh07BsAjjzxCRkYGGRkZ9WW39+/fz+DBg7n99ttJT09n\n0qRJZ7yOq3Tt6bpDroXlf4KtC2HcfU49JSTQjweuTOen/93AK6v3c9u4FIuDFKKLWDIfjmx17TET\nhsBlDzf5cFRUFKNGjWLJkiVcddVVLFiwgJkzZxIcHMx7771HeHg4x44dY/To0UydOrXJ9Y+ffvpp\nunXrxs6dO9myZcsZpa//9Kc/ERUVRW1tLRdddBFbtmzh7rvv5pFHHmH58uXExMSccawNGzbw0ksv\nsWbNGrTWnHvuuYwfP57u3buTk5PDG2+8wfPPP8/MmTN55513uOGGG1zzXtl13TMFgKhk6D3aNCG1\nYg7CpenxXDgojkc/3c3hEtdnaiGE+zg2IdU1HWmt+d///V+GDh3KxRdfTH5+PgUFTRfL/Prrr+u/\nnIcOHcrQoUPrH1u4cCGZmZmMGDGC7du3t1jsbuXKlVxzzTWEhIQQGhrKtGnTWLFiBQDJyckMHz4c\naL48d3t07TMFgGGz4KOfw+HN0HO4U09RSvHglelc8uhX/OGjHfzr+pEWBylEF9DML3orXXXVVfz8\n5z9n48aNnD59mpEjR/Lyyy9TWFjIhg0b8Pf3JykpqdFy2S3Zt28ff//731m3bh3du3fnpptuatNx\n6tSV3QZTetuK5qOufaYAkHY1+PifVTm1JX2iu3HXhf1YvPUIX2YftSg4IYTVQkNDmThxIrfcckt9\nB3NJSQlxcXH4+/uzfPlyDhw40OwxLrjgAl5//XUAtm3bxpYtWwBTdjskJISIiAgKCgpYsmRJ/XPC\nwsI4derUWccaN24c77//PqdPn6asrIz33nuPcePGuerPbZEkhW5RMOBS2PoW1LZuwYvbL0ghJTaE\nBz7YTkV1rUUBCiGsNmfOHDZv3lyfFK6//nrWr1/PkCFDePXVVxk0aFCzz583bx6lpaUMHjyYBx54\ngJEjTevBsGHDGDFiBIMGDeK66647o+z23LlzmTx5cn1Hc53MzExuuukmRo0axbnnnsttt93GiBEj\nXPwXN63rlc5uzI5FsPBHcMM70O/iVj11Ve4xrn9hDXdf1J97Lxng2riE6OSkdLY1pHR2ew24FIIi\nWjVnoc7YfjFMHdaTZ77cw75jUmZbCNGxSVIA8AuE9Gtg10dQWdrqp//28sEE+vnwwAfbpJKqEKJD\nk6RQZ+gsqD5tEkMrxYUHcd+kAazIOcbHWw9bEJwQnZf8kHKt9r6fkhTq9B4NkX2aXHynJTeM7kt6\nz3Ae+nAHpyqqXRycEJ1TUFAQRUVFkhhcRGtNUVERQUFBbT6GzFOo4+MDQ2bCykfg1BEIS2jV0/18\nffjj1RlMe3o1j36awwNXplkUqBCdR69evcjLy6OwUFY3dJWgoCB69erV5udLUnA0dBas+DtsfRvG\n3Nnqp4/o0505o/rw8up9TB+ZSHrPCAuCFKLz8Pf3Jzk52dNhCAfSfOQodgD0HAFbWl58pyn3XzqI\n7t0C+O3722SVNiFEh2NpUlBKTVZKZSulcpVS85vYZ6ZSaodSartS6nUr43HK0NmmKFdB8/VJmhLR\nzZ9fTxnMd9+fkFXahBAdjmVJQSnlCzwFXAakAXOUUmkN9ukP/BoYq7VOB+6xKh6nZUwH5dvmDmeA\n6ZmJskqbEKJDsvJMYRSQq7Xeq7WuAhYAVzXY53bgKa31cQCtteeLCIXGQr+LTNkLW9vWS1BK8cdr\nZJU2IUTHY2VSSAQc20/y7NscDQAGKKVWKaW+VUpNbuxASqm5Sqn1Sqn1bhmlMHQWnMw3C/C00YD4\nMG4dJ6u0CSE6Fk93NPsB/YEJwBzgeaXUWevRaa2f01pnaa2zYmNjrY9q4BQICG1XExLA3RfKKm1C\niI7FyqSQD/R2uN/Lvs1RHrBIa12ttd4H7MYkCc8K6AaDp5pCedVtr1ceEujH76amk11wipdX7Xdd\nfEIIYRErk8I6oL9SKlkpFQDMBhY12Od9zFkCSqkYTHPSXgtjct6wWVB5ErKXtLxvMyal2Vdp+0xW\naRNCeD/LkoLWuga4E1gG7AQWaq23K6UeUkpNte+2DChSSu0AlgO/1FoXWRVTqySNg7Ae7W5CUkrx\n+6np1No0D33YtmGuQgjhLpbOaNZaLwYWN9j2gMNtDdxrv3gXH18Yci18+y8oOwYhMS0/pwm9o7ox\nb0Iqj32Ww4GiMvpGh7gwUCGEcB1PdzR7t6GzwFYD295t96GuGNoTgFW53nEiJIQQjZGk0JyEDIjP\naHcTEkBqbAjx4YGs2nPMBYEJIYQ1JCm0ZOhMyF8Px3LbdRilFGNSY/hmT5HURBJCeC1JCi0Zci2g\nYOvCdh9qTGo0xWVV7Dpyqv1xCSGEBSQptCS8JyRfYJqQ2rkQyNh+prN6tTQhCSG8lCQFZwybDcf3\nw8G17TpMz8hgkmNCWL1HOpuFEN5JkoIzBl8JfsHtWmehzpjUaNbsLZKyF0IIryRJwRmBYTDoctjy\nFnz1Vzi8pc1NSWNSYyirqmVL3gkXBymEEO0nScFZ438FcYNh+Z/h2XHwaAZ8fB/kfAY1lU4f5rzU\naABWy3wFIYQXkjWanRU7EG77FEqPwu5lsHspbHod1r0A/iGQOhEGXgb9LzVrMjQhKiSAtB7hrNpz\njLsu8nztPyGEcCRJobVC4yDzR+ZSXQH7V5iiebuXwq6PAAW9zoGBk2HAZebsQqkzDjG2XzSvrD5A\neVUtwQG+nvk7hBCiEdJ81B7+QdD/ErjiEfj5dvjJ1zDh12Crhs8fgqfPg8eHwuJfwZ7lUGOW5hzT\nL4aqWhvrD8jiO0II7yJnCq6iFPQYZi4T7oeTh83Zw+6lsPEVWPsshMbDXRsZlRSFn49iVW4R4/q7\nYdEgIYRwkpwpWCW8B2TdDNe9Cb/aB5c8BKUFULCdkEA/hveO5BuZxCaE8DKSFNwhoBukTzO3C7YC\npglpa34JJeXVHgxMCCHOJEnBXSJ6QVAEHNkGwNjUaGwavt0rQ1OFEN5DkoK7KGXKcBeYpDC8TyRB\n/j6szpUmJCGE95Ck4E7xGVCwA2w2Av18OScpSuogCSG8iiQFd0rIgOoyOL4PMFVTc46WcvRkhYcD\nE0IIQ5KCO8VnmOuCun6FulLacrYghPAOkhTcKW4wKJ/6zua0nuFEBPuzSvoVhBBeQpKCO/kHQ3S/\n+jMFXx/F6BTTr6DbuYCPEEK4giQFd3MYgQSmXyH/RDnfF5/2YFBCCGFIUnC3hAw48T1UlABmfQWA\nVVJKWwjhBSQpuFt9Z/N2AFJjQ4gPD2SVlLwQQngBSQruVpcU7J3NSinGpsbwzZ4ibDbpVxBCeJYk\nBXcL7wnB3c/oVzgvNZrisiqyC055MDAhhJCk4H4Nyl2A6WwGZGiqEMLjJCl4QsIQe7mLWgB6RgaT\nHBMik9iEEB4nScET4tOhphyK99ZvGpMazZq9RVTX2jwYmBCiq5Ok4An1nc1b6zeNSY2hrKqWLXkl\nHgpKCCEkKXhG7CBQvvXDUsF0NgNSSlsI4VGWJgWl1GSlVLZSKlcpNb+Rx29SShUqpTbZL7dZGY/X\n8A+CmAFndDZHhQSQ1iNc5isIITzKsqSglPIFngIuA9KAOUqptEZ2fVNrPdx+ecGqeLxOQkb9XIU6\nY/tFs/HACcqraj0UlBCiq7PyTGEUkKu13qu1rgIWAFdZ+HodS3w6nMyD08X1m8b0i6Gq1sb6A8XN\nPFEIIaxjZVJIBA463M+zb2toulJqi1LqbaVU78YOpJSaq5Rar5RaX1hYaEWs7hc/xFwf3VG/aVRS\nFH4+SoamCiE8xtMdzR8CSVrrocCnwCuN7aS1fk5rnaW1zoqNjXVrgJZJOLPcBUBIoB/De0dKZ7MQ\nwmOsTAr5gOMv/172bfW01kVa60r73ReAkRbG411C46FbDBRsPWPzmH4xbM0voaS82kOBCSG6MiuT\nwjqgv1IqWSkVAMwGFjnuoJTq4XB3KrDTwni8i1KmX6FhZ3NqNDYN3+6VJiQhhPtZlhS01jXAncAy\nzJf9Qq31dqXUQ0qpqfbd7lZKbVdKbQbuBm6yKh6vlDAEju6E2pr6TSP6dCfI34dvpF9BCOEBflYe\nXGu9GFjcYNsDDrd/Dfzayhi8WnwG1FZC8R6IHQhAgJ8P5yRFSXE8IYRHeLqjuWtLOLvcBZiqqTlH\nSzl6ssIDQQkhujJJCp4UMxB8/M6Y2Qww1r5EpwxNFUK4myQFT/ILMImhQWdzWs9wIoL9WS0lL4QQ\nbiZJwdMSMs4ojAfg66MYnRLFqtwitJYlOoUQ7iNJwdPiM+DUoTPKXYDpV8g/Uc73xac9FJgQoiuS\npOBpTXQ2j0mtW6JT+hWEEO4jScHT6hbcadDZnBobQnx4oJTSFkK4lSQFTwuNg5C4s/oVlFKMTY3h\n2z1F2GzSryCEcA9JCt4gIeOs5iMwq7EVlVWRXXDKA0EJIboip5KCUipVKRVovz1BKXW3UirS2tC6\nkPgMKNwFtWcWwRvbr65fQZqQhBDu4eyZwjtArVKqH/Acpvrp65ZF1dUkDIHaKjiWc8bmnpHBJMeE\nyCQ2IYTbOJsUbPYCd9cAT2qtfwn0aOE5wlnx6ea6QWczwJjUaNbsLaK61ubmoIQQXZGzSaFaKTUH\n+DHwkX2bvzUhdUExA8A3oNGkMLZfDGVVtWzJK/FAYEKIrsbZpHAzcB7wJ631PqVUMvAf68LqYnz9\nTZXUI2cnhdEp0QCyGpsQwi2cSgpa6x1a67u11m8opboDYVrrv1gcW9cSP6TRM4WokADSeoTLfAUh\nhFs4O/roS6VUuFIqCtgIPK+UesTa0LqYhAwoLYDSwrMeGtsvmo0HTlBRXeuBwIQQXYmzzUcRWuuT\nwDTgVa31ucDF1oXVBTXX2dwvhqpaG+v3H3dzUEKIrsbZpOBnX095Jj90NAtXih9irhtJCqOSovDz\nUdKEJISwnLNJ4SHMWst7tNbrlFIpQE4LzxGtERINYT0a7WwOCfRjeO9I6WwWQljO2Y7mt7TWQ7XW\n8+z392qtp1sbWhcUn9HomQKYJqSt+SUcPSVLdAohrONsR3MvpdR7Sqmj9ss7SqleVgfX5cSnQ2E2\n1FSd9dA1IxJRSvHMl3s9EJgQoqtwtvnoJWAR0NN++dC+TbhSwhCwVcOx3Wc9lBwTwvTMRP675gCH\nS8o9EJwQoitwNinEaq1f0lrX2C8vA7EWxtU1NbG2Qp27LuyP1pqnlue6MSghRFfibFIoUkrdoJTy\ntV9uAKRKm6tF9wPfwEbLaAP0jurGrHN68+a6gxyUZTqFEBZwNincghmOegQ4DMwAbrIopq7L1w/i\nBjd5pgBw58T+KKV44nMZ/CWEcD1nRx8d0FpP1VrHaq3jtNZXAzL6yArxGWetwuYoISKIH43uyzsb\n89hbWOrGwIQQXUF7Vl6712VRiB8kZEBZIZwqaHKXeRNSCfTz5XE5WxBCuFh7koJyWRTiB/WdzY33\nKwDEhAZy09gkFm0+RPYRWapTCOE67UkKspq8FRLsSaGRmc2OfnJBCqEBfjz66dnDV4UQoq2aTQpK\nqVNKqZONXE5h5isIVwvuDuG9mu1XAIjsFsCt45JZuv0IW2UBHiGEizSbFLTWYVrr8EYuYVprP3cF\n2eXEpzc7AqnOLecnE9nNn0c+zXZDUO1zuKScf3ySTVWNLCsqhDdrT/ORsEpChpnVXFPZ7G7hQf7M\nvSCF5dmFbDjgvWW1a22an72xiSe/yOWTHUc8HY4QohmWJgWl1GSlVLZSKlcpNb+Z/aYrpbRSKsvK\neDqM+Ayw1UDhrhZ3vWlMEjGhAV59tvD8ir2s3V9MgJ8P73+X7+lwhBDNsCwpKKV8gaeAy4A0YI5S\nKq2R/cKAnwFrrIqlw0mwr63QQmczQLcAP+ZN6Meq3CJWe+F6CzsOneQfn2QzOT2Bm8ck8WV2IUWl\nzZ8BCSE8x8ozhVFArr3MdhWwALiqkf3+APwFkJrQdaJSwC+4xc7mOtef24eE8CAe+WQ3WnvPoLCK\n6lruefM7IrsF8OdpQ7gmM5Eam+bjrYc9HZoQoglWJoVE4KDD/Tz7tnpKqUygt9b64+YOpJSaq5Ra\nr5RaX1h49hrGnY6Pr73cRdNzFRwF+ftyx4X9WH/gOF/neM/Zwt+XZbO7oJS/zhhKVEgAgxLCGZQQ\nxrsbpQlJCG/lsY5mpZQP8AhwX0v7aq2f01pnaa2zYmO7SHHWhAzTfOTkL/9ZWb1JjAzmH59ke8XZ\nwurcY7ywch8/Gt2XiQPj6rdPy0xk08ET7DtW5sHohBBNsTIp5AO9He73sm+rEwZkAF8qpfYDo4FF\n0tlsFz8EyovhlHNNLQF+Pvzs4v5sySvh0x1Nl8hwh5Lyau57azMpMSH875TBZzw2dVgiSiEdzkJ4\nKSuTwjqgv1IqWSkVAMzGLNQDgNa6RGsdo7VO0lonAd8CU7XW6y2MqeOom9nsZL8CwLQRiSTHhPDI\np7ux2Tx3tvDAB9s4eqqSR2cNJzjA94zHEiKCGJsaw/ub8r3ijEYIcSbLkoLWuga4E1gG7AQWaq23\nK6UeUkpNtep1O404+0CtJtZWaIyfrw/3XNyfXUdOeawzd9HmQ3yw6RB3X9ifYb0jG93n6hGJHCg6\nzcbvT7g5OiFESyztU9BaL9ZaD9Bap2qt/2Tf9oDWelEj+06QswQHwZEQ0cepmc2OrhzakwHxoTz6\n2W5qat07e/hwSTm/fW8rw3tHcsfE1Cb3m5yRQJC/D+99l+fG6IQQzpAZzd6srrO5FXx8FPdeMoC9\nhWV8sOmQRYGdzWbT/OKtzVTXah6dNRw/36Y/WqGBfkxKS+CjLYel7IXoGMqPw/MXQd4GT0diOUkK\n3iw+A4pyoLq8VU+7ND2B9J7hPPb5bqrddLbw8ur9rMot4v+uSCM5JqTF/a8ZkciJ09V8mX3UDdEJ\n0U7ZSyF/PWx82dORWE6SgjeLTwdtc6rchSOlFL+YNJCDxeW8td76JpqcglM8vHQXFw2KY86o3i0/\nARjXP4bokADe3ySjkEQHsHuJuc5eCrbOfXYrScGbtaLcRUMTBsYyok8kT36RQ0V1rYsD+0FVjY17\n3txEaKAfD08filLOrb3k5+vDlcN68tnOo5SUV1sWnxDtVlMFuV9AaDyUHYX8zt2EJEnBm3VPBv+Q\nVnc2ww9nC4dLKliw9nsLgjMe+2w32w+d5OFpQ4gNC2zVc6dlJlJVY2OJlL0Q3uzAKqg6BRf/Hnz8\nILvZAgwdniQFb+bjA/FpbTpTABiTGs3olCj+uXwP5VWuP1tYt7+YZ77aw6ys3kxKT2j184ckRpAS\nG8J7MpFNeLPdS8EvCNKugr5jYNdiT0dkKUkK3i4+w5wptGGil1KK+yYN5FhpJa9+s9+lYZ2qqObe\nhZvo1b0b/3flWcVvnY5v2ohE1uwrJu/4aZfGJ4RLaA3ZSyD5AgjoBgMvh2PZULTH05FZRpKCt4tP\nh4oTcLJtv6bPSYriggGxPPPVHkora1wW1h8+2kH+8XIemTmM0MC2L8J31XBTI9Gdw2eFcFrhLjhx\nAAZMNvcHTTHX2Z33bEGSgrdrR2dznfsuGcDx09W8tHKfS0Jatv0IC9fnMW9CKllJUe06Vu+obpyT\n1J33vpOyF8ILZdtHHdUlhcg+pi5ZJ25CkqTg7eLTzbWTZbQbM6x3JJekxfP0V3u4981NvLRqH+v3\nF3O6qvVnDkdPVfDrd7eSkRjOzy4a0OaYHF0zohe5R0vZfuikS44nhMvsXgoJQyHCoer/wMvg4LdQ\nVuS5uCzU9vN+4R6BYdA9qVWF8RrzwBVpPPTRDlbmHuNde8euj4LU2FCGJEaQkRjB0F4RpPUMp1tA\n4x8LrTX3v72FssoaHps1nAA/1/ymuHxIDx5ctJ13N+aTkRjhkmMK0W5lx+DgWhj/qzO3D5oCX/8V\ncpbB8Os8E5uFJCl0BPGtL3fRUO+objx/o6lKXnCygq15JWzNL2FbfonTieL1td+zPLuQB69Mo19c\nWLv/rDoR3fy5cFAcizYf4n+nDGq2RIYQbpPzCaB/aDqq02M4hPWEXR9LUhAeEp9hOraqTpsREO09\nXHgQ8WlBXJwWX7/NmURx8PhpxvWP4cbzktodQ0NXj0hk6fYjrNpTxPgBXWQhJeHddi+F0ASTBBwp\nZZqQNr9hStD4B3smPotIUugIEjJMuYujO6HXSEtewplEERrkx99mDMPHx7lZy60xcVAsEcH+vLcx\nT5KC8Lx2S7Z7AAAesUlEQVS6WcwZ08x8oYYGTYH1/4Z9X8OAS90fn4UkKXQE8XUL7my1LCk0+rKN\nJAqrBPr5cvnQHry3MZ+yyhpC2jHMVYh2O7DSzGIeeFnjjyeNg4Aw04TUyZKCNN52BJF9zQewnZ3N\n3m7aiETKq2tZtv2Ip0MRXV22fRZz8vjGH/cLhP4XmyamTlYgT5JCR9DOchftpjV8vwZWP2n6NSwy\nsm93ekcFS9kL4Vlam6qoKROa78MbOAVKC+DQRndF5haSFDqKunIX1RXue82q07DxVXj2AnhxEnzy\nW3jxUiixphy3UoprhieyKvcYR0+68e8UwtHRnXDi+5abhfpfAsrXNCF1IpIUOoqksVB5Ev7eH96b\nBzmfQa1FJaeL98Ky38Ajg2HRXWCrhSsehZn/geP74bmJZvy2Ba4akYhNm7WehfCI3Q1mMTcluLv5\nf9nJSl5Ib15HkT4NgiJh27uw80PY/DoER5nKjRnTTfVGH9+2H99mgz2fw9rnIOdTc6zBV8KoudDn\nPDMMDyBmALwxG16+HK583OXjtFNjQxnWK4J3N+Zz27gUlx5bCKdkL4UewyC8Z8v7DpwCS+ebAnnR\nTa9L3pHImUJHoRT0uwiufgp+mQOzX4fUibDlTXjlCng0HZb+GvLWt66i6uli01fwZCa8NgMOb4bx\n98M92+Dal02ycVw4J24Q3P4F9BkN788zZxQ215blvmZEIjsOnyT7yCmXHleIFpUdg7x1MKCJUUcN\nDawrkLfEupjcTJJCR+QXCIMuhxkvwi9zzXXiSFj3ArxwETw+FD57EI5sbTpBHN4MH9wJj6SZvoKw\nBHOce7bBxF9DeI+mX79bFNzwLpxzO3zzT3h9FlSUuOzPu2JYT3x9lCzVKdyvbhbzwBaajup07/vD\n5NJOQpqPOrqAENN8lDHdfDHv+hi2vQOrnoCVj5rmnrrHI/vCzkWmiejgGvALhqEzYdTtP1RjdZav\nP1z+dzMqavEv4YWLYc4Cl5xCx4QGMn5ALB98l88vJw20ZLKcEI3KXgJhPc6exdycgVNgxd9NgbyQ\naOticxM5U+hMgiJMG/8N78AvdpvO4ZA4+PJh+GcW/DUZ3rkVSo/CpX+G+3bC1CdanxAcZd0CN35g\nTrufvxD2LHfJn3L1iEQOlVSwZl+xS44nRItqKmHPF2bUkZNrjQNmgpu22c8yOj5JCp1VSIz5wr75\nY7h3B1z6/yDtarj+HbhrI5x3hxk94QpJ58Pc5aZj7r/TYc2zbVopztElg+MJDfTjve+sGf4qxFn2\nr4SqUuf7E+r0HGEK5HWStZslKXQF4T3hvP8xndT9L268lkt7dU+CWz8xv7KW/Ao+/JmpH9NGwQG+\nTM5IYMnWI1RUu359aSHOsnupaVJNaWIWc1PqCuTlfuHeeUQWkaQgXCcwDGa9BuffCxtfgf9cbZqV\n2mjaiEROVdbw2c4CFwYpRCO0NkNRUya0rerpoClQXWYK5HVwkhSEa/n4wMW/g2kvQP4GeH5im8tz\nnJsSTUJ4EO9L2QthtaM7oOR750cdNVRXIK8TNCFJUhDWGHot3LzYzLr+96Q2lQLw9VFcNbwnX2YX\nUlzW9qYoIVpUN8+gfxsrnvoFmnlE2Us6fIE8SQrCOokj4fblEDsQFlxnRkG1cl3bazITqbFpPtoi\nZS+EhXYvM8NQm5uf05JBl3eKAnmSFIS1wnuYM4YhM+HL/wd/S4FnxpkJc7mftVh1dVBCOIMSwqRy\nqrBOaaGZxdzU2gnO6nexKZDXwSeySVIQ1vMPhmnPwW1fwIW/NfMp1jxrhq/+pS+8fAV8/Tc4uA5q\na856+rTMRL77/gT7jpV5IHjR6TW1FnNrdYsyZWF2SVJoklJqslIqWymVq5Sa38jjP1VKbVVKbVJK\nrVRKpVkZj/AgpcyqcRf8Em76CO4/YEplnPtTMxP7iz/Cvy82E+zeuA7WPAeF2aA1U4clohSe6XAu\nPwH7V5kk9vlDcOKg+2MQ1tq9xMwz6DGs/ccadDkU7jSVhjsoy8pcKKV8gaeAS4A8YJ1SapHWeofD\nbq9rrZ+x7z8VeARoZ7oWHUJAN9Mx1+8ic7/smBnOt/dLc6kbxRHWg4SUCfwqoTdfbCzjngtTUL4W\nfGxtNji+z6xZcWTbD9cl35+537fPwEX/Z6rHtqcqrfAONZVmFv6Qa1s3i7kpAy8zVVOzl5gJoh2Q\nlbWPRgG5Wuu9AEqpBcBVQH1S0FqfdNg/BGjfNFjRcYXEmEXSM6aZ+8X7YN9XJkHsXsa88mLmAbY/\n/ATdLRqf0DgIjTVlPELjICTWfu2wPSTG1GhqqPIUFOwwa17XJYCCHWacOYDygej+0PscyLrZlAGJ\nzwBbNXx0r/lPv/UtuPIJSMhw1zskrLB/hZnF3N7+hDrdkyAu3TQhSVI4SyLgeK6dB5zbcCel1B3A\nvUAAcGFjB1JKzQXmAvTp08flgQovFJVsLiNvApsN2+EtrPpqCZt37iax4hTjojQxVSVQvAbKCqG6\niQ7r4KgfkkZAiGmSOr7vh8cDI8wX+4gbzHV8BsQNbnoC0/VvmYKDS+6H58bDmLth/K/aNuFJeF62\nfRZz8gWuO+agKbDiH6Ysfbco1x3XTZRuZ42aJg+s1Axgstb6Nvv9HwHnaq3vbGL/64BLtdY/bu64\nWVlZev369S6PV3QM2/JLuOfNTeQeLeWWscn8avJAgvx9obLUDAcsKzQF/8qOmlElZUft9wvNGUJ0\nvx9++SdkQETvtjUbnC42I6g2vQZRKWbBIVd+sQjraQ2PDTGfhzlvuO64+RvNpM2rn4Hhc1x33HZS\nSm3QWme1tJ+VZwr5QG+H+73s25qyAHjawnhEJ5CRGMFHd53Pw0t28eKqfazMLeSxWSNI6xkOgaHu\nW/2qWxRc/S8YOsvUeXrlShh+A0z6Q4f8ddglFWyHkoNm8IMr9Rhuym9nL/aqpOAsK0cfrQP6K6WS\nlVIBwGxgkeMOSqn+DncvB3IsjEd0EkH+vjw4NZ2Xbz6H46erufqpVTz39R5sNg90SaWMh//5Bs7/\nOWx+A54aBVvfbneVWOEGu5ea6wFtnMXcFB8fe4G8zztkgTzLkoLWuga4E1gG7AQWaq23K6Ueso80\nArhTKbVdKbUJ06/QbNOREI4mDIxj2T0XMHFQLH9evIvrX1jDoRPl7g/EPxgufhB+8pVpjnrnVnh9\npgxfdaXaarM+ecXJlvd11u6l0DPTrDroagMv77AF8izrU7CK9CmIhrTWvLUhj98v2o6vj+KP1wxh\n6jAnFl23gq3WzGn44o/mvjcMXy3MNqvt7f4ELv0TpE1t+TnepKwIFt4IB1ZCXBpctxAie7f8vOaU\nFsLf+8OEX8OE+10Tp6OaSvhrihnqeuVjrj9+GzjbpyAzmkWHp5RiZlZvFv9sHP3iQrn7je+4Z8F3\nlJRXuz8YH1+zdsUd35rZrUvnw78vaXOl2Daz1cLOj+CVqaZJa+OrplnjrR/Dd6+5N5b2KNhhOm3z\n1sG4X0BJnln69dCm9h03ZxmtWou5tTpwgTxJCqLT6BsdwsKfnMe9lwzgwy2Hueyxr/lmT+sK8LlM\nZB8zfHX6v+H4ATN89dMHoHC3tf0Np4vN2tyPD4M3r4eiXLjw/+DenTBvtVkv4IP/gW87wJiOXR+b\nhFpTCTcvMWddtywDHz94aYoZTtpW2UsgPBEShrou3oYGXg6lR+DQd9a9hgWk+Uh0SpsOnuCeBd9x\noPg0cy9I4d5LBhDo17omHJtNc6K8muKySopKq6iqtXFucjQBfq38LXW6GD75P9j0X3M/rKf5ck4Z\nD8nj21eZs86hTbD2edj2NtRUmPr+o+aaReUdZ4DXVMI7t8HORTB+PkyY75qZvK6kNax8BD7/A/Qc\nDrNfN6sH1jl1xPTZHNkKl/0VRt3euuNXV5imnWGzzDrmVjldDH/rB+ffAxc9YN3rOMnZ5iNJCqLT\nKqus4Y8f7+SNtd+T1iOcf8wcRnRoAMVlVRSXVlFUVkVxWd11pbldarYVl1Vx/HQVDQc09YwI4qcT\nUpmZ1dvMj2iN4n0/lPHY9zWUF5vtMQN/SBJJ55uCgc6oqTJf7mufg4NrwL8bDJsN59wO8c2UEaut\nMcNoN/0Xzp0Hl/7ZmiVa26K6HD640yS3jBlw1T8bnxhYWWqS2+4lMPoOMxTY2X6bnM/gtelw3Vsw\nYJJr42/o5SvgdJEZoeZhkhSEsPtsRwH3v7OFoiYW6lEKIoP9iQoJIDokkKiQAKJCA4gOCTC37dtL\nK2t4fsVeNhw4TmxYIHPHpXDduX0ICWzDdB+bzZTZ2Psl7P0KDqyGmnJTYiNxpDmDSJkAvUeZ9mlH\nJw/Dhpdhw0tmwl5UikkEw6+D4EjnX/+T38C3/4Lh15uSHVbUlGqNk4fMuhuHNpmmovPvbf4sxlYL\nS38Na5+FQVfAtOdNTa2WfHwfbHodfrUP/INcF39jvvkXLPs13L3JzND3IEkKQjgoPFXJe9/lEezv\nS5T9iz861HzhRwb74+fr3C9lrTXf7i3mn8tzWJVbRPdu/tx6fjI3jkkiPKiROkvOqqk0nal1SSJ/\nA+haU4Kh73kmScQOhC0LzdmBrRb6TzJNRKkXtu2Xvtbw1V/hyz+bL9UZL56dgNwlb4NJCFWl5st9\n0BTnn/vt0yY5JGbCnAWmrElTtIZHM0xF1Dmvtz/ulhzfb/p3Lv2zx2shSVIQwmIbDhznqeW5fLHr\nKGFBftw0JombxyYTFRLQ/oNXnIQDq35IEoU7zfagCBjxI8i6xXWzt799BpbeDykTYdZ/zcxwd9r8\nJiy6y8wXmLOg+aavpuz8yDQnhcbC9W+bBNqYI9vgmbEw9UnIvLF9cTvrX2PMLPebPnLP6zVBkoIQ\nbrItv4SnlueyZNsRugX4csPovtw2Lpm4MBc2TZw6Ysoy9BltCvu52qbX4YM7TNPV9W9BcHfXv0ZD\ntlr4/Pew6nHTMX7tKxAS3fbj5W+A12dBbZVJbo3Vovr6b2YOyX27ISy+7a/VGp//wYwI+2WuR0ug\nyDwFIdwkIzGCp28YySc/v4BJafG8sGIv4/6ynN99sM11M6zDEsy4dysSApj+iJmvwuHNpnP0VIE1\nr1On4iS8McckhKxb4EfvtS8hgElot30OoQnwn2mwqZEid9lLzX7uSghgmsJ0rX2Ftzay2aAk3yz6\nZDE5UxDCxfYfK+PpL/fwzsY8lILpmb2YNyGVvtEWfaG70p7lsOB686V54wdmvoWrFe0xCaEoFy77\nS+uHlLak/AQs/JEZ4eU47Lb0KPx9AEz8DYx3cRG85ths8GiaGTQw89Wm99PanBEW74XiPeZ9Kt4D\nRXvNtppyuOIxs8ZHG0jzkRAeln+inGe/2sOCdQepqbUxdVhPZmb15tyUaHx9vGxugKODa+G1GeAf\nAje+33T7fFvs/cqUrFDKNBeljHfdsR3VVJlht5tfh6GzTR/Cljdh0Z3wkxXQw8JJa4358B6zMNMv\n90DlSYcv/AZf/NUO65D7+JtFe6JTISoVolMgeQLE9GtTCJIUhPASR09W8MLKfbz27QHKqmqJCQ3g\nsoweXDG0B+ckReHjjQniyDb4zzWm2eOGd80ksrbS2gyd3fauWYMipr9ZvyAqxXXxNvW6X/8Nlv/J\n9Fn4+MKxXPj5NvdP2Mv51CRav2Dzi7+Ojx9E9jXvheOXf1SqKa7owmHCkhSE8DIV1bUs33WUj7Yc\n5vNdBVRU24gPD2TKEJMgRvTu7l0JomgPvHo1VJwwo4KSxja9b90Xf9GeBs0f+878BTxgshlyGhTu\nnr8BzOimD+4wy6lm3QpXPOK+165TU2VGePn42b/4U00iiOzT+JKxFpCkIIQXK6us4YtdR/loyyGW\nZxdSVWOjZ0QQlw/twRVDezK0VwTKG8pPlOTDf66GE9/DzP+YZpfivT80exTvbb7po/4XcArEDDAz\ntj1RMXb/SlOccOqT0HOE+1/fC0hSEKKDOFVRzWc7C/ho82G+zimkulbTOyqYy4f05IqhPUjvGe7Z\nBFF2DP47zYxMcuTjZ//iTz3zyz8qxeVNH6L9JCkI0QGVlFfzyfYjfLTlMKtyj1Fj0yTHhHD5kB5M\nSo+nW4AvFdU2KmtsVNXYqKyppbKmwf1qG1W1Niqrzf0q++MazeiUaCYMjCO0taU5Kk7CuhcgINSy\nNm9hLUkKQnRwx8uqWGZPEKv3HDurOJ8z/HwUgX4+BPj5UF2rKa2sIcDPhwv6xzI5I4FLBscT0c09\nbdrCsyQpCNGJHCut5Js9RSgFAb4+BPr71n/ZB9ZfHLf5EuDnc8bQ11qbZt3+YpZuO8Ky7Uc4XFKB\nn4/ivNRoJmckMCktgdgwD9U+EpaTpCCEaJLWms15JSzddoSl2w6zv+g0SsE5faOYnJHApRkJJEY2\nUrK6jWw2zenq2tY3WwmXkaQghHCK1prsglMs2WrOIHYdOQXA0F4RTM5IYHJ6AimxjRfJ01pzsqKG\ngpMV9kslBScrOGq/fcR+++ipSmpsmjGp0dw+LoXxA2K9a/htFyBJQQjRJvuOldWfQWzOKwFgYHwY\nEwbFUlOr7V/6lRScMomgovrsNYjDg/yIDw8iISKIuLAg4sMD8fVRvLU+jyMnK+gfF8pt45K5anhi\n6xcrEm0iSUEI0W75J8r5ZPsRlmw7wrr9xQT5+dq/6AOJDzdf9uY6qP5+XFgQwQGNf9FX1dj4eOsh\nnvt6HzsPnyQmNJAfn9eXG0b3pbsrSo6LJklSEEK4VFWNDX9f5ZI5E1prVu8p4vkVe/kyu5Agfx+u\nHdmbW89PJimmAxQO7IAkKQghOoTdBad4YcVe3v/uENU2G5PS4rl9XAoj+3b3jlndnYQkBSFEh3L0\nVAWvrj7Af749QEl5NcN7RzL3ghQuTU/w7qqyHYQkBSFEh3S6qoa3N+Tx75X7OFB0mt5Rwdw6Nplr\ns3oTIkNa20ySghCiQ6u1aT7dcYTnV+xjw4HjhAX6kRIXSmxoILFhgcSFmev6i327jGZqnLNJQdKu\nEMIr+fooJmf0YHJGDzYcOM5b6w+Sf6KcvOOn+e774xSfrqKx37ThQX4OySKI2NBA4sJN0kiKCSE1\nNoTIbjLSqSmSFIQQXm9k3+6M7Nv9jG3VtTaKy6ooPFX5w6W0kqMnKygsNfe35p2g8FQlZVW1Zzw3\nOiSA1NhQUuNCzLX9ktg9uMv3X0hSEEJ0SP6+PvXzI1pSVmlmXe8vKmPP0TL2FJayp7CUZdsLKC47\nWL9fgJ8PKTEhpMSemSxSYkOa7c+otWmqa0112ppac9tcfrhts0G/uNAm53B4C0kKQohOLyTQj5TY\nUFJiQ7lw0JmPHS+rYu+x0jOSxc7Dp1i67cgZlWnjwgLx9/Whyv4lX1Or62872zUb4OfDqKQoxvWP\nYVz/WAYlhHlduQ/paBZCiEZU1tTyfdFpe6IoY/+xMjTmDMXfV9mvf7jt56sI8PXBz0fh79fgMR8f\ntNZsOHCcFTnHyC4w9aViQgPtCSKG8/vFEOfEWU9becXoI6XUZOBxwBd4QWv9cIPH7wVuA2qAQuAW\nrfWB5o4pSUEI0dEdKalgZe4xVuQUsjLnGEVlVQAMSgjjggGxjOsfwzlJUS4dSeXxpKCU8gV2A5cA\necA6YI7WeofDPhOBNVrr00qpecAErfWs5o4rSUEI0ZnYbJodh0+yIsckifX7j1NVayPQz4dRyWc2\nNbVnhrc3DEkdBeRqrffaA1oAXAXUJwWt9XKH/b8FbrAwHiGE8Do+PoqMxAgyEiOYNyGV01U1rN1X\nXJ8k/rx4F7CL2LBAfnv5YK4anmhpPFYmhUTgoMP9PODcZva/FVjS2ANKqbnAXIA+ffq4Kj4hhPA6\n3QL8mDAwjgkD4wDT1LQip5AVOcecGmnVXl4x+kgpdQOQBYxv7HGt9XPAc2Caj9wYmhBCeFRCRBDX\nZvXm2qzebnk9K5NCPuD4V/SybzuDUupi4DfAeK11pYXxCCGEaIGPhcdeB/RXSiUrpQKA2cAixx2U\nUiOAZ4GpWuujFsYihBDCCZYlBa11DXAnsAzYCSzUWm9XSj2klJpq3+1vQCjwllJqk1JqUROHE0II\n4QaW9ilorRcDixtse8Dh9sVWvr4QQojWsbL5SAghRAcjSUEIIUQ9SQpCCCHqSVIQQghRr8NVSVVK\nFQLNFs1rRgxwzIXhuJrE1z4SX/t5e4wSX9v11VrHtrRTh0sK7aGUWu9MQShPkfjaR+JrP2+PUeKz\nnjQfCSGEqCdJQQghRL2ulhSe83QALZD42kfiaz9vj1His1iX6lMQQgjRvK52piCEEKIZnTIpKKUm\nK6WylVK5Sqn5jTweqJR60/74GqVUkhtj662UWq6U2qGU2q6U+lkj+0xQSpXYiwRuUko90NixLIxx\nv1Jqq/21z1r7VBlP2N+/LUqpTDfGNtDhfdmklDqplLqnwT5uf/+UUi8qpY4qpbY5bItSSn2qlMqx\nX3dv4rk/tu+To5T6sZti+5tSapf93+89pVRkE89t9rNgcYwPKqXyHf4dpzTx3Gb/v1sY35sOse1X\nSm1q4rlueQ9dRmvdqS6AL7AHSAECgM1AWoN9/gd4xn57NvCmG+PrAWTab4dh1rFuGN8E4CMPvof7\ngZhmHp+CWSVPAaMx62x76t/6CGb8tUffP+ACIBPY5rDtr8B8++35wF8aeV4UsNd+3d1+u7sbYpsE\n+Nlv/6Wx2Jz5LFgc44PAL5z4DDT7/92q+Bo8/g/gAU++h666dMYzhfq1obXWVUDd2tCOrgJesd9+\nG7hItWdF7FbQWh/WWm+03z6FKStu7aKrrncV8Ko2vgUilVI9PBDHRcAerXVbJzO6jNb6a6C4wWbH\nz9krwNWNPPVS4FOtdbHW+jjwKTDZ6ti01p9oU94ezProvVz5mq3VxPvnDGf+v7dbc/HZvztmAm+4\n+nU9oTMmhcbWhm74pVu/j/0/RgkQ7ZboHNibrUYAaxp5+Dyl1Gal1BKlVLpbAwMNfKKU2mBfH7sh\nZ95jd5hN0/8RPfn+1YnXWh+23z4CxDeyjze8l7fQxProtPxZsNqd9iauF5tofvOG928cUKC1zmni\ncU+/h63SGZNCh6CUCgXeAe7RWp9s8PBGTJPIMOBJ4H03h3e+1joTuAy4Qyl1gZtfv0X21fymAm81\n8rCn37+zaNOO4HVD/ZRSvwFqgNea2MWTn4WngVRgOHAY00TjjebQ/FmC1/9/ctQZk4Iza0PX76OU\n8gMigCK3RGde0x+TEF7TWr/b8HGt9Umtdan99mLAXykV4674tNb59uujwHuYU3RHTq2/bbHLgI1a\n64KGD3j6/XNQUNesZr9ubMlZj72XSqmbgCuA6+1J6yxOfBYso7Uu0FrXaq1twPNNvLZHP4v2749p\nwJtN7ePJ97AtOmNSaHFtaPv9ulEeM4AvmvpP4Wr29sd/Azu11o80sU9CXR+HUmoU5t/JLUlLKRWi\nlAqru43pkNzWYLdFwI32UUijgRKHZhJ3afLXmSffvwYcP2c/Bj5oZJ9lwCSlVHd788gk+zZLKaUm\nA7/CrI9+uol9nPksWBmjYz/VNU28tjP/3610MbBLa53X2IOefg/bxNM93VZcMKNjdmNGJfzGvu0h\nzH8AgCBMs0MusBZIcWNs52OaEbYAm+yXKcBPgZ/a97kT2I4ZSfEtMMaN8aXYX3ezPYa6988xPgU8\nZX9/twJZbv73DcF8yUc4bPPo+4dJUIeBaky79q2YfqrPgRzgMyDKvm8W8ILDc2+xfxZzgZvdFFsu\npi2+7jNYNxqvJ7C4uc+CG9+//9g/X1swX/Q9GsZov3/W/3d3xGff/nLd585hX4+8h666yIxmIYQQ\n9Tpj85EQQog2kqQghBCiniQFIYQQ9SQpCCGEqCdJQQghRD1JCkLYKaVqG1RgdVnFTaVUkmOFTSG8\nlZ+nAxDCi5RrrYd7OgghPEnOFIRogb0e/l/tNfHXKqX62bcnKaW+sBds+1wp1ce+Pd6+RsFm+2WM\n/VC+SqnnlVlH4xOlVLB9/7uVWV9ji1JqgYf+TCEASQpCOApu0Hw0y+GxEq31EOCfwGP2bU8Cr2it\nh2IKyj1h3/4E8JU2BfkyMTNZAfoDT2mt04ETwHT79vnACPtxfmrVHyeEM2RGsxB2SqlSrXVoI9v3\nAxdqrffaixke0VpHK6WOYUovVNu3H9ZaxyilCoFeWutKh2MkYdZN6G+/fz/gr7X+o1JqKVCKqeb6\nvrYX8xPCE+RMQQjn6CZut0alw+1afujTuxxTSyoTWGevvCmER0hSEMI5sxyuv7HfXo2pyglwPbDC\nfvtzYB6AUspXKRXR1EGVUj5Ab631cuB+TBn3s85WhHAX+UUixA+CGyy+vlRrXTcstbtSagvm1/4c\n+7a7gJeUUr8ECoGb7dt/BjynlLoVc0YwD1NhszG+wH/tiUMBT2itT7jsLxKilaRPQYgW2PsUsrTW\nxzwdixBWk+YjIYQQ9eRMQQghRD05UxBCCFFPkoIQQoh6khSEEELUk6QghBCiniQFIYQQ9SQpCCGE\nqPf/ARpw0hH49b9cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1cbb93510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0c731d750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_75 (Conv2D)           (None, 71, 112, 128)      36992     \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 70, 111, 64)       32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 33, 53, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 14, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 14, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 12, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 16898     \n",
      "=================================================================\n",
      "Total params: 144,226\n",
      "Trainable params: 144,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.6869 - acc: 0.5526 - val_loss: 0.6815 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.6767 - acc: 0.5673 - val_loss: 0.6570 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.6024 - acc: 0.6542 - val_loss: 0.4935 - val_acc: 0.8068\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.4763 - acc: 0.7827 - val_loss: 0.4053 - val_acc: 0.8729\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.3948 - acc: 0.8372 - val_loss: 0.3749 - val_acc: 0.8557\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.4085 - acc: 0.8323 - val_loss: 0.3585 - val_acc: 0.8900\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.3101 - acc: 0.8782 - val_loss: 0.4050 - val_acc: 0.8435\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2899 - acc: 0.8880 - val_loss: 0.3117 - val_acc: 0.8998\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2726 - acc: 0.8960 - val_loss: 0.3119 - val_acc: 0.8875\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2713 - acc: 0.8972 - val_loss: 0.3168 - val_acc: 0.8655\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2359 - acc: 0.9051 - val_loss: 0.2859 - val_acc: 0.9022\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2341 - acc: 0.9076 - val_loss: 0.2944 - val_acc: 0.9095\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2414 - acc: 0.9064 - val_loss: 0.2798 - val_acc: 0.8998\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2191 - acc: 0.9106 - val_loss: 0.2931 - val_acc: 0.8924\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2084 - acc: 0.9217 - val_loss: 0.2793 - val_acc: 0.9095\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2009 - acc: 0.9204 - val_loss: 0.2735 - val_acc: 0.9144\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.1955 - acc: 0.9229 - val_loss: 0.2849 - val_acc: 0.9095\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2002 - acc: 0.9198 - val_loss: 0.2876 - val_acc: 0.8973\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.1855 - acc: 0.9321 - val_loss: 0.2571 - val_acc: 0.9169\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.1870 - acc: 0.9290 - val_loss: 0.3066 - val_acc: 0.8924\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_81 (Conv2D)           (None, 71, 112, 128)      36992     \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 70, 111, 64)       32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 33, 53, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 14, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 14, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 12, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 16898     \n",
      "=================================================================\n",
      "Total params: 144,226\n",
      "Trainable params: 144,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.6784 - acc: 0.5563 - val_loss: 0.6555 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.6392 - acc: 0.6457 - val_loss: 0.5306 - val_acc: 0.7702\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.4727 - acc: 0.7882 - val_loss: 0.4378 - val_acc: 0.7922\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.3884 - acc: 0.8543 - val_loss: 0.3494 - val_acc: 0.8337\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.3285 - acc: 0.8709 - val_loss: 0.3323 - val_acc: 0.8435\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2969 - acc: 0.8849 - val_loss: 0.3360 - val_acc: 0.8582\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2670 - acc: 0.8990 - val_loss: 0.3160 - val_acc: 0.8509\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2796 - acc: 0.8996 - val_loss: 0.3420 - val_acc: 0.8582\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2473 - acc: 0.9076 - val_loss: 0.2888 - val_acc: 0.8631\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2303 - acc: 0.9119 - val_loss: 0.4295 - val_acc: 0.8460\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2468 - acc: 0.8996 - val_loss: 0.2902 - val_acc: 0.8655\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2161 - acc: 0.9113 - val_loss: 0.2961 - val_acc: 0.8778\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2189 - acc: 0.9168 - val_loss: 0.3097 - val_acc: 0.8753\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.1993 - acc: 0.9315 - val_loss: 0.3600 - val_acc: 0.8753\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2096 - acc: 0.9186 - val_loss: 0.2776 - val_acc: 0.8778\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.1951 - acc: 0.9241 - val_loss: 0.2565 - val_acc: 0.8900\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.1856 - acc: 0.9278 - val_loss: 0.3296 - val_acc: 0.8826\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.1911 - acc: 0.9272 - val_loss: 0.2935 - val_acc: 0.8778\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.1764 - acc: 0.9315 - val_loss: 0.3287 - val_acc: 0.8729\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.1817 - acc: 0.9315 - val_loss: 0.2825 - val_acc: 0.8778\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([ 813,  815,  819,  820,  821,  822,  824,  825,  826,  827,  828,\n",
      "        829,  830,  831,  832,  833,  834,  835,  836,  837,  838,  839,\n",
      "        840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081,\n",
      "       1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092,\n",
      "       1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103,\n",
      "       1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125,\n",
      "       1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136,\n",
      "       1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "       1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n",
      "       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169,\n",
      "       1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,\n",
      "       1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "       1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202,\n",
      "       1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1213, 1214,\n",
      "       1215, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1229, 1230, 1233,\n",
      "       1236, 1237]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_87 (Conv2D)           (None, 71, 112, 128)      36992     \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 70, 111, 64)       32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_90 (Conv2D)           (None, 33, 53, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 14, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 14, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 12, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 16898     \n",
      "=================================================================\n",
      "Total params: 144,226\n",
      "Trainable params: 144,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.6892 - acc: 0.5526 - val_loss: 0.6850 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.6747 - acc: 0.5673 - val_loss: 0.6982 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.6438 - acc: 0.6034 - val_loss: 0.5408 - val_acc: 0.8289\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.4886 - acc: 0.7687 - val_loss: 0.4077 - val_acc: 0.8313\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.3792 - acc: 0.8335 - val_loss: 0.3332 - val_acc: 0.8729\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.3496 - acc: 0.8574 - val_loss: 0.3969 - val_acc: 0.8142\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.3404 - acc: 0.8684 - val_loss: 0.2962 - val_acc: 0.8655\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2918 - acc: 0.8843 - val_loss: 0.2942 - val_acc: 0.8753\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2894 - acc: 0.8856 - val_loss: 0.3065 - val_acc: 0.8680\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2660 - acc: 0.8923 - val_loss: 0.2832 - val_acc: 0.8851\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2421 - acc: 0.9070 - val_loss: 0.2834 - val_acc: 0.8778\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2452 - acc: 0.8972 - val_loss: 0.2578 - val_acc: 0.8949\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2349 - acc: 0.9058 - val_loss: 0.2654 - val_acc: 0.8924\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2275 - acc: 0.9027 - val_loss: 0.2660 - val_acc: 0.8949\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2184 - acc: 0.9125 - val_loss: 0.2300 - val_acc: 0.9046\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2079 - acc: 0.9204 - val_loss: 0.2439 - val_acc: 0.8998\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2024 - acc: 0.9180 - val_loss: 0.2667 - val_acc: 0.8924\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2088 - acc: 0.9180 - val_loss: 0.3188 - val_acc: 0.8753\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2057 - acc: 0.9192 - val_loss: 0.2372 - val_acc: 0.9022\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.1878 - acc: 0.9211 - val_loss: 0.2315 - val_acc: 0.9095\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([1209, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1234, 1235,\n",
      "       1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248,\n",
      "       1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,\n",
      "       1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270,\n",
      "       1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281,\n",
      "       1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292,\n",
      "       1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "       1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,\n",
      "       1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325,\n",
      "       1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,\n",
      "       1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "       1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358,\n",
      "       1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369,\n",
      "       1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380,\n",
      "       1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391,\n",
      "       1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402,\n",
      "       1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413,\n",
      "       1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424,\n",
      "       1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "       1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446,\n",
      "       1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457,\n",
      "       1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468,\n",
      "       1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "       1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490,\n",
      "       1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501,\n",
      "       1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512,\n",
      "       1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523,\n",
      "       1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534,\n",
      "       1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
      "       1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556,\n",
      "       1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567,\n",
      "       1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578,\n",
      "       1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589,\n",
      "       1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600,\n",
      "       1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611,\n",
      "       1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622,\n",
      "       1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1633, 1635,\n",
      "       1637, 1638]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_93 (Conv2D)           (None, 71, 112, 128)      36992     \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 70, 111, 64)       32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 33, 53, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 14, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 14, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 12, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 16898     \n",
      "=================================================================\n",
      "Total params: 144,226\n",
      "Trainable params: 144,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.6857 - acc: 0.5643 - val_loss: 0.6827 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.6681 - acc: 0.5673 - val_loss: 0.6447 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.6401 - acc: 0.6916 - val_loss: 0.5369 - val_acc: 0.7482\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.4665 - acc: 0.8005 - val_loss: 0.4542 - val_acc: 0.7751\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.3710 - acc: 0.8421 - val_loss: 0.3337 - val_acc: 0.8826\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.3709 - acc: 0.8433 - val_loss: 0.3263 - val_acc: 0.8704\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.3192 - acc: 0.8721 - val_loss: 0.2957 - val_acc: 0.8704\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2868 - acc: 0.8941 - val_loss: 0.2699 - val_acc: 0.8924\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2839 - acc: 0.8941 - val_loss: 0.2770 - val_acc: 0.8875\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2747 - acc: 0.8911 - val_loss: 0.3294 - val_acc: 0.8704\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2582 - acc: 0.8990 - val_loss: 0.2138 - val_acc: 0.9193\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2674 - acc: 0.8996 - val_loss: 0.2266 - val_acc: 0.9046\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.2480 - acc: 0.9015 - val_loss: 0.2093 - val_acc: 0.9193\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2272 - acc: 0.9076 - val_loss: 0.2382 - val_acc: 0.9071\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2382 - acc: 0.9125 - val_loss: 0.2239 - val_acc: 0.9120\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2073 - acc: 0.9125 - val_loss: 0.2016 - val_acc: 0.9242\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2164 - acc: 0.9168 - val_loss: 0.2134 - val_acc: 0.9120\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2072 - acc: 0.9186 - val_loss: 0.2378 - val_acc: 0.9095\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 55s - loss: 0.2044 - acc: 0.9204 - val_loss: 0.2297 - val_acc: 0.9120\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 56s - loss: 0.1981 - acc: 0.9174 - val_loss: 0.2353 - val_acc: 0.9071\n",
      "('TRAIN:', array([   0,    1,    2, ..., 1635, 1637, 1638]), 'TEST:', array([1632, 1634, 1636, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
      "       1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
      "       1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668,\n",
      "       1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
      "       1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
      "       1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
      "       1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
      "       1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "       1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
      "       1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745,\n",
      "       1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756,\n",
      "       1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767,\n",
      "       1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778,\n",
      "       1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789,\n",
      "       1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
      "       1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811,\n",
      "       1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822,\n",
      "       1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833,\n",
      "       1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844,\n",
      "       1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,\n",
      "       1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866,\n",
      "       1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877,\n",
      "       1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888,\n",
      "       1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899,\n",
      "       1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910,\n",
      "       1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921,\n",
      "       1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932,\n",
      "       1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943,\n",
      "       1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954,\n",
      "       1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965,\n",
      "       1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976,\n",
      "       1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
      "       1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
      "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
      "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n",
      "       2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n",
      "       2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_99 (Conv2D)           (None, 71, 112, 128)      36992     \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 70, 111, 64)       32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 33, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 33, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 33, 53, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 14, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 14, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 12, 22, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 16898     \n",
      "=================================================================\n",
      "Total params: 144,226\n",
      "Trainable params: 144,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.6817 - acc: 0.5623 - val_loss: 0.6735 - val_acc: 0.5676\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.6366 - acc: 0.6137 - val_loss: 0.5163 - val_acc: 0.8133\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.5891 - acc: 0.7592 - val_loss: 0.7066 - val_acc: 0.6462\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 55s - loss: 0.5757 - acc: 0.7567 - val_loss: 0.4150 - val_acc: 0.8059\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.3754 - acc: 0.8337 - val_loss: 0.3571 - val_acc: 0.8305\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.3643 - acc: 0.8521 - val_loss: 0.3445 - val_acc: 0.8477\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.3306 - acc: 0.8661 - val_loss: 0.3202 - val_acc: 0.8771\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.3353 - acc: 0.8698 - val_loss: 0.3008 - val_acc: 0.8771\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2868 - acc: 0.8955 - val_loss: 0.3011 - val_acc: 0.8722\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.3014 - acc: 0.8729 - val_loss: 0.2849 - val_acc: 0.8894\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2550 - acc: 0.8979 - val_loss: 0.2662 - val_acc: 0.9140\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 55s - loss: 0.2698 - acc: 0.8943 - val_loss: 0.2681 - val_acc: 0.8894\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2410 - acc: 0.9028 - val_loss: 0.3087 - val_acc: 0.8747\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636/1636 [==============================] - 56s - loss: 0.2552 - acc: 0.9028 - val_loss: 0.2718 - val_acc: 0.8821\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2360 - acc: 0.9059 - val_loss: 0.3167 - val_acc: 0.8698\n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2223 - acc: 0.9083 - val_loss: 0.2494 - val_acc: 0.9066\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2250 - acc: 0.9077 - val_loss: 0.2524 - val_acc: 0.9115\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2072 - acc: 0.9211 - val_loss: 0.2518 - val_acc: 0.9091\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2146 - acc: 0.9108 - val_loss: 0.2644 - val_acc: 0.9066\n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 56s - loss: 0.2050 - acc: 0.9175 - val_loss: 0.2463 - val_acc: 0.9115\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.1055 - acc: 0.9621    \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.1004 - acc: 0.9670    \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.1049 - acc: 0.9652    \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.1027 - acc: 0.9682    \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0912 - acc: 0.9700    \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0938 - acc: 0.9658    \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0950 - acc: 0.9682    \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0901 - acc: 0.9707    \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 68s - loss: 0.0939 - acc: 0.9719    \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0930 - acc: 0.9639    \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0881 - acc: 0.9682    \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0891 - acc: 0.9694    \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0880 - acc: 0.9682    \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0860 - acc: 0.9707    \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0837 - acc: 0.9713    \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0816 - acc: 0.9737    \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0847 - acc: 0.9688    \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0777 - acc: 0.9743    \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0730 - acc: 0.9749    \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 67s - loss: 0.0752 - acc: 0.9743    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91042584434654916"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn deeper with different kernel shapes (try to beat 92%)\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n",
    "num_classes = 2\n",
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `accuracy_score(Y_test, cls_predictions)`:0.91042584434654916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16, 26, 2070)      68310     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 26, 2070)      0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16, 26, 2070)      4286970   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 861120)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1722242   \n",
      "=================================================================\n",
      "Total params: 6,190,642\n",
      "Trainable params: 6,190,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.6244 - acc: 0.6304 - val_loss: 0.5144 - val_acc: 0.7188\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.5451 - acc: 0.7534 - val_loss: 0.4446 - val_acc: 0.8582\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.3815 - acc: 0.8397 - val_loss: 0.4071 - val_acc: 0.8606\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.3554 - acc: 0.8550 - val_loss: 0.3474 - val_acc: 0.8851\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2955 - acc: 0.8880 - val_loss: 0.3718 - val_acc: 0.8753\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.3201 - acc: 0.8745 - val_loss: 0.4997 - val_acc: 0.8435\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2984 - acc: 0.8856 - val_loss: 0.3927 - val_acc: 0.8704\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2941 - acc: 0.8923 - val_loss: 0.3298 - val_acc: 0.8851\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2312 - acc: 0.9051 - val_loss: 0.8800 - val_acc: 0.6381\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.3182 - acc: 0.8672 - val_loss: 0.3544 - val_acc: 0.8826\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2292 - acc: 0.9076 - val_loss: 0.3690 - val_acc: 0.8753\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2277 - acc: 0.9106 - val_loss: 0.4002 - val_acc: 0.8778\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2067 - acc: 0.9180 - val_loss: 0.3027 - val_acc: 0.8949\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2650 - acc: 0.9076 - val_loss: 0.5024 - val_acc: 0.8411\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2038 - acc: 0.9180 - val_loss: 0.2891 - val_acc: 0.9095\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1766 - acc: 0.9302 - val_loss: 0.3081 - val_acc: 0.8826\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.1829 - acc: 0.9296 - val_loss: 0.2948 - val_acc: 0.9022\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.1982 - acc: 0.9192 - val_loss: 0.3284 - val_acc: 0.8802\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1688 - acc: 0.9370 - val_loss: 0.3142 - val_acc: 0.8949\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.1783 - acc: 0.9278 - val_loss: 0.3518 - val_acc: 0.8826\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16, 26, 2070)      68310     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 26, 2070)      0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16, 26, 2070)      4286970   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 861120)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 1722242   \n",
      "=================================================================\n",
      "Total params: 6,190,642\n",
      "Trainable params: 6,190,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 177s - loss: 1.0765 - acc: 0.5967 - val_loss: 0.5354 - val_acc: 0.6186\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.5224 - acc: 0.7668 - val_loss: 0.4014 - val_acc: 0.8142\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.3466 - acc: 0.8617 - val_loss: 0.4111 - val_acc: 0.8264\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.3567 - acc: 0.8501 - val_loss: 0.3489 - val_acc: 0.8729\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.3375 - acc: 0.8635 - val_loss: 0.3706 - val_acc: 0.8435\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.3328 - acc: 0.8641 - val_loss: 0.3784 - val_acc: 0.8484\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2976 - acc: 0.8727 - val_loss: 0.4584 - val_acc: 0.8411\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.3503 - acc: 0.8629 - val_loss: 0.5020 - val_acc: 0.8362\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2714 - acc: 0.8905 - val_loss: 0.3405 - val_acc: 0.8704\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2917 - acc: 0.8953 - val_loss: 0.7841 - val_acc: 0.8337\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.7593 - acc: 0.8311 - val_loss: 0.3696 - val_acc: 0.8557\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2404 - acc: 0.9125 - val_loss: 0.3592 - val_acc: 0.8606\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2298 - acc: 0.9119 - val_loss: 0.3381 - val_acc: 0.8704\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2143 - acc: 0.9204 - val_loss: 0.3753 - val_acc: 0.8631\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.2148 - acc: 0.9229 - val_loss: 0.3555 - val_acc: 0.8557\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2032 - acc: 0.9253 - val_loss: 0.3438 - val_acc: 0.8582\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1936 - acc: 0.9296 - val_loss: 0.3461 - val_acc: 0.8704\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2126 - acc: 0.9180 - val_loss: 0.3952 - val_acc: 0.8802\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 177s - loss: 0.1869 - acc: 0.9308 - val_loss: 0.3427 - val_acc: 0.8729\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1989 - acc: 0.9253 - val_loss: 0.3544 - val_acc: 0.8778\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([ 813,  815,  819,  820,  821,  822,  824,  825,  826,  827,  828,\n",
      "        829,  830,  831,  832,  833,  834,  835,  836,  837,  838,  839,\n",
      "        840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
      "        851,  852,  853,  854,  855,  856,  857,  858,  859,  860,  861,\n",
      "        862,  863,  864,  865,  866,  867,  868,  869,  870,  871,  872,\n",
      "        873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
      "        884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,\n",
      "        895,  896,  897,  898,  899,  900,  901,  902,  903,  904,  905,\n",
      "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
      "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
      "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
      "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
      "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
      "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
      "        983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
      "        994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
      "       1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
      "       1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
      "       1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
      "       1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
      "       1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n",
      "       1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070,\n",
      "       1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081,\n",
      "       1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092,\n",
      "       1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103,\n",
      "       1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114,\n",
      "       1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125,\n",
      "       1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136,\n",
      "       1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
      "       1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158,\n",
      "       1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169,\n",
      "       1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180,\n",
      "       1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "       1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202,\n",
      "       1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1213, 1214,\n",
      "       1215, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1229, 1230, 1233,\n",
      "       1236, 1237]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16, 26, 2070)      68310     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 26, 2070)      0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16, 26, 2070)      4286970   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 861120)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 1722242   \n",
      "=================================================================\n",
      "Total params: 6,190,642\n",
      "Trainable params: 6,190,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.6256 - acc: 0.6873 - val_loss: 0.5838 - val_acc: 0.7139\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.5499 - acc: 0.7736 - val_loss: 0.3902 - val_acc: 0.8460\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.4084 - acc: 0.8348 - val_loss: 0.4391 - val_acc: 0.7800\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.5307 - acc: 0.8403 - val_loss: 1.9354 - val_acc: 0.7482\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.5217 - acc: 0.8433 - val_loss: 1.3314 - val_acc: 0.6479\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.4623 - acc: 0.8605 - val_loss: 0.3259 - val_acc: 0.8826\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.3045 - acc: 0.8819 - val_loss: 0.4331 - val_acc: 0.8337\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2812 - acc: 0.8917 - val_loss: 0.5094 - val_acc: 0.8191\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2817 - acc: 0.8886 - val_loss: 0.3213 - val_acc: 0.8729\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2799 - acc: 0.8819 - val_loss: 0.4648 - val_acc: 0.8313\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2906 - acc: 0.8849 - val_loss: 0.2907 - val_acc: 0.8826\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2474 - acc: 0.9002 - val_loss: 0.3033 - val_acc: 0.8729\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.2189 - acc: 0.9058 - val_loss: 0.3087 - val_acc: 0.8753\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2256 - acc: 0.9045 - val_loss: 0.2917 - val_acc: 0.8924\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2476 - acc: 0.9045 - val_loss: 0.2945 - val_acc: 0.8802\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2217 - acc: 0.9119 - val_loss: 0.2738 - val_acc: 0.9046\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.2090 - acc: 0.9162 - val_loss: 0.3498 - val_acc: 0.8753\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2330 - acc: 0.9051 - val_loss: 0.3301 - val_acc: 0.8778\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1879 - acc: 0.9296 - val_loss: 0.2812 - val_acc: 0.9022\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1937 - acc: 0.9266 - val_loss: 0.3243 - val_acc: 0.8802\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([1209, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1234, 1235,\n",
      "       1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248,\n",
      "       1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259,\n",
      "       1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270,\n",
      "       1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281,\n",
      "       1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292,\n",
      "       1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
      "       1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,\n",
      "       1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325,\n",
      "       1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336,\n",
      "       1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "       1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358,\n",
      "       1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369,\n",
      "       1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380,\n",
      "       1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391,\n",
      "       1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402,\n",
      "       1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413,\n",
      "       1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424,\n",
      "       1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
      "       1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446,\n",
      "       1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457,\n",
      "       1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468,\n",
      "       1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "       1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490,\n",
      "       1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501,\n",
      "       1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512,\n",
      "       1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523,\n",
      "       1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534,\n",
      "       1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
      "       1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556,\n",
      "       1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567,\n",
      "       1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578,\n",
      "       1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589,\n",
      "       1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600,\n",
      "       1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611,\n",
      "       1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622,\n",
      "       1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1633, 1635,\n",
      "       1637, 1638]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16, 26, 2070)      68310     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16, 26, 2070)      0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16, 26, 2070)      4286970   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 861120)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 1722242   \n",
      "=================================================================\n",
      "Total params: 6,190,642\n",
      "Trainable params: 6,190,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.6740 - acc: 0.5783 - val_loss: 0.6424 - val_acc: 0.5721\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.5220 - acc: 0.7509 - val_loss: 0.4455 - val_acc: 0.8411\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.3798 - acc: 0.8494 - val_loss: 0.3707 - val_acc: 0.8533\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.3469 - acc: 0.8562 - val_loss: 0.3099 - val_acc: 0.8875\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.5910 - acc: 0.8329 - val_loss: 1.2400 - val_acc: 0.6430\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.4138 - acc: 0.8611 - val_loss: 0.2972 - val_acc: 0.9022\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.2865 - acc: 0.8892 - val_loss: 0.2869 - val_acc: 0.8900\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2569 - acc: 0.8978 - val_loss: 0.2842 - val_acc: 0.8924\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2734 - acc: 0.9021 - val_loss: 0.2926 - val_acc: 0.8924\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.2616 - acc: 0.8990 - val_loss: 0.5832 - val_acc: 0.8337\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.2575 - acc: 0.8953 - val_loss: 0.3233 - val_acc: 0.8973\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2744 - acc: 0.8892 - val_loss: 0.2860 - val_acc: 0.9071\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2346 - acc: 0.9070 - val_loss: 0.3201 - val_acc: 0.8509\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2148 - acc: 0.9149 - val_loss: 0.2820 - val_acc: 0.8973\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.2451 - acc: 0.9033 - val_loss: 0.3185 - val_acc: 0.8851\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2068 - acc: 0.9217 - val_loss: 0.2688 - val_acc: 0.8851\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 179s - loss: 0.2118 - acc: 0.9186 - val_loss: 0.2710 - val_acc: 0.8973\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.2038 - acc: 0.9211 - val_loss: 0.3074 - val_acc: 0.8973\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1948 - acc: 0.9241 - val_loss: 0.2668 - val_acc: 0.8998\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 178s - loss: 0.1806 - acc: 0.9266 - val_loss: 0.3142 - val_acc: 0.8973\n",
      "('TRAIN:', array([   0,    1,    2, ..., 1635, 1637, 1638]), 'TEST:', array([1632, 1634, 1636, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
      "       1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657,\n",
      "       1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668,\n",
      "       1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
      "       1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
      "       1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
      "       1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
      "       1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "       1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
      "       1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745,\n",
      "       1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756,\n",
      "       1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767,\n",
      "       1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778,\n",
      "       1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789,\n",
      "       1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
      "       1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811,\n",
      "       1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822,\n",
      "       1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833,\n",
      "       1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844,\n",
      "       1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,\n",
      "       1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866,\n",
      "       1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877,\n",
      "       1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888,\n",
      "       1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899,\n",
      "       1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910,\n",
      "       1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921,\n",
      "       1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932,\n",
      "       1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943,\n",
      "       1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954,\n",
      "       1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965,\n",
      "       1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976,\n",
      "       1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
      "       1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
      "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
      "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020,\n",
      "       2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031,\n",
      "       2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 73, 113, 128)      20864     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 71, 111, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 35, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 33, 53, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 16, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16, 26, 2070)      68310     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16, 26, 2070)      0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16, 26, 2070)      4286970   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 861120)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 1722242   \n",
      "=================================================================\n",
      "Total params: 6,190,642\n",
      "Trainable params: 6,190,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1636 samples, validate on 407 samples\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 180s - loss: 0.6816 - acc: 0.5831 - val_loss: 0.6176 - val_acc: 0.5995\n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 178s - loss: 0.5130 - acc: 0.7567 - val_loss: 0.9104 - val_acc: 0.7273\n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.5780 - acc: 0.8123 - val_loss: 0.3319 - val_acc: 0.8428\n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.5469 - acc: 0.8313 - val_loss: 0.7239 - val_acc: 0.7592\n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.3489 - acc: 0.8661 - val_loss: 0.3076 - val_acc: 0.8796\n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.2924 - acc: 0.8845 - val_loss: 0.2930 - val_acc: 0.8845\n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.2942 - acc: 0.8888 - val_loss: 0.3028 - val_acc: 0.8796\n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.3074 - acc: 0.8820 - val_loss: 0.3331 - val_acc: 0.8698\n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.2584 - acc: 0.8985 - val_loss: 0.3033 - val_acc: 0.8771\n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.2366 - acc: 0.9022 - val_loss: 0.5092 - val_acc: 0.8354\n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.3453 - acc: 0.8796 - val_loss: 0.3929 - val_acc: 0.8354\n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.5466 - acc: 0.8545 - val_loss: 0.3350 - val_acc: 0.8796\n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.2401 - acc: 0.8991 - val_loss: 0.2928 - val_acc: 0.8919\n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.2145 - acc: 0.9144 - val_loss: 0.3043 - val_acc: 0.8821\n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 178s - loss: 0.1985 - acc: 0.9144 - val_loss: 0.3126 - val_acc: 0.8796\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636/1636 [==============================] - 179s - loss: 0.1952 - acc: 0.9193 - val_loss: 0.3613 - val_acc: 0.8870\n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.1933 - acc: 0.9156 - val_loss: 0.3083 - val_acc: 0.8943\n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.2164 - acc: 0.9187 - val_loss: 0.2913 - val_acc: 0.9017\n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.1809 - acc: 0.9267 - val_loss: 0.2813 - val_acc: 0.8968\n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 179s - loss: 0.1809 - acc: 0.9322 - val_loss: 0.3288 - val_acc: 0.8870\n",
      "Epoch 1/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.2109 - acc: 0.9205   \n",
      "Epoch 2/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.2063 - acc: 0.9199   \n",
      "Epoch 3/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.1949 - acc: 0.9187   \n",
      "Epoch 4/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1849 - acc: 0.9322   \n",
      "Epoch 5/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.2178 - acc: 0.9150   \n",
      "Epoch 6/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1806 - acc: 0.9309   \n",
      "Epoch 7/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.3036 - acc: 0.9016   \n",
      "Epoch 8/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1669 - acc: 0.9328   \n",
      "Epoch 9/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.1532 - acc: 0.9413   \n",
      "Epoch 10/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.1557 - acc: 0.9413   \n",
      "Epoch 11/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.1427 - acc: 0.9468   \n",
      "Epoch 12/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.1429 - acc: 0.9450   \n",
      "Epoch 13/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1408 - acc: 0.9450   \n",
      "Epoch 14/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.1481 - acc: 0.9407   \n",
      "Epoch 15/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1418 - acc: 0.9474   \n",
      "Epoch 16/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1303 - acc: 0.9523   \n",
      "Epoch 17/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1298 - acc: 0.9535   \n",
      "Epoch 18/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1412 - acc: 0.9487   \n",
      "Epoch 19/20\n",
      "1636/1636 [==============================] - 164s - loss: 0.1254 - acc: 0.9511   \n",
      "Epoch 20/20\n",
      "1636/1636 [==============================] - 165s - loss: 0.1369 - acc: 0.9499   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88693098384728342"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn deeper with different kernel shapes (try to beat 92%) and just 2 Cojnv layers\n",
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n",
    "num_classes = 2\n",
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conv 1 d : doesn't work\n",
    "NB: needs X_ch_sum which is defined later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (2043, 115, 18), 'X_test: ', (681, 115, 18), 'Y_train: ', (2043,), 'Y_test: ', (681,))\n",
      "(115, 18)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_ch_sum, Y, test_size=0.25, random_state=42)\n",
    "print('X_train: ', X_train.shape, 'X_test: ', X_test.shape, 'Y_train: ', Y_train.shape, 'Y_test: ', Y_test.shape)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "print(input_shape)\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 400,  404,  405, ..., 2040, 2041, 2042]), 'TEST:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
      "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403,\n",
      "       409, 414, 416, 418, 420, 421]))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 113, 128)          7040      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 113, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 111, 64)           24640     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 109, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 109, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 3488)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 6978      \n",
      "=================================================================\n",
      "Total params: 44,834\n",
      "Trainable params: 44,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6921 - acc: 0.5526 - val_loss: 0.6901 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6863 - acc: 0.5673 - val_loss: 0.6836 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6845 - acc: 0.5673 - val_loss: 0.6835 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6846 - acc: 0.5673 - val_loss: 0.6829 - val_acc: 0.5672\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6835 - acc: 0.5673 - val_loss: 0.6824 - val_acc: 0.5672\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6830 - acc: 0.5673 - val_loss: 0.6825 - val_acc: 0.5672\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6816 - acc: 0.5673 - val_loss: 0.6811 - val_acc: 0.5672\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6828 - acc: 0.5673 - val_loss: 0.6820 - val_acc: 0.5672\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6813 - acc: 0.5673 - val_loss: 0.6811 - val_acc: 0.5672\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6796 - acc: 0.5673 - val_loss: 0.6787 - val_acc: 0.5672\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6783 - acc: 0.5673 - val_loss: 0.6777 - val_acc: 0.5672\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6744 - acc: 0.5673 - val_loss: 0.6907 - val_acc: 0.5672\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6745 - acc: 0.5685 - val_loss: 0.6744 - val_acc: 0.5672\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6687 - acc: 0.5716 - val_loss: 0.6714 - val_acc: 0.5672\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6680 - acc: 0.5741 - val_loss: 0.6671 - val_acc: 0.5770\n",
      "Epoch 16/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6546 - acc: 0.5771 - val_loss: 0.6605 - val_acc: 0.5868\n",
      "Epoch 17/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6450 - acc: 0.5942 - val_loss: 0.6384 - val_acc: 0.6430\n",
      "Epoch 18/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6239 - acc: 0.6463 - val_loss: 0.6408 - val_acc: 0.5770\n",
      "Epoch 19/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6045 - acc: 0.6640 - val_loss: 0.5873 - val_acc: 0.7335\n",
      "Epoch 20/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.5534 - acc: 0.7173 - val_loss: 0.5306 - val_acc: 0.7555\n",
      "('TRAIN:', array([   0,    1,    2, ..., 2040, 2041, 2042]), 'TEST:', array([400, 404, 405, 406, 407, 408, 410, 411, 412, 413, 415, 417, 419,\n",
      "       422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "       435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
      "       474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
      "       565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
      "       578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
      "       591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
      "       604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
      "       617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "       630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
      "       708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
      "       721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
      "       760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
      "       773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
      "       786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
      "       799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "       812, 814, 816, 817, 818, 823]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 113, 128)          7040      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 113, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 111, 64)           24640     \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 109, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 109, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 3488)              0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 6978      \n",
      "=================================================================\n",
      "Total params: 44,834\n",
      "Trainable params: 44,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6916 - acc: 0.5636 - val_loss: 0.6885 - val_acc: 0.5672\n",
      "Epoch 2/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6866 - acc: 0.5673 - val_loss: 0.6839 - val_acc: 0.5672\n",
      "Epoch 3/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6847 - acc: 0.5673 - val_loss: 0.6832 - val_acc: 0.5672\n",
      "Epoch 4/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6851 - acc: 0.5673 - val_loss: 0.6833 - val_acc: 0.5672\n",
      "Epoch 5/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6847 - acc: 0.5673 - val_loss: 0.6829 - val_acc: 0.5672\n",
      "Epoch 6/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6846 - acc: 0.5673 - val_loss: 0.6829 - val_acc: 0.5672\n",
      "Epoch 7/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6833 - acc: 0.5673 - val_loss: 0.6820 - val_acc: 0.5672\n",
      "Epoch 8/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6830 - acc: 0.5673 - val_loss: 0.6799 - val_acc: 0.5672\n",
      "Epoch 9/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6825 - acc: 0.5673 - val_loss: 0.6793 - val_acc: 0.5672\n",
      "Epoch 10/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6812 - acc: 0.5673 - val_loss: 0.6785 - val_acc: 0.5672\n",
      "Epoch 11/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6809 - acc: 0.5673 - val_loss: 0.6749 - val_acc: 0.5672\n",
      "Epoch 12/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6817 - acc: 0.5673 - val_loss: 0.6752 - val_acc: 0.5672\n",
      "Epoch 13/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6774 - acc: 0.5673 - val_loss: 0.6695 - val_acc: 0.5672\n",
      "Epoch 14/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6718 - acc: 0.5679 - val_loss: 0.6701 - val_acc: 0.5697\n",
      "Epoch 15/20\n",
      "1634/1634 [==============================] - 0s - loss: 0.6676 - acc: 0.5716 - val_loss: 0.6643 - val_acc: 0.5697\n",
      "Epoch 16/20\n",
      " 640/1634 [==========>...................] - ETA: 0s - loss: 0.6631 - acc: 0.5656"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-e5203eb68abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     history = model.fit(Xtrain, Ytrain, batch_size=128,\n\u001b[1;32m     10\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                         validation_data = (Xvalidation, Yvalidation))\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_validation_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/ifelse.pyc\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", validation_index)\n",
    "    Xtrain, Xvalidation = X_train[train_index], X_train[validation_index]\n",
    "    ytrain, yvalidation = Y_train[train_index], Y_train[validation_index]\n",
    "    model = create_nn_model()\n",
    "    Ytrain = np_utils.to_categorical(ytrain)\n",
    "    Yvalidation = np_utils.to_categorical(yvalidation)\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain, batch_size=128,\n",
    "                        epochs=20, verbose=1,\n",
    "                        validation_data = (Xvalidation, Yvalidation))\n",
    "    validation_acc = history.history['val_acc']\n",
    "    if validation_acc > best_validation_acc:\n",
    "        best_validation_acc = validation_acc\n",
    "        best_model = model\n",
    "Ytrain = np_utils.to_categorical(ytrain)\n",
    "best_model.fit(Xtrain, Ytrain, batch_size=115, epochs=20)  # replicate conf.\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "cls_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, cls_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### also try: sklearn.preprocessing.StandardScaler to apply feature scaling at the beginning ( because of the sparse matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2 convoluzioni + altri layer\n",
    "#### aggiungere traslazioni e rotazioni\n",
    "#### controllare parametri di training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ch_id_numu, dom_id_numu, trig_numu, times_numu = import_trees(numufile)\n",
    "ch_id_nue, dom_id_nue, trig_nue, times_nue = import_trees(nuefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dom_filter = dom_id_numu[evt][trig_numu[evt]==True]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([1397, 1397, 1398, 1398, 1398, 1398, 1398, 1398, 1398, 1398, 1398,\n",
      "       1399, 1399, 1399, 1399, 1399, 1401, 1401, 1401, 1907, 1907, 1907], dtype=int32))\n",
      "(1, array([1415, 1415, 1416, 1416, 1416, 1416, 1416, 1416, 1416, 1416, 1416,\n",
      "       1417, 1417, 1417, 1417, 1417, 1419, 1419, 1419, 1925, 1925, 1925], dtype=int32))\n",
      "(2, array([1433, 1433, 1434, 1434, 1434, 1434, 1434, 1434, 1434, 1434, 1434,\n",
      "       1435, 1435, 1435, 1435, 1435, 1437, 1437, 1437, 1943, 1943, 1943], dtype=int32))\n",
      "(3, array([1451, 1451, 1452, 1452, 1452, 1452, 1452, 1452, 1452, 1452, 1452,\n",
      "       1453, 1453, 1453, 1453, 1453, 1455, 1455, 1455, 1961, 1961, 1961], dtype=int32))\n",
      "(4, array([1469, 1469, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470, 1470,\n",
      "       1471, 1471, 1471, 1471, 1471, 1473, 1473, 1473, 1979, 1979, 1979], dtype=int32))\n",
      "(5, array([1487, 1487, 1488, 1488, 1488, 1488, 1488, 1488, 1488, 1488, 1488,\n",
      "       1489, 1489, 1489, 1489, 1489, 1491, 1491, 1491, 1997, 1997, 1997], dtype=int32))\n",
      "(6, array([1505, 1505, 1506, 1506, 1506, 1506, 1506, 1506, 1506, 1506, 1506,\n",
      "       1507, 1507, 1507, 1507, 1507, 1509, 1509, 1509, 2015, 2015, 2015], dtype=int32))\n",
      "(7, array([1523, 1523, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524,\n",
      "       1525, 1525, 1525, 1525, 1525, 1527, 1527, 1527, 2033, 2033, 2033], dtype=int32))\n",
      "(8, array([1541, 1541, 1542, 1542, 1542, 1542, 1542, 1542, 1542, 1542, 1542,\n",
      "       1543, 1543, 1543, 1543, 1543, 1545, 1545, 1545, 2051, 2051, 2051], dtype=int32))\n",
      "(9, array([1559, 1559, 1560, 1560, 1560, 1560, 1560, 1560, 1560, 1560, 1560,\n",
      "       1561, 1561, 1561, 1561, 1561, 1563, 1563, 1563, 2069, 2069, 2069], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "tr_evt = []\n",
    "for evt in range(10):\n",
    "    tr_evt = dom_id_numu[dom_filter]+18*i if np.all(triggered_dom_id[0]+18*i<2070) else triggered_dom_id[0]\n",
    "    print ((i, tr_evt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to plot data set (X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 115, 18)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_times(evt):\n",
    "    summed_X = np.sum(X[evt], axis=0)\n",
    "    #print(summed_X.shape, Y[evt])\n",
    "    return summed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2724, 75, 115, 18)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ch_sum = np.asarray([sum_times(evt) for evt in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2724, 115, 18)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ch_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1541, 1542, 1543, ..., 2721, 2722, 2723]),)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(Y==0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuEAAAJOCAYAAAAZCtmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wceQK167yXl9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8c0142110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "# Displaying the first training data\n",
    "fig = pyplot.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "#for numu, put evt number < 1541\n",
    "#img2 = ax2.imshow(sum_times(1500), cmap=mpl.cm.Blues)\n",
    "img2 = ax2.imshow(X_ch_sum[1500], cmap=mpl.cm.Blues)\n",
    "#for nue, put 1541< evt number < 2723 \n",
    "imgplot = ax.imshow(X_ch_sum[2720], cmap=mpl.cm.Blues)\n",
    "\n",
    "imgplot.set_interpolation('nearest')\n",
    "img2.set_interpolation('nearest')\n",
    "#ax.xaxis.set_ticks_position('top')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.set_title(\"nue\")\n",
    "ax2.set_title(\"numu\")\n",
    "ax.grid()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((115, 18), 1.0)\n",
      "((115, 18), 1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  4.,  3.,  3.,  1.,  1.,  1.,  3.,  4.,  4.,  3.,  2.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  3.,  2.,  2.,  1.,\n",
       "        1.,  1.,  2.,  4.,  3.,  3.,  3.,  1.,  1.,  3.,  4.,  4.,  6.,\n",
       "        4.,  1.,  1.,  2.,  2.,  4.,  4.,  5.,  2.,  3.,  1.,  1.,  3.,\n",
       "        1.,  1.,  1.,  1.,  2.,  4.,  6.,  5.,  4.,  1.,  2.,  1.,  1.,\n",
       "        1.,  1.,  1.,  3.,  3.,  3.,  3.,  1.,  1.,  2.,  3.,  3.,  5.,\n",
       "        5.,  3.,  3.,  2.,  4.,  3.,  4.,  4.,  2.,  1.,  2.,  1.,  1.,\n",
       "        4.,  1.,  3.,  2.,  1.,  1.,  1.,  1.,  2.,  1.,  2.,  1.,  1.,\n",
       "        1.,  5.,  4.,  5.,  4.,  2.,  2.,  1.,  2.,  1.,  3.,  1.,  1.,\n",
       "        3.,  5.,  5.,  5.,  2.,  2.,  1.,  1.,  2.,  1.,  2.,  1.,  2.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  3.,  3.,  2.,  2.,  2.,\n",
       "        1.,  5.,  2.,  4.,  5.,  4.,  4.,  3.,  3.,  4.,  3.,  4.,  4.,\n",
       "        3.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  3.,  4.,  4.,  5.,  3.,\n",
       "        3.,  1.,  2.,  2.,  1.,  2.,  2.,  2.,  4.,  3.,  4.,  3.,  5.,\n",
       "        3.,  2.,  2.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,\n",
       "        3.,  3.,  3.,  2.,  3.,  1.,  1.,  3.,  3.,  4.,  5.,  4.,  4.,\n",
       "        2.,  1.,  2.,  2.,  3.,  4.,  3.,  3.,  3.,  1.,  2.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  2.,  4.,  4.,  4.,  5.,  5.,  2.,  3.,\n",
       "        2.,  3.,  2.,  1.,  1.,  2.,  3.,  6.,  4.,  3.,  2.,  1.,  1.,\n",
       "        1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.,\n",
       "        2.,  4.,  1.,  3.,  1.,  1.,  3.,  4.,  6.,  6.,  5.,  5.,  3.,\n",
       "        1.,  1.,  2.,  3.,  5.,  5.,  5.,  3.,  3.,  2.,  1.,  2.,  1.,\n",
       "        2.,  2.,  2.,  1.,  1.,  1.,  2.,  2.,  3.,  2.,  1.,  1.,  3.,\n",
       "        2.,  2.,  5.,  2.,  2.,  1.,  1.,  1.,  1.,  4.,  4.,  6.,  5.,\n",
       "        4.,  4.,  3.,  2.,  2.,  1.,  3.,  3.,  2.,  1.,  2.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_times(1500)[sum_times(1500)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((115, 18), 0.0)\n",
      "((115, 18), 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.,   1.,   1.,   1.,   2.,   1.,   3.,   2.,   1.,   3.,   1.,\n",
       "         2.,   3.,   4.,   2.,   3.,   3.,   3.,   2.,   1.,   1.,   1.,\n",
       "         2.,   2.,   3.,   3.,   4.,   3.,   5.,   4.,   2.,   1.,   4.,\n",
       "         5.,   6.,   5.,   4.,   6.,   6.,   5.,   3.,   3.,   1.,   1.,\n",
       "         1.,   1.,   3.,   4.,   4.,   4.,   5.,   4.,   3.,   4.,   2.,\n",
       "         4.,   5.,   5.,   7.,   7.,   8.,   7.,   7.,   5.,   6.,   2.,\n",
       "         1.,   1.,   1.,   1.,   2.,   3.,   4.,   3.,   3.,   3.,   4.,\n",
       "         2.,   2.,   4.,   5.,   5.,   5.,   7.,   6.,   6.,   5.,   5.,\n",
       "         5.,   2.,   1.,   4.,   8.,   7.,   8.,   8.,   8.,   9.,   7.,\n",
       "         8.,   7.,   6.,   3.,   3.,   1.,   1.,   2.,   2.,   3.,   4.,\n",
       "         2.,   3.,   1.,   1.,   2.,   2.,   1.,   5.,   6.,   6.,   9.,\n",
       "         8.,   6.,   9.,   6.,   5.,   3.,   1.,   1.,   7.,   7.,   8.,\n",
       "        10.,  10.,  11.,  10.,   9.,   8.,   7.,   6.,   5.,   4.,   1.,\n",
       "         1.,   1.,   1.,   2.,   2.,   3.,   4.,   3.,   3.,   4.,   1.,\n",
       "         3.,   5.,   5.,   6.,   5.,   5.,   4.,   3.,   4.,   2.,   2.,\n",
       "         2.,   7.,   6.,   7.,   8.,   8.,   9.,  10.,   7.,   7.,   7.,\n",
       "         4.,   3.,   1.,   1.,   2.,   2.,   1.,   2.,   3.,   1.,   1.,\n",
       "         4.,   6.,   4.,   7.,   4.,   4.,   6.,   5.,   3.,   4.,   1.,\n",
       "         1.,   7.,   8.,  11.,   9.,  10.,  10.,   8.,   9.,   5.,   5.,\n",
       "         5.,   5.,   1.,   1.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_times(2720)[sum_times(2720)!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try Convolution 1D with `X_ch_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2724, 115, 18)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ch_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2724,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (2043, 115, 18), 'X_test: ', (681, 115, 18), 'Y_train: ', (2043,), 'Y_test: ', (681,))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_ch_sum, Y, test_size=0.25, random_state=42)\n",
    "print('X_train: ', X_train.shape, 'X_test: ', X_test.shape, 'Y_train: ', Y_train.shape, 'Y_test: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "num_classes=2\n",
    "Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "Y_test = np_utils.to_categorical(Y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 114, 64)           2368      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 114, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 113, 32)           4128      \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 111, 16)           1552      \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 111, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 1776)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 2)                 3554      \n",
      "=================================================================\n",
      "Total params: 11,602\n",
      "Trainable params: 11,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Activation, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(64, kernel_size=2, activation='relu', input_shape=input_shape))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(32, kernel_size=2, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Convolution1D(16, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2043 samples, validate on 681 samples\n",
      "Epoch 1/20\n",
      "2043/2043 [==============================] - 0s - loss: 0.6916 - acc: 0.5634 - val_loss: 0.6892 - val_acc: 0.5609\n",
      "Epoch 2/20\n",
      "2043/2043 [==============================] - 0s - loss: 0.6865 - acc: 0.5673 - val_loss: 0.6856 - val_acc: 0.5609\n",
      "Epoch 3/20\n",
      "2043/2043 [==============================] - 0s - loss: 0.6847 - acc: 0.5673 - val_loss: 0.6855 - val_acc: 0.5609\n",
      "Epoch 4/20\n",
      "2043/2043 [==============================] - 0s - loss: 0.6842 - acc: 0.5673 - val_loss: 0.6854 - val_acc: 0.5609\n",
      "Epoch 5/20\n",
      "2043/2043 [==============================] - 0s - loss: 0.6841 - acc: 0.5673 - val_loss: 0.6855 - val_acc: 0.5609\n",
      "Epoch 6/20\n",
      "2043/2043 [==============================] - 0s - loss: 0.6828 - acc: 0.5673 - val_loss: 0.6854 - val_acc: 0.5609\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "network_history_conv1D = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=20, verbose=1, validation_data=(X_test, Y_test), \n",
    "                            callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### without cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 75, 2070)          4286970   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 75, 2070)          0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 75, 2070)          4286970   \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 155250)            0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 310502    \n",
      "=================================================================\n",
      "Total params: 8,884,442\n",
      "Trainable params: 8,884,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "model = Sequential()\n",
    "model.add(Dense(2070, input_shape=((X_train.shape[1], X_train.shape[2]*X_train.shape[3])), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2070, activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2179 samples, validate on 545 samples\n",
      "Epoch 1/20\n",
      "2179/2179 [==============================] - 70s - loss: 0.6807 - acc: 0.6241 - val_loss: 0.6701 - val_acc: 0.5908\n",
      "Epoch 2/20\n",
      "2179/2179 [==============================] - 70s - loss: 0.6575 - acc: 0.6416 - val_loss: 0.6522 - val_acc: 0.7009\n",
      "Epoch 3/20\n",
      "2179/2179 [==============================] - 70s - loss: 0.6377 - acc: 0.7531 - val_loss: 0.6363 - val_acc: 0.7303\n",
      "Epoch 4/20\n",
      "2179/2179 [==============================] - 71s - loss: 0.6207 - acc: 0.7568 - val_loss: 0.6224 - val_acc: 0.7486\n",
      "Epoch 5/20\n",
      "2179/2179 [==============================] - 71s - loss: 0.6054 - acc: 0.7655 - val_loss: 0.6098 - val_acc: 0.7596\n",
      "Epoch 6/20\n",
      "2179/2179 [==============================] - 72s - loss: 0.5915 - acc: 0.7715 - val_loss: 0.5983 - val_acc: 0.7670\n",
      "Epoch 7/20\n",
      "2179/2179 [==============================] - 72s - loss: 0.5790 - acc: 0.7774 - val_loss: 0.5877 - val_acc: 0.7743\n",
      "Epoch 8/20\n",
      "2179/2179 [==============================] - 72s - loss: 0.5675 - acc: 0.7737 - val_loss: 0.5782 - val_acc: 0.7817\n",
      "Epoch 9/20\n",
      "2179/2179 [==============================] - 72s - loss: 0.5567 - acc: 0.7857 - val_loss: 0.5688 - val_acc: 0.7835\n",
      "Epoch 10/20\n",
      "2179/2179 [==============================] - 72s - loss: 0.5473 - acc: 0.7875 - val_loss: 0.5602 - val_acc: 0.7853\n",
      "Epoch 11/20\n",
      "2179/2179 [==============================] - 72s - loss: 0.5377 - acc: 0.7871 - val_loss: 0.5523 - val_acc: 0.7982\n",
      "Epoch 12/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.5292 - acc: 0.7985 - val_loss: 0.5445 - val_acc: 0.7963\n",
      "Epoch 13/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.5213 - acc: 0.7921 - val_loss: 0.5377 - val_acc: 0.8000\n",
      "Epoch 14/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.5137 - acc: 0.8031 - val_loss: 0.5308 - val_acc: 0.8000\n",
      "Epoch 15/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.5065 - acc: 0.8073 - val_loss: 0.5242 - val_acc: 0.8000\n",
      "Epoch 16/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.4997 - acc: 0.8045 - val_loss: 0.5185 - val_acc: 0.8037\n",
      "Epoch 17/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.4937 - acc: 0.8137 - val_loss: 0.5126 - val_acc: 0.8000\n",
      "Epoch 18/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.4876 - acc: 0.8073 - val_loss: 0.5074 - val_acc: 0.8165\n",
      "Epoch 19/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.4819 - acc: 0.8146 - val_loss: 0.5024 - val_acc: 0.8165\n",
      "Epoch 20/20\n",
      "2179/2179 [==============================] - 73s - loss: 0.4768 - acc: 0.8187 - val_loss: 0.4973 - val_acc: 0.8183\n"
     ]
    }
   ],
   "source": [
    "network_history = model.fit(X_train, Y_train, batch_size=115, \n",
    "                            epochs=20, verbose=1, validation_data=(X_test, y_test), \n",
    "                            callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efb259b7f90>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efdb420af90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efb25860e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(network_history.history['loss'])\n",
    "plt.plot(network_history.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(network_history.history['acc'])\n",
    "plt.plot(network_history.history['val_acc'])\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = np.load('history.npy').item()['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f127fbbf7d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSV1MJJKkLiYSSVIXE4kkqcvfimCsFy7yy2wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f127fff8b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f127fb86b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.plot(np.load('history.npy').item()['loss'])\n",
    "plt.plot(np.load('history.npy').item()['val_loss'])\n",
    "\n",
    "plt.legend(['Training', 'Validation'])\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.plot(np.load('history.npy').item()['acc'])\n",
    "plt.plot(np.load('history.npy').item()['val_acc'])\n",
    "\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Try with a different kind of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Activation, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAR SESSION \n",
    "\n",
    "#### Free Memory on GPU\n",
    "\n",
    "```python\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'clear_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-28d159846ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'clear_session'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2043, 74, 115, 18)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows = X_train.shape[1]\n",
    "img_cols = X_train.shape[2]\n",
    "img_channel = X_train.shape[3]\n",
    "input_shape = (img_rows, img_cols, img_channel)\n",
    "num_classes = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try with new dataset\n",
    "#img_rows = X_train.shape[1]\n",
    "#img_cols = X_train.shape[2]\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "num_classes = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "Y_test = np_utils.to_categorical(Y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try with new dataset\n",
    "#x_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "#x_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "Y_test = np_utils.to_categorical(Y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Activation, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_66 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 8480)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 16962     \n",
      "=================================================================\n",
      "Total params: 195,746\n",
      "Trainable params: 195,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(128, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#hist = model.fit(x_train, y_train,\n",
    "#          batch_size=batch_size,\n",
    "#          epochs=20,\n",
    "#          verbose=1,\n",
    "#validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_29 to have 4 dimensions, but got array with shape (2043, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-51ebf85345ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m network_history3 = model.fit(X_train, Y_train, batch_size=128, \n\u001b[1;32m      2\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                             callbacks=[early_stop])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1430\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1310\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1311\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_29 to have 4 dimensions, but got array with shape (2043, 1)"
     ]
    }
   ],
   "source": [
    "network_history3 = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=10, verbose=1, validation_data=(X_test, Y_test), \n",
    "                            callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2043 samples, validate on 681 samples\n",
      "Epoch 1/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.2108 - acc: 0.9173 - val_loss: 0.2793 - val_acc: 0.9060\n",
      "Epoch 2/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1970 - acc: 0.9241 - val_loss: 0.2604 - val_acc: 0.9119\n",
      "Epoch 3/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.2019 - acc: 0.9217 - val_loss: 0.2789 - val_acc: 0.9090\n",
      "Epoch 4/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1836 - acc: 0.9320 - val_loss: 0.2671 - val_acc: 0.9119\n",
      "Epoch 5/20\n",
      "2043/2043 [==============================] - 94s - loss: 0.1802 - acc: 0.9310 - val_loss: 0.2636 - val_acc: 0.9090\n",
      "Epoch 6/20\n",
      "2043/2043 [==============================] - 94s - loss: 0.1759 - acc: 0.9315 - val_loss: 0.2665 - val_acc: 0.9134\n",
      "Epoch 7/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.3819 - acc: 0.8918 - val_loss: 0.2892 - val_acc: 0.9060\n",
      "Epoch 8/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1709 - acc: 0.9290 - val_loss: 0.2657 - val_acc: 0.9104\n",
      "Epoch 9/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1639 - acc: 0.9349 - val_loss: 0.2649 - val_acc: 0.9016\n",
      "Epoch 10/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1625 - acc: 0.9398 - val_loss: 0.2665 - val_acc: 0.9134\n",
      "Epoch 11/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1593 - acc: 0.9369 - val_loss: 0.2605 - val_acc: 0.9163\n",
      "Epoch 12/20\n",
      "2043/2043 [==============================] - 94s - loss: 0.1549 - acc: 0.9398 - val_loss: 0.2656 - val_acc: 0.9148\n",
      "Epoch 13/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1539 - acc: 0.9432 - val_loss: 0.2615 - val_acc: 0.9207\n",
      "Epoch 14/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1491 - acc: 0.9427 - val_loss: 0.2701 - val_acc: 0.9134\n",
      "Epoch 15/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1503 - acc: 0.9457 - val_loss: 0.2633 - val_acc: 0.9148\n",
      "Epoch 16/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1479 - acc: 0.9476 - val_loss: 0.2693 - val_acc: 0.9163\n",
      "Epoch 17/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1484 - acc: 0.9442 - val_loss: 0.2630 - val_acc: 0.9236\n",
      "Epoch 18/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1430 - acc: 0.9432 - val_loss: 0.2623 - val_acc: 0.9236\n",
      "Epoch 19/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1368 - acc: 0.9486 - val_loss: 0.2639 - val_acc: 0.9148\n",
      "Epoch 20/20\n",
      "2043/2043 [==============================] - 95s - loss: 0.1354 - acc: 0.9476 - val_loss: 0.2662 - val_acc: 0.9178\n"
     ]
    }
   ],
   "source": [
    "network_history4 = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=20, verbose=1, validation_data=(X_test, Y_test), \n",
    "                            callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/cdesio/.theano/compiledir_Linux-4.2--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2043 samples, validate on 681 samples\n",
      "Epoch 1/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.6836 - acc: 0.5580 - val_loss: 0.6792 - val_acc: 0.5609\n",
      "Epoch 2/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.6539 - acc: 0.5673 - val_loss: 0.6352 - val_acc: 0.5609\n",
      "Epoch 3/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.5546 - acc: 0.7171 - val_loss: 0.4934 - val_acc: 0.8576\n",
      "Epoch 4/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.4683 - acc: 0.8169 - val_loss: 0.4070 - val_acc: 0.8634\n",
      "Epoch 5/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.3493 - acc: 0.8767 - val_loss: 0.3370 - val_acc: 0.8590\n",
      "Epoch 6/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.3281 - acc: 0.8634 - val_loss: 0.2994 - val_acc: 0.8840\n",
      "Epoch 7/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2729 - acc: 0.8982 - val_loss: 0.2745 - val_acc: 0.8869\n",
      "Epoch 8/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2521 - acc: 0.9046 - val_loss: 0.2707 - val_acc: 0.8913\n",
      "Epoch 9/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2577 - acc: 0.8967 - val_loss: 0.2628 - val_acc: 0.8972\n",
      "Epoch 10/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2278 - acc: 0.9124 - val_loss: 0.2566 - val_acc: 0.9031\n",
      "Epoch 11/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2212 - acc: 0.9158 - val_loss: 0.2531 - val_acc: 0.9090\n",
      "Epoch 12/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2112 - acc: 0.9183 - val_loss: 0.2641 - val_acc: 0.9060\n",
      "Epoch 13/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2054 - acc: 0.9212 - val_loss: 0.2622 - val_acc: 0.9104\n",
      "Epoch 14/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2011 - acc: 0.9222 - val_loss: 0.2565 - val_acc: 0.9119\n",
      "Epoch 15/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.1948 - acc: 0.9241 - val_loss: 0.2573 - val_acc: 0.9090\n",
      "Epoch 16/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.1875 - acc: 0.9285 - val_loss: 0.2630 - val_acc: 0.9104\n",
      "Epoch 17/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.1841 - acc: 0.9280 - val_loss: 0.2542 - val_acc: 0.9075\n",
      "Epoch 18/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.1776 - acc: 0.9339 - val_loss: 0.2608 - val_acc: 0.9134\n",
      "Epoch 19/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.1757 - acc: 0.9344 - val_loss: 0.2549 - val_acc: 0.9075\n",
      "Epoch 20/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.1673 - acc: 0.9349 - val_loss: 0.2602 - val_acc: 0.9060\n"
     ]
    }
   ],
   "source": [
    "network_history4 = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=20, verbose=1, validation_data=(X_test, Y_test), \n",
    "                            callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe9003b8e10>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSinloqGglFLKRUNBKaWUi4aCUkopl/8Pa9jsvHy2xigAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8fffae210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAuoAYEHCUdI9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8e006c390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(network_history4.history['loss'])\n",
    "plt.plot(network_history4.history['val_loss'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(network_history4.history['acc'])\n",
    "plt.plot(network_history4.history['val_acc'])\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_69 (Conv2D)           (None, 128, 113, 16)      86528     \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 64, 111, 14)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64, 55, 7)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 32, 53, 5)         18464     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 32, 53, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 32, 51, 3)         9248      \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 4896)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 2)                 9794      \n",
      "=================================================================\n",
      "Total params: 197,826\n",
      "Trainable params: 197,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_acc', patience=4, verbose=1)\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(128, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2043 samples, validate on 681 samples\n",
      "Epoch 1/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.6854 - acc: 0.5629 - val_loss: 0.6795 - val_acc: 0.5609\n",
      "Epoch 2/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.6694 - acc: 0.5673 - val_loss: 0.6606 - val_acc: 0.5609\n",
      "Epoch 3/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.6081 - acc: 0.5898 - val_loss: 0.5763 - val_acc: 0.7137\n",
      "Epoch 4/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.5196 - acc: 0.7513 - val_loss: 0.4580 - val_acc: 0.8267\n",
      "Epoch 5/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.4140 - acc: 0.8169 - val_loss: 0.3527 - val_acc: 0.8458\n",
      "Epoch 6/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.3214 - acc: 0.8757 - val_loss: 0.2873 - val_acc: 0.8634\n",
      "Epoch 7/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.3345 - acc: 0.8629 - val_loss: 0.2680 - val_acc: 0.8825\n",
      "Epoch 8/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2680 - acc: 0.8987 - val_loss: 0.2675 - val_acc: 0.8781\n",
      "Epoch 9/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2585 - acc: 0.9050 - val_loss: 0.2525 - val_acc: 0.8899\n",
      "Epoch 10/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2358 - acc: 0.9046 - val_loss: 0.2441 - val_acc: 0.9001\n",
      "Epoch 11/20\n",
      "2043/2043 [==============================] - 6s - loss: 0.2172 - acc: 0.9158 - val_loss: 0.2351 - val_acc: 0.9075\n",
      "Epoch 12/20\n",
      "2043/2043 [==============================] - 7s - loss: 0.2098 - acc: 0.9158 - val_loss: 0.2444 - val_acc: 0.9016\n",
      "Epoch 13/20\n",
      "2043/2043 [==============================] - 7s - loss: 0.1929 - acc: 0.9246 - val_loss: 0.2337 - val_acc: 0.9104\n",
      "Epoch 14/20\n",
      "2043/2043 [==============================] - 7s - loss: 0.1873 - acc: 0.9271 - val_loss: 0.2483 - val_acc: 0.8987\n",
      "Epoch 15/20\n",
      "2043/2043 [==============================] - 7s - loss: 0.1790 - acc: 0.9266 - val_loss: 0.2605 - val_acc: 0.8943\n",
      "Epoch 16/20\n",
      "2043/2043 [==============================] - 7s - loss: 0.1780 - acc: 0.9295 - val_loss: 0.2292 - val_acc: 0.9075\n",
      "Epoch 17/20\n",
      "2043/2043 [==============================] - 7s - loss: 0.1627 - acc: 0.9359 - val_loss: 0.2252 - val_acc: 0.9075\n",
      "Epoch 18/20\n",
      "2043/2043 [==============================] - 7s - loss: 0.1652 - acc: 0.9344 - val_loss: 0.2306 - val_acc: 0.9031\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "network_history5 = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=20, verbose=1, validation_data=(X_test, Y_test), \n",
    "                            callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###VGG16\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Test pretrained model\n",
    "model = VGG_16()\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "network_history1 = model.fit(X_train, y_train, batch_size=115, \n",
    "                            epochs=20, verbose=1, validation_data=(X_test, y_test), \n",
    "                            callbacks=[early_stop])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
